name: fuhsin_ai_demo
services:
# ElasticSearch settings
  elasticsearch:
    build: .
    container_name: es01
    hostname: es01
    environment:
    # cluster settings
      - node.name=es01
      - cluster.name=elasticsearch-cluster
      - discovery.type=single-node

    # security settings
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ES_PASS}
      
    # memory settings
      - ES_JAVA_OPTS=-Xms1g -Xmx1g

      - network.host=0.0.0.0

      - bootstrap.memory_lock=true
      
    # 優化設定
      - indices.memory.index_buffer_size=30%
      - indices.queries.cache.size=15%
      - indices.requests.cache.size=2%
    ports:
      - "9200:9200"    # REST API
      - "9300:9300"    # 節點間要通訊，單機雖用不到但仍需保留
    volumes:
      - es_data:/usr/share/elasticsearch/data
      - es_logs:/usr/share/elasticsearch/logs
      - ./data/import:/data/import:rw
# Linux 實機長需要; Mac/Win 通常不用更動
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile: 
        soft: 262144
        hard: 262144
    
    healthcheck:
      # test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      test: ["CMD-SHELL", "curl -fsS -u elastic:${ES_PASS} 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=30s' >/dev/null"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    restart: unless-stopped

    networks:
      - elastic

# kibana settings
  kibana:
    image: docker.elastic.co/kibana/kibana:8.15.1
    container_name: kibana
    hostname: kibana
    depends_on: 
      elasticsearch:
        condition: service_healthy
    environment:
    # connection settings
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=kibana123

    # kibana server settings
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=5601

    # language settings
      - I18N_LOCALE=zh-TW
      - I18N_DEFAULTLOCALE=zh-TW
      - KIBANA_DEFAULTAPPID=home
    ports:
      - "5601:5601"      #kibana Web
    volumes:
      - kibana_data:/usr/share/kibana/data

     # 健康檢查
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    
    restart: unless-stopped
    networks:
      - elastic

# 向量檢索服務
  vector-generator:
    image: python:3.11-slim
    container_name: vector-generator
    depends_on:
      - elasticsearch
    environment:
      - ES_URL=http://elasticsearch:9200
      - ES_USER=${ES_USER}
      - ES_PASS=${ES_PASS}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - INDEX_PATTERN=erp-*
      - VECTOR_BATCH_SIZE=${VECTOR_BATCH_SIZE:-50}
      - SLEEP=${VECTOR_SLEEP:-5}
      - ES_WAIT_TIMEOUT=180
      - REQUESTS_TIMEOUT=30
      - MAX_RETRIES=5
    volumes:
      - ./scripts:/scripts:ro
      - ./logs/vector:/logs:rw
    working_dir: /scripts
    command: >
      bash -c "
        pip install --no-cache-dir openai requests numpy &&
        python vector_service.py 2>&1 | tee -a /logs/vector.log
      "
    restart: unless-stopped
    networks:
      - elastic

# 後端 -> 用於偵測問題與回應
  rag-api:
    build:
      context: ./scripts/rag-api
      dockerfile: Dockerfile.rag-api
    container_name: rag-api
    depends_on:
      - elasticsearch
      - vector-generator
    environment:
      - ES_URL=http://elasticsearch:9200
      - ES_USER=${ES_USER}
      - ES_PASS=${ES_PASS}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - GPT_MODEL=${GPT_MODEL}
    ports:
      - "8010:8010"  # FastAPI 服務
    volumes:
      - ./scripts/rag-api:/scripts
    healthcheck:
      test: [
        "CMD",
        "python",
        "-c",
        "import urllib.request,sys; \
        urllib.request.urlopen('http://localhost:8010/health', timeout=5).read(); \
        sys.exit(0)"
      ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - elastic

# ============== DataBase 相關 ==============
  # 資料庫設定
  mysql:
    image: mysql:8.0
    container_name: mysql3316
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=fuhsin_erp_demo
      - MYSQL_CHARACTER_SET=utf8mb4
      - MYSQL_COLLATION=utf8mb4_unicode_ci
    ports:
      - "3316:3306"
    volumes:
      - ./sql/init:/docker-entrypoint-initdb.d:ro
      - ./sql/incoming:/data/sql_import:rw
      - mysql_data:/var/lib/mysql
    command: >
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --default-time-zone='+08:00'
      --max_connections=1000
      --max_allowed_packet=64M
      --innodb_buffer_pool_size=2G
      --innodb_log_file_size=256M
      --innodb_flush_log_at_trx_commit=2
      --innodb_flush_method=O_DIRECT
      --innodb_file_per_table=1
      --slow_query_log=1
      --long_query_time=2
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -uroot -proot | grep 'mysqld is alive'"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped
    networks:
      - elastic

  # 自動檔案處理服務
  mysql_auto_importer:
    build:
      context: ./scripts/mysql_auto_importer
      dockerfile: Dockerfile.mysql_auto_importer
    container_name: mysql_auto_importer
    depends_on:
      mysql:
        condition: service_healthy
    environment:
    # MySQL 連線設定（可從 .env 帶入）
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
      - MYSQL_DATABASE=fuhsin_erp_demo
    # 監控目錄與掃描頻率
      - SQL_WATCH_DIR=/mnt/sql/incoming
      - SCAN_INTERVAL=${SQL_SCAN_INTERVAL:-10}
      - SQL_BATCH_SIZE=${SQL_BATCH_SIZE:-1000}
      - MAX_RETRY=3
      - POOL_SIZE=5
    volumes:
      # - ./scripts/db:/app/scripts:ro        # 放你的 mysql_auto_importer.py
      - ./sql:/mnt/sql                   # 掛載 SQL 資料夾（請在 ./sql 建 incoming/ .done/ .error/）
      - ./state:/scripts                 # 狀態檔持久化（.import_state.json）
      - ./logs/importer:/logs:rw
    working_dir: /app
    # command: >
    #   bash -lc "pip install --no-cache-dir -r scripts/db/requirements.txt &&
    #             python scripts/db/mysql_auto_importer.py 2>&1 | tee -a /logs/importer/auto_importer.log"
    restart: unless-stopped
    networks:
      - elastic
  
  pdf-processor:
    build:
      context: ./scripts/pdf_processor
      dockerfile: Dockerfile.pdf_processor
    container_name: pdf-processor
    depends_on:
      mysql:
        condition: service_healthy
    environment:
    # MySQL 連線設定（可從 .env 帶入）
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
      - MYSQL_DATABASE=fuhsin_erp_demo
    
    # PDF 處理設定
      - PDF_WATCH_DIR=/mnt/pdf/incoming     # 監控目錄
      - SCAN_INTERVAL=${PDF_SCAN_INTERVAL:-20}  # 掃描間隔（秒）
      - PROCESS_BATCH_SIZE=${PDF_BATCH_SIZE:-3} # 每批處理檔案數
      - ENABLE_OCR=${ENABLE_OCR:-true}         # 是否啟用 OCR
      - OCR_LANG=chi_tra+eng                    # OCR 語言（繁體中文+英文）
    
    # 日誌設定
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    
    volumes:
      - ./scripts/pdf_processor:/app:ro
      - ./pdf:/mnt/pdf
      - ./state/pdf:/state
      - ./logs/pdf:/logs:rw
    working_dir: /app
    restart: unless-stopped
    networks:
      - elastic
    
    # 資源限制（PDF 處理可能消耗較多記憶體）
    mem_limit: 4g
    cpus: '2.0'
    
    # 健康檢查
    healthcheck:
      test: ["CMD", "python", "-c", "import pymysql; pymysql.connect(host='mysql', user='root', password='root')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # 將資料表中資料同步至 ES
  db-sync:
    build:
      context: ./scripts/db-sync-2
      dockerfile: Dockerfile.db-sync-2
    container_name: db-sync-2
    depends_on:
      mysql:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    environment:
    # 資料庫（SQLAlchemy DSN；統一用 utf8mb4）
      - DB_URL=mysql+pymysql://root:root@mysql:3306/fuhsin_erp_demo?charset=utf8mb4
    # ES 連線
      - ES_URL=http://elasticsearch:9200
      - ES_USER=elastic
      - ES_PASS=${ES_PASS}
    # 批次與輪詢
      - BATCH_SIZE=${DB_BATCH_SIZE:-1000}
      - PAGE_SIZE=${DB_PAGE_SIZE:-3000}
      - PARALLEL_THREADS=${PARALLEL_THREADS:-4}
      - SLEEP_SECONDS=${DB_SYNC_INTERVAL:-30}
    volumes:
      # - ./scripts:/app/scripts:ro        # 放你的 db-sync-2.py
      - ./state:/state                 # 狀態檔持久化（.sync_state.json）
      - ./logs/db-sync-2:/logs:rw
    working_dir: /app
    # command: >
    #   bash -lc "pip install --no-cache-dir -r scripts/db/requirements.txt &&
    #             python scripts/db/db-sync-2.py 2>&1 | tee -a /logs/db-sync-2/db_sync_2.log"
    deploy:
      resources:                        # 在 Swarm 模式會啟動, 單純 up 不會
        limits:                         # 代表使用記憶體上限值
          memory: 2G
        reservations:
          memory: 512M                  # 代表使用記憶體下限值
    restart: unless-stopped
    networks:
      - elastic

# ============== React 前端應用 ==============
  # web-ui -> 部署用
  web-ui:
    profiles: ["prod"]
    build: 
      context: ./web
      dockerfile: Dockerfile
    container_name: web-ui
    depends_on:
      rag-api:
        condition: service_healthy
    ports:
      - "80:80"
    environment:
      - VITE_API_URL=/api       # VITE -> VITE_* ; REACT CRA -> REACT_APP_*
    restart: unless-stopped
    networks:
      - elastic

  # 開發：Vite dev server + HMR
  web-ui-dev:
    profiles: ["dev"]
    image: node:20
    working_dir: /app
    # 掛載原始碼；另外掛一個匿名卷避免覆蓋容器內的 node_modules
    volumes:
      - ./web:/app
      - /app/node_modules
    ports:
      - "5174:5174"
    environment:
      # 有些平台需要 polling 才能偵測檔案更動（WSL/Docker Desktop）
      - CHOKIDAR_USEPOLLING=true
      - VITE_PROXY_TARGET=http://rag-api:8010  
      - VITE_HOST=0.0.0.0
    # 安裝依賴後啟動 dev server；--host 讓外部（你的瀏覽器）可訪問
    command: sh -c "npm ci && npm run dev -- --host 0.0.0.0 --port 5174"
    networks:
      - elastic


volumes:
  es_data:
    name: fuhsin_ai_demo_es_data
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/elasticsearch
  es_logs:
    name: fuhsin_ai_demo_es_logs
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./logs/elasticsearch
  # es_plugins:
  #   driver: local
  #   driver_opts:
  #     type: none
  #     o: bind
  #     device: ./plugins/elasticsearch
  kibana_data:
    name: fuhsin_ai_demo_kibana_data
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/kibana
  mysql_data:
    name: fuhsin_ai_demo_mysql_data
    driver: local

networks:
  elastic:
    driver: bridge
