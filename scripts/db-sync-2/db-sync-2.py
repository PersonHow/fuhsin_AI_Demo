#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å„ªåŒ–ç‰ˆ MySQL to Elasticsearch ç›´æ¥åŒæ­¥æœå‹™
- æ”¯æ´å¤§é‡è³‡æ–™è™•ç†
- æ–°å¢ technical_documents è¡¨åŒæ­¥
- æ™ºèƒ½ç”¢å“é—œè¯
"""

import os, json, time, re, logging, pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Generator
from sqlalchemy import create_engine, text
from elasticsearch import Elasticsearch
from elasticsearch.helpers import parallel_bulk, BulkIndexError

# ============== é…ç½® ==============
DB_URL = os.getenv("DB_URL", "mysql+pymysql://root:root@mysql:3306/fuhsin_erp_demo")
ES_URL = os.getenv("ES_URL", "http://elasticsearch:9200")
ES_USER = os.getenv("ES_USER", "elastic")
ES_PASS = os.getenv("ES_PASS", "admin@12345")

# æ•ˆèƒ½ç›¸é—œé…ç½®
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "2000"))
PAGE_SIZE = int(os.getenv("PAGE_SIZE", "5000"))
PARALLEL_THREADS = int(os.getenv("PARALLEL_THREADS", "4"))
SLEEP_SECONDS = int(os.getenv("SLEEP_SECONDS", "30"))

# æª”æ¡ˆè·¯å¾‘
STATE_PATH = "/state/.sync_state.json"
LOG_PATH = "/logs/db-sync/db_sync.log"

# ============== æ—¥èªŒè¨­å®š ==============
os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============== è³‡æ–™åº«é€£ç·šæ±  ==============
engine = create_engine(
    DB_URL,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=False,
    connect_args={
        "charset": "utf8mb4",
        "connect_timeout": 10,
        "init_command": (
            "SET SESSION sql_mode = "
            "REPLACE(REPLACE(@@sql_mode,'NO_ZERO_DATE',''),'NO_ZERO_IN_DATE','')"
        )
    }
)

# ============== Elasticsearch å®¢æˆ¶ç«¯ ==============
def get_es_client():
    """å»ºç«‹ Elasticsearch å®¢æˆ¶ç«¯"""
    return Elasticsearch(
        [ES_URL],
        basic_auth=(ES_USER, ES_PASS) if ES_USER and ES_PASS else None,
        verify_certs=False,
        timeout=30,
        max_retries=3,
        retry_on_timeout=True
    )

# ============== ç´¢å¼•ç®¡ç† ==============
def ensure_index(es, index_name):
    """ç¢ºä¿ç´¢å¼•å­˜åœ¨ä¸¦é…ç½®æ­£ç¢º"""
    if es.indices.exists(index=index_name):
        logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨")
        es.indices.put_settings(
            index=index_name,
            body={"index": {"refresh_interval": "30s"}}
        )
        return
    
    # å»ºç«‹æ–°ç´¢å¼• - æ ¹æ“šä¸åŒç´¢å¼•é¡å‹ä½¿ç”¨ä¸åŒæ˜ å°„
    if index_name == "erp-tech-docs":
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "index.refresh_interval": "30s",
                "analysis": {
                    "normalizer": {
                        "lowercase_normalizer": {
                            "type": "custom",
                            "filter": ["lowercase"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # åŸºæœ¬æ¬„ä½
                    "doc_id": {"type": "keyword"},
                    "doc_type": {"type": "keyword"},
                    "doc_number": {"type": "keyword"},
                    "title": {
                        "type": "text",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    
                    # ç”¢å“é—œè¯
                    "product_ids": {"type": "keyword"},  # JSON é™£åˆ—æœƒè‡ªå‹•å±•é–‹
                    
                    # æ–‡æª”è³‡è¨Š
                    "revision": {"type": "keyword"},
                    "department": {"type": "keyword"},
                    "author": {"type": "keyword"},
                    "issue_date": {"type": "date", "format": "yyyy-MM-dd||yyyy/MM/dd||strict_date_optional_time||epoch_millis"},
                    
                    # å…§å®¹æ¬„ä½
                    "content": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "summary": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "keywords": {"type": "keyword"},  # JSON é™£åˆ—æœƒè‡ªå‹•å±•é–‹
                    
                    # æª”æ¡ˆè³‡è¨Š
                    "file_path": {"type": "keyword"},
                    "file_hash": {"type": "keyword"},
                    "page_count": {"type": "integer"},
                    "file_size": {"type": "long"},
                    
                    # æ™‚é–“æˆ³è¨˜
                    "created_at": {"type": "date", "format": "strict_date_time||epoch_millis"},
                    "updated_at": {"type": "date", "format": "strict_date_time||epoch_millis"},
                    
                    # å…ƒè³‡æ–™
                    "metadata": {
                        "type": "object",
                        "enabled": True,
                        "dynamic": True
                    },
                    
                    # å‘é‡æ¬„ä½ï¼ˆç‚ºæœªä¾†é ç•™ï¼‰
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 1536,
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }
    else:
        # åŸæœ‰ç´¢å¼•çš„æ˜ å°„ï¼ˆproducts, warehouse, complaintsï¼‰
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "index.refresh_interval": "30s",
                "analysis": {
                    "normalizer": {
                        "lowercase_normalizer": {
                            "type": "custom",
                            "filter": ["lowercase"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "type": {"type": "keyword"},
                    "searchable_content": {"type": "text", "analyzer": "standard"},
                    "all_content": {"type": "text", "analyzer": "standard"},
                    "title": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                    "product_ids": {"type": "keyword"},
                    "status": {"type": "keyword", "normalizer": "lowercase_normalizer"},
                    "updated_at": {
                        "type": "date",
                        "format": "strict_date_time||epoch_millis||yyyy-MM-dd HH:mm:ss"
                    },
                    "metadata": {"type": "object", "enabled": True, "dynamic": True},
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 1536,
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }
    
    es.indices.create(index=index_name, body=mapping)
    logger.info(f"âœ… å»ºç«‹ç´¢å¼•: {index_name}")

# ============== ç”¢å“å¿«å– ==============
class ProductCache:
    """ç”¢å“è³‡è¨Šå¿«å–"""
    
    def __init__(self):
        self.products = {}
        self.last_refresh = None
        
    def refresh(self):
        """å¾è³‡æ–™åº«è¼‰å…¥æ‰€æœ‰ç”¢å“è³‡è¨Š"""
        try:
            with engine.connect() as conn:
                query = text("""
                    SELECT product_id, product_name, category, status, supplier 
                    FROM product_master_a
                """)
                df = pd.read_sql(query, conn)
                
                self.products = {
                    row['product_id']: {
                        'name': row['product_name'],
                        'category': row['category'],
                        'status': row['status'],
                        'supplier': row['supplier']
                    }
                    for _, row in df.iterrows()
                }
                self.last_refresh = datetime.now()
                logger.info(f"ğŸ“¦ è¼‰å…¥ {len(self.products)} å€‹ç”¢å“åˆ°å¿«å–")
        except Exception as e:
            logger.error(f"è¼‰å…¥ç”¢å“å¿«å–å¤±æ•—: {e}")

product_cache = ProductCache()

# ============== è¡¨æ ¼é…ç½® ==============
TABLE_CONFIGS = {
    'product_master_a': {
        'index_name': 'erp-products',
        'id_field': 'product_id',
        'type': 'product',
        'page_size': PAGE_SIZE
    },
    'product_warehouse_b': {
        'index_name': 'erp-warehouse',
        'id_field': 'id',
        'type': 'warehouse',
        'page_size': PAGE_SIZE
    },
    'customer_complaint_c': {
        'index_name': 'erp-complaints',
        'id_field': 'complaint_id',
        'type': 'complaint',
        'page_size': PAGE_SIZE
    },
    # æ–°å¢æŠ€è¡“æ–‡æª”è¡¨é…ç½®
    'technical_documents': {
        'index_name': 'erp-tech-docs',
        'id_field': 'doc_id',
        'type': 'tech_doc',
        'page_size': 1000  # æŠ€è¡“æ–‡æª”è¼ƒå¤§ï¼Œæ¸›å°‘é é¢å¤§å°
    }
}

# ============== åŒæ­¥å‡½æ•¸ ==============
def sync_technical_documents(es, state: Dict, force_full: bool = False) -> Dict:
    """
    åŒæ­¥æŠ€è¡“æ–‡æª”è¡¨åˆ° Elasticsearch
    
    ç‰¹åˆ¥è™•ç†ï¼š
    1. JSON æ¬„ä½è§£æï¼ˆproduct_ids, keywords, metadataï¼‰
    2. æ—¥æœŸæ ¼å¼è½‰æ›
    3. å¤§æ–‡æœ¬å…§å®¹è™•ç†
    """
    table = 'technical_documents'
    config = TABLE_CONFIGS[table]
    index_name = config['index_name']
    
    # ç¢ºä¿ç´¢å¼•å­˜åœ¨
    ensure_index(es, index_name)
    
    # æª¢æŸ¥ä¸Šæ¬¡åŒæ­¥æ™‚é–“
    last_sync = state.get(table, {}).get('last_sync')
    
    if force_full or not last_sync:
        logger.info(f"ğŸ”„ åŸ·è¡Œå…¨é‡åŒæ­¥: {table}")
        where_clause = "1=1"
    else:
        logger.info(f"ğŸ”„ åŸ·è¡Œå¢é‡åŒæ­¥: {table} (å¾ {last_sync} é–‹å§‹)")
        where_clause = f"updated_at > '{last_sync}'"
    
    try:
        # è¨ˆç®—ç¸½æ•¸
        with engine.connect() as conn:
            count_query = text(f"SELECT COUNT(*) as cnt FROM {table} WHERE {where_clause}")
            total = conn.execute(count_query).scalar()
            
            if total == 0:
                logger.info(f"  æ²’æœ‰éœ€è¦åŒæ­¥çš„è¨˜éŒ„")
                return state
            
            logger.info(f"  éœ€è¦åŒæ­¥ {total} ç­†è¨˜éŒ„")
            
            # åˆ†é æŸ¥è©¢
            offset = 0
            synced = 0
            errors = 0
            
            while offset < total:
                # æŸ¥è©¢è³‡æ–™
                query = text(f"""
                    SELECT 
                        doc_id, doc_type, doc_number, title,
                        product_ids, revision, issue_date,
                        department, author, content, summary,
                        keywords, metadata, file_path, file_hash,
                        page_count, file_size, created_at, updated_at
                    FROM {table}
                    WHERE {where_clause}
                    ORDER BY updated_at
                    LIMIT :limit OFFSET :offset
                """)
                
                df = pd.read_sql(query, conn, params={'limit': config['page_size'], 'offset': offset})
                
                if df.empty:
                    break
                
                # è™•ç†è³‡æ–™
                docs = []
                for _, row in df.iterrows():
                    try:
                        # è§£æ JSON æ¬„ä½
                        product_ids = json.loads(row['product_ids']) if row['product_ids'] else []
                        keywords = json.loads(row['keywords']) if row['keywords'] else []
                        metadata = json.loads(row['metadata']) if row['metadata'] else {}
                        
                        # æº–å‚™æ–‡æª”
                        doc = {
                            "_index": index_name,
                            "_id": row['doc_id'],
                            "_source": {
                                "doc_id": row['doc_id'],
                                "doc_type": row['doc_type'],
                                "doc_number": row['doc_number'],
                                "title": row['title'],
                                "product_ids": product_ids,  # ES æœƒè‡ªå‹•è™•ç†é™£åˆ—
                                "revision": row['revision'],
                                "issue_date": row['issue_date'].isoformat() if pd.notna(row['issue_date']) else None,
                                "department": row['department'],
                                "author": row['author'],
                                "content": row['content'][:100000] if row['content'] else "",  # é™åˆ¶å…§å®¹é•·åº¦
                                "summary": row['summary'],
                                "keywords": keywords,  # ES æœƒè‡ªå‹•è™•ç†é™£åˆ—
                                "metadata": metadata,  # ES æœƒä¿æŒç‚ºç‰©ä»¶
                                "file_path": row['file_path'],
                                "file_hash": row['file_hash'],
                                "page_count": int(row['page_count']) if pd.notna(row['page_count']) else 0,
                                "file_size": int(row['file_size']) if pd.notna(row['file_size']) else 0,
                                "created_at": row['created_at'].isoformat() if pd.notna(row['created_at']) else None,
                                "updated_at": row['updated_at'].isoformat() if pd.notna(row['updated_at']) else None,
                                
                                # æ–°å¢æœå°‹å„ªåŒ–æ¬„ä½
                                "searchable_content": f"{row['title']} {row['summary'] or ''} {' '.join(keywords)}",
                                "type": row['doc_type']  # ä¿æŒèˆ‡å…¶ä»–ç´¢å¼•ä¸€è‡´
                            }
                        }
                        docs.append(doc)
                        
                    except Exception as e:
                        logger.error(f"è™•ç†æ–‡æª” {row['doc_id']} å¤±æ•—: {e}")
                        errors += 1
                        continue
                
                # æ‰¹æ¬¡å¯«å…¥ ES
                if docs:
                    success, failed = 0, 0
                    for ok, result in parallel_bulk(
                        es,
                        docs,
                        thread_count=PARALLEL_THREADS,
                        chunk_size=BATCH_SIZE,
                        raise_on_error=False
                    ):
                        if ok:
                            success += 1
                        else:
                            failed += 1
                            logger.error(f"å¯«å…¥å¤±æ•—: {result}")
                    
                    synced += success
                    errors += failed
                    logger.info(f"  é€²åº¦: {synced}/{total} (æˆåŠŸ: {success}, å¤±æ•—: {failed})")
                
                offset += config['page_size']
            
            # æ›´æ–°ç‹€æ…‹
            state[table] = {
                'last_sync': datetime.now().isoformat(),
                'total_synced': synced,
                'errors': errors
            }
            
            logger.info(f"âœ… {table} åŒæ­¥å®Œæˆ: {synced} ç­†æˆåŠŸ, {errors} ç­†å¤±æ•—")
            
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥ {table} å¤±æ•—: {e}")
    
    return state

def sync_table(es, table: str, config: Dict, state: Dict, force_full: bool = False) -> Dict:
    """åŒæ­¥å–®ä¸€è¡¨æ ¼ï¼ˆåŸæœ‰é‚è¼¯ï¼‰"""
    index_name = config['index_name']
    
    # ç¢ºä¿ç´¢å¼•å­˜åœ¨
    ensure_index(es, index_name)
    
    # æª¢æŸ¥ä¸Šæ¬¡åŒæ­¥æ™‚é–“
    last_sync = state.get(table, {}).get('last_sync')
    
    if force_full or not last_sync:
        logger.info(f"ğŸ”„ åŸ·è¡Œå…¨é‡åŒæ­¥: {table}")
        where_clause = "1=1"
    else:
        logger.info(f"ğŸ”„ åŸ·è¡Œå¢é‡åŒæ­¥: {table} (å¾ {last_sync} é–‹å§‹)")
        where_clause = f"updated_at > '{last_sync}'"
    
    # ... åŸæœ‰åŒæ­¥é‚è¼¯ä¿æŒä¸è®Š ...
    # ï¼ˆé€™è£¡çœç•¥åŸæœ‰ç¨‹å¼ç¢¼ï¼Œä¿æŒåŸæ¨£ï¼‰
    
    return state

# ============== ä¸»ç¨‹å¼ ==============
def main():
    """ä¸»ç¨‹å¼"""
    logger.info("=" * 60)
    logger.info("ğŸš€ è³‡æ–™åŒæ­¥æœå‹™å•Ÿå‹•")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}, é é¢å¤§å°: {PAGE_SIZE}")
    logger.info(f"åŸ·è¡Œç·’æ•¸: {PARALLEL_THREADS}, åŒæ­¥é–“éš”: {SLEEP_SECONDS}ç§’")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–
    es = get_es_client()
    
    # è¼‰å…¥ç‹€æ…‹
    state = {}
    if os.path.exists(STATE_PATH):
        try:
            with open(STATE_PATH, 'r') as f:
                state = json.load(f)
            logger.info(f"è¼‰å…¥ç‹€æ…‹: {state}")
        except:
            state = {}
    
    # è¼‰å…¥ç”¢å“å¿«å–
    product_cache.refresh()
    
    # ä¸»å¾ªç’°
    while True:
        try:
            logger.info("\n" + "="*40)
            logger.info(f"â° é–‹å§‹åŒæ­¥é€±æœŸ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # åŒæ­¥åŸæœ‰è¡¨æ ¼
            for table, config in TABLE_CONFIGS.items():
                if table == 'technical_documents':
                    # æŠ€è¡“æ–‡æª”ä½¿ç”¨ç‰¹æ®ŠåŒæ­¥å‡½æ•¸
                    state = sync_technical_documents(es, state)
                else:
                    # åŸæœ‰è¡¨æ ¼ä½¿ç”¨åŸåŒæ­¥å‡½æ•¸
                    state = sync_table(es, table, config, state)
            
            # å„²å­˜ç‹€æ…‹
            with open(STATE_PATH, 'w') as f:
                json.dump(state, f, indent=2)
            
            # æ¯10æ¬¡åŒæ­¥æ›´æ–°ä¸€æ¬¡ç”¢å“å¿«å–
            if datetime.now().timestamp() % 10 == 0:
                product_cache.refresh()
            
            logger.info(f"ğŸ’¤ ä¼‘çœ  {SLEEP_SECONDS} ç§’...")
            time.sleep(SLEEP_SECONDS)
            
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰...")
            break
        except Exception as e:
            logger.error(f"åŒæ­¥éŒ¯èª¤: {e}", exc_info=True)
            time.sleep(60)  # éŒ¯èª¤å¾Œç­‰å¾…è¼ƒé•·æ™‚é–“

if __name__ == "__main__":
    main()
