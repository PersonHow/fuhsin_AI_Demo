#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SQL æ™ºèƒ½è§£æå™¨ - å°‡ SQL INSERT èªå¥è§£ææˆçµæ§‹åŒ–æ•¸æ“š
æ›¿æ›åŸæœ‰çš„ auto_importer.py
"""
import os, re, time, json, hashlib
from datetime import datetime, timezone
from pathlib import Path
import requests
from opencc import OpenCC

ES_URL   = os.environ.get("ES_URL",  "http://es01:9200")
ES_USER  = os.environ.get("ES_USER", "elastic")
ES_PASS  = os.environ.get("ES_PASS", "admin@12345")
IMPORT_DIR = Path(os.environ.get("IMPORT_DIR", "/data/import"))
BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "500"))
SLEEP_SEC  = int(os.environ.get("SLEEP", "5"))

session = requests.Session()
session.auth = (ES_USER, ES_PASS)
session.headers.update({"Content-Type": "application/json"})

cc_s2t = OpenCC("s2t")
cc_t2s = OpenCC("t2s")

def log(msg): 
    print(time.strftime("%Y-%m-%d %H:%M:%S"), msg, flush=True)

class SQLParser:
    """SQL INSERT èªå¥è§£æå™¨"""
    
    @staticmethod
    def parse_insert_statement(sql_text):
        """è§£æ INSERT INTO èªå¥ï¼Œæå–è¡¨åã€æ¬„ä½åå’Œæ•¸æ“š"""
        # æ¸…ç† SQL
        clean_sql = re.sub(r'--[^\n]*', '', sql_text)  # ç§»é™¤è¡Œè¨»é‡‹
        clean_sql = re.sub(r'/\*.*?\*/', '', clean_sql, flags=re.DOTALL)  # ç§»é™¤å¡Šè¨»é‡‹
        clean_sql = re.sub(r'\s+', ' ', clean_sql).strip()  # è¦ç¯„åŒ–ç©ºç™½
        
        # è§£æ INSERT INTO table_name(columns) VALUES
        insert_pattern = r'insert\s+into\s+([^\s(]+)\s*\(([^)]+)\)\s+values\s*(.+)'
        match = re.match(insert_pattern, clean_sql, re.IGNORECASE)
        
        if not match:
            return None
        
        table_name = match.group(1).strip()
        columns_str = match.group(2).strip()
        values_str = match.group(3).strip()
        
        # è§£ææ¬„ä½å
        columns = [col.strip() for col in columns_str.split(',')]
        
        # è§£æ VALUES éƒ¨åˆ†çš„å¤šè¡Œæ•¸æ“š
        records = SQLParser.parse_values_section(values_str)
        
        return {
            'table_name': table_name,
            'columns': columns,
            'records': records
        }
    
    @staticmethod
    def parse_values_section(values_str):
        """è§£æ VALUES éƒ¨åˆ†ï¼Œæ”¯æŒå¤šè¡Œè¨˜éŒ„"""
        records = []
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æ‰¾åˆ°æ‰€æœ‰çš„ (...) è¨˜éŒ„
        pattern = r'\(([^)]+(?:\([^)]*\)[^)]*)*)\)'
        matches = re.findall(pattern, values_str)
        
        for match in matches:
            values = SQLParser.parse_single_record(match)
            if values:
                records.append(values)
        
        return records
    
    @staticmethod
    def parse_single_record(record_str):
        """è§£æå–®å€‹è¨˜éŒ„çš„å€¼"""
        values = []
        current_value = ""
        in_quotes = False
        quote_char = None
        paren_depth = 0
        i = 0
        
        while i < len(record_str):
            char = record_str[i]
            
            if char in ('"', "'") and (i == 0 or record_str[i-1] != '\\'):
                if not in_quotes:
                    in_quotes = True
                    quote_char = char
                elif char == quote_char:
                    in_quotes = False
                    quote_char = None
                current_value += char
            elif char == '(' and not in_quotes:
                paren_depth += 1
                current_value += char
            elif char == ')' and not in_quotes:
                paren_depth -= 1
                current_value += char
            elif char == ',' and not in_quotes and paren_depth == 0:
                values.append(SQLParser.clean_value(current_value.strip()))
                current_value = ""
            else:
                current_value += char
            
            i += 1
        
        # æ·»åŠ æœ€å¾Œä¸€å€‹å€¼
        if current_value.strip():
            values.append(SQLParser.clean_value(current_value.strip()))
        
        return values
    
    @staticmethod
    def clean_value(value):
        """æ¸…ç†å’Œè½‰æ›å€¼"""
        value = value.strip()
        
        # NULL å€¼
        if value.lower() == 'null':
            return None
        
        # å»é™¤ N' å‰ç¶´ï¼ˆUnicode å­—ç¬¦ä¸²ï¼‰
        if value.startswith(("N'", "n'")):
            value = value[2:-1] if value.endswith("'") else value[2:]
            return value
        
        # æ™®é€šå­—ç¬¦ä¸²
        if value.startswith("'") and value.endswith("'"):
            return value[1:-1]
        
        # æ•¸å­—
        try:
            if '.' in value:
                return float(value)
            else:
                return int(value)
        except ValueError:
            pass
        
        # æ—¥æœŸ/æ™‚é–“æˆ³
        if value.upper().startswith(('DATE', 'TIMESTAMP')):
            date_match = re.search(r"'([^']+)'", value)
            if date_match:
                return date_match.group(1)
        
        return value

def ensure_structured_index_template():
    """å»ºç«‹çµæ§‹åŒ–ç´¢å¼•æ¨¡æ¿"""
    template = {
        "index_patterns": ["erp-*"],
        "template": {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "ik_analyzer": {
                            "type": "ik_max_word"
                        },
                        "ik_search": {
                            "type": "ik_smart"
                        }
                    }
                }
            },
            "mappings": {
                "dynamic_templates": [
                    {
                        "field_strings": {
                            "match": "field_*",
                            "mapping": {
                                "type": "text",
                                "analyzer": "ik_analyzer",
                                "search_analyzer": "ik_search",
                                "fields": {
                                    "keyword": {"type": "keyword", "ignore_above": 256}
                                }
                            }
                        }
                    }
                ],
                "properties": {
                    "@timestamp": {"type": "date"},
                    "metadata": {
                        "properties": {
                            "source_file": {"type": "keyword"},
                            "table_name": {"type": "keyword"},
                            "record_index": {"type": "integer"},
                            "total_records": {"type": "integer"}
                        }
                    },
                    "searchable_content": {
                        "type": "text",
                        "analyzer": "ik_analyzer", 
                        "search_analyzer": "ik_search"
                    },
                    "all_content": {
                        "type": "text",
                        "analyzer": "ik_analyzer",
                        "search_analyzer": "ik_search"
                    }
                }
            }
        }
    }
    
    try:
        r = session.put(f"{ES_URL}/_index_template/erp-template", 
                       data=json.dumps(template), timeout=30)
        r.raise_for_status()
        log("ç´¢å¼•æ¨¡æ¿å·²å»ºç«‹: erp-template")
    except requests.exceptions.HTTPError as e:
        log(f"å»ºç«‹ç´¢å¼•æ¨¡æ¿å¤±æ•—: {e}")
        log(f"å›æ‡‰å…§å®¹: {e.response.text}")
        # å˜—è©¦ä½¿ç”¨ç°¡åŒ–ç‰ˆæ¨¡æ¿
        simple_template = {
            "index_patterns": ["erp-*"],
            "template": {
                "mappings": {
                    "properties": {
                        "@timestamp": {"type": "date"},
                        "metadata": {
                            "properties": {
                                "source_file": {"type": "keyword"},
                                "table_name": {"type": "keyword"},
                                "record_index": {"type": "integer"},
                                "total_records": {"type": "integer"}
                            }
                        }
                    }
                }
            }
        }
        try:
            r2 = session.put(f"{ES_URL}/_index_template/erp-simple-template", 
                           data=json.dumps(simple_template), timeout=30)
            r2.raise_for_status()
            log("ç°¡åŒ–ç´¢å¼•æ¨¡æ¿å·²å»ºç«‹: erp-simple-template")
        except Exception as e2:
            log(f"ç°¡åŒ–æ¨¡æ¿ä¹Ÿå¤±æ•—: {e2}")
            log("å°‡ä½¿ç”¨é»˜èªæ˜ å°„ç¹¼çºŒåŸ·è¡Œ")

def create_structured_documents(filename, parsed_data, raw_sql):
    """å°‡è§£æå¾Œçš„æ•¸æ“šè½‰æ›ç‚ºçµæ§‹åŒ–æ–‡æª”"""
    if not parsed_data:
        return []
    
    table_name = parsed_data['table_name']
    columns = parsed_data['columns']
    records = parsed_data['records']
    
    documents = []
    
    for record_idx, record_values in enumerate(records):
        # å»ºç«‹åŸºæœ¬æ–‡æª”çµæ§‹
        doc = {
            "@timestamp": datetime.now(timezone.utc).isoformat(),
            "metadata": {
                "source_file": filename,
                "table_name": table_name,
                "record_index": record_idx,
                "total_records": len(records)
            }
            # ç§»é™¤ raw_sql å­—æ®µä»¥ä¿æŒçµæœæ¸…çˆ½
        }
        
        # å°‡æ¯å€‹æ¬„ä½çš„å€¼æ·»åŠ åˆ°æ–‡æª”ä¸­
        searchable_content = []
        
        for col_idx, column_name in enumerate(columns):
            if col_idx < len(record_values):
                value = record_values[col_idx]
                
                if value is not None:
                    # æ¸…ç†æ¬„ä½åï¼ˆç§»é™¤å¯èƒ½çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
                    clean_col_name = re.sub(r'[^\w]', '_', column_name.lower())
                    field_key = f"field_{clean_col_name}"
                    
                    # å­˜å„²åŸå§‹å€¼
                    doc[field_key] = str(value)
                    
                    # å¦‚æœæ˜¯æ–‡æœ¬ï¼ŒåŠ å…¥ç¹ç°¡è½‰æ›
                    if isinstance(value, str) and len(value) > 0:
                        # è½‰ç¹é«”
                        traditional = cc_s2t.convert(str(value))
                        # è½‰ç°¡é«”  
                        simplified = cc_t2s.convert(str(value))
                        
                        # åŠ å…¥å¯æœç´¢å…§å®¹
                        searchable_content.extend([str(value), traditional, simplified])
        
        # æ·»åŠ ä¾¿æ–¼æœç´¢çš„ç¶œåˆå…§å®¹å­—æ®µ
        doc["searchable_content"] = " ".join(set(searchable_content))
        
        # æ–°å¢ï¼šå…¨å…§å®¹æœç´¢å­—æ®µï¼ˆåŒ…å«æ‰€æœ‰æ–‡æœ¬å…§å®¹ï¼‰
        all_text_content = []
        for key, value in doc.items():
            if key.startswith('field_') and isinstance(value, str):
                all_text_content.append(str(value))
        doc["all_content"] = " ".join(set(all_text_content + searchable_content))
        
        # ç”Ÿæˆæ–‡æª” ID
        doc_id = hashlib.sha256(f"{filename}::{table_name}::{record_idx}".encode()).hexdigest()
        
        documents.append((doc_id, doc))
    
    return documents

def get_index_name(table_name):
    """æ ¹æ“šè¡¨åç”Ÿæˆç´¢å¼•åç¨±"""
    # å°‡ schema.table æ ¼å¼è½‰æ›ç‚º erp-schema-table-YYYY.MM.DD
    clean_table = re.sub(r'[^\w.]', '_', table_name.lower())
    clean_table = clean_table.replace('.', '-')
    date_suffix = datetime.now(timezone.utc).strftime("%Y.%m.%d")
    return f"erp-{clean_table}-{date_suffix}"

def bulk_index_documents(index_name, documents):
    """æ‰¹é‡ç´¢å¼•æ–‡æª”"""
    if not documents:
        return
    
    lines = []
    for doc_id, doc in documents:
        lines.append(json.dumps({"index": {"_index": index_name, "_id": doc_id}}, ensure_ascii=False))
        lines.append(json.dumps(doc, ensure_ascii=False))
    
    payload = "\n".join(lines) + "\n"
    
    r = session.post(f"{ES_URL}/_bulk", 
                    data=payload.encode("utf-8"), 
                    headers={"Content-Type": "application/x-ndjson"}, 
                    timeout=60)
    r.raise_for_status()
    
    result = r.json()
    if result.get("errors"):
        errors = [item for item in result.get("items", []) 
                 if item.get("index", {}).get("error")]
        log(f"âš ï¸ ç´¢å¼•éŒ¯èª¤ç¯„ä¾‹: {errors[:2]}")
    
    log(f"âœ… æˆåŠŸç´¢å¼• {len(documents)} ä»½æ–‡æª”åˆ° {index_name}")

def robust_read_text(path):
    """å¼·å¥çš„æ–‡æœ¬è®€å–"""
    b = path.read_bytes()
    encodings = ["utf-8", "utf-8-sig", "cp950", "big5", "utf-16le", "utf-16be"]
    
    for encoding in encodings:
        try:
            return b.decode(encoding).replace("\r", "")
        except:
            continue
    
    return b.decode("utf-8", "ignore").replace("\r", "")

def process_sql_file(file_path):
    """è™•ç† SQL æ–‡ä»¶"""
    log(f"ğŸ”„ è™•ç†æ–‡ä»¶: {file_path.name}")
    
    try:
        # è®€å–æ–‡ä»¶å…§å®¹
        content = robust_read_text(file_path)
        
        # åˆ†å‰² SQL èªå¥
        statements = []
        for stmt in re.split(r';\s*(?:\n|$)', content):
            stmt = stmt.strip()
            if stmt and not stmt.startswith('--'):
                statements.append(stmt)
        
        total_docs = 0
        
        for stmt_idx, statement in enumerate(statements):
            if 'insert into' in statement.lower():
                # è§£æ INSERT èªå¥
                parsed = SQLParser.parse_insert_statement(statement)
                
                if parsed:
                    # å»ºç«‹çµæ§‹åŒ–æ–‡æª”
                    docs = create_structured_documents(file_path.name, parsed, statement)
                    
                    if docs:
                        # ç²å–ç´¢å¼•åç¨±
                        index_name = get_index_name(parsed['table_name'])
                        
                        # æ‰¹é‡ç´¢å¼•
                        for i in range(0, len(docs), BATCH_SIZE):
                            batch = docs[i:i + BATCH_SIZE]
                            bulk_index_documents(index_name, batch)
                        
                        total_docs += len(docs)
                        log(f"ğŸ“Š è¡¨ {parsed['table_name']}: {len(docs)} è¨˜éŒ„")
                else:
                    log(f"âš ï¸ ç„¡æ³•è§£æèªå¥ {stmt_idx + 1}")
        
        # é‡å‘½åç‚º .done
        done_path = file_path.with_suffix(file_path.suffix + ".done")
        try:
            file_path.rename(done_path)
            log(f"âœ… å®Œæˆè™•ç†: {done_path.name} (å…± {total_docs} è¨˜éŒ„)")
        except Exception as e:
            log(f"âš ï¸ é‡å‘½åå¤±æ•—: {e}")
            
    except Exception as e:
        log(f"âŒ è™•ç†æ–‡ä»¶å¤±æ•— {file_path.name}: {e}")

def main():
    """ä¸»ç¨‹åº"""
    IMPORT_DIR.mkdir(parents=True, exist_ok=True)
    ensure_structured_index_template()
    
    log(f"ğŸ‘ï¸ ç›£æ§ç›®éŒ„: {IMPORT_DIR}")
    log("ğŸš€ SQL æ™ºèƒ½è§£æå™¨å•Ÿå‹•")
    
    while True:
        # å°‹æ‰¾å¾…è™•ç†çš„ .sql æ–‡ä»¶
        sql_files = sorted([
            f for f in IMPORT_DIR.glob("*.sql") 
            if not f.name.endswith(".done")
        ])
        
        for file_path in sql_files:
            process_sql_file(file_path)
        
        if not sql_files:
            log("ğŸ˜´ æ²’æœ‰æ–°æ–‡ä»¶ï¼Œç­‰å¾…ä¸­...")
        
        time.sleep(SLEEP_SEC)

if __name__ == "__main__":
    main()
