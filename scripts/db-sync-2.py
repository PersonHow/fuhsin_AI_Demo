#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å„ªåŒ–ç‰ˆ MySQL to Elasticsearch ç›´æ¥åŒæ­¥æœå‹™
- æ”¯æ´å¤§é‡è³‡æ–™è™•ç†
- åˆ†é æŸ¥è©¢é¿å…è¨˜æ†¶é«”æº¢å‡º
- å¹³è¡Œè™•ç†æå‡æ•ˆèƒ½
- æ™ºèƒ½ç”¢å“é—œè¯
"""

import os, json, time, re, logging, pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Generator
from sqlalchemy import create_engine, text
from elasticsearch import Elasticsearch
from elasticsearch.helpers import parallel_bulk, BulkIndexError

# ============== é…ç½® ==============
DB_URL = os.getenv("DB_URL", "mysql+pymysql://root:root@mysql:3306/fuhsin_erp_demo")
ES_URL = os.getenv("ES_URL", "http://elasticsearch:9200")
ES_USER = os.getenv("ES_USER", "elastic")
ES_PASS = os.getenv("ES_PASS", "admin@12345")

# æ•ˆèƒ½ç›¸é—œé…ç½®
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "2000"))         # æ¯æ‰¹è™•ç†æ•¸é‡
PAGE_SIZE = int(os.getenv("PAGE_SIZE", "5000"))           # åˆ†é æŸ¥è©¢å¤§å°
PARALLEL_THREADS = int(os.getenv("PARALLEL_THREADS", "4")) # å¹³è¡ŒåŸ·è¡Œç·’æ•¸
SLEEP_SECONDS = int(os.getenv("SLEEP_SECONDS", "30"))     # åŒæ­¥é–“éš”

# æª”æ¡ˆè·¯å¾‘
STATE_PATH = "/state/.sync_state.json"
LOG_PATH = "/logs/db-sync/db_sync.log"

# ============== æ—¥èªŒè¨­å®š ==============
os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============== è³‡æ–™åº«é€£ç·šæ±  ==============
engine = create_engine(
    DB_URL,
    pool_size=20,           # é€£ç·šæ± å¤§å°
    max_overflow=10,        # æœ€å¤§æº¢å‡ºé€£ç·š
    pool_pre_ping=True,     # é€£ç·šå‰æª¢æŸ¥
    pool_recycle=3600,      # é€£ç·šå›æ”¶æ™‚é–“
    echo=False,
    connect_args={
        "charset": "utf8mb4",
        "connect_timeout": 10,
        "init_command": (
            "SET SESSION sql_mode = "
            "REPLACE(REPLACE(@@sql_mode,'NO_ZERO_DATE',''),'NO_ZERO_IN_DATE','')"
        )
    }
)

# ============== Elasticsearch å®¢æˆ¶ç«¯ ==============
def get_es_client():
    """å»ºç«‹ Elasticsearch å®¢æˆ¶ç«¯"""
    return Elasticsearch(
        [ES_URL],
        basic_auth=(ES_USER, ES_PASS) if ES_USER and ES_PASS else None,
        verify_certs=False,
        timeout=30,
        max_retries=3,
        retry_on_timeout=True
    )

# ============== ç´¢å¼•ç®¡ç† ==============
def ensure_index(es, index_name):
    """ç¢ºä¿ç´¢å¼•å­˜åœ¨ä¸¦è¨­å®šæ­£ç¢ºçš„ mapping"""
    if not es.indices.exists(index=index_name):
        mapping = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "refresh_interval": "30s",  # å»¶é²åˆ·æ–°æå‡å¯«å…¥æ•ˆèƒ½
                "index": {
                    "max_result_window": 50000,  # å¢åŠ æŸ¥è©¢çª—å£
                    "max_terms_count": 65536     # å¢åŠ  terms æŸ¥è©¢é™åˆ¶
                },
                "analysis": {
                    "analyzer": {
                        "ik_smart": {
                            "type": "custom",
                            "tokenizer": "ik_smart"
                        },
                        "ik_max_word": {
                            "type": "custom", 
                            "tokenizer": "ik_max_word"
                        }
                    },
                    "normalizer": {
                        "lowercase_normalizer": {
                            "type": "custom",
                            "filter": ["lowercase", "asciifolding"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # åŸºæœ¬æ¬„ä½
                    "type": {"type": "keyword"},
                    "id": {"type": "keyword"},
                    "doc_id": {"type": "keyword"},
                    
                    # æ–‡å­—æœå°‹æ¬„ä½
                    "title": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart",
                        "fields": {
                            "keyword": {"type": "keyword", "ignore_above": 256}
                        }
                    },
                    "content": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "all_content": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    "searchable_content": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "search_analyzer": "ik_smart"
                    },
                    
                    # ç²¾ç¢ºæœå°‹æ¬„ä½
                    "product_ids": {"type": "keyword"},
                    "status": {"type": "keyword", "normalizer": "lowercase_normalizer"},
                    "warehouse_location": {"type": "keyword"},
                    "severity": {"type": "keyword"},
                    
                    # æ™‚é–“æ¬„ä½
                    "updated_at": {
                        "type": "date",
                        "format": "strict_date_time||epoch_millis||yyyy-MM-dd HH:mm:ss"
                    },
                    
                    # å…ƒè³‡æ–™
                    "metadata": {
                        "type": "object",
                        "enabled": True,
                        "dynamic": True
                    },
                    
                    # å‘é‡æ¬„ä½ï¼ˆç‚ºæœªä¾†é ç•™ï¼‰
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 1536,
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }
        es.indices.create(index=index_name, body=mapping)
        logger.info(f"âœ… å‰µå»ºç´¢å¼•: {index_name}")
    else:
        # æ›´æ–°ç¾æœ‰ç´¢å¼•çš„è¨­å®š
        es.indices.put_settings(
            index=index_name,
            body={"index": {"refresh_interval": "30s"}}
        )

# ============== ç”¢å“å¿«å– ==============
class ProductCache:
    """ç”¢å“è³‡è¨Šå¿«å–ï¼Œç”¨æ–¼å¿«é€Ÿé—œè¯æŸ¥è©¢"""
    
    def __init__(self):
        self.products = {}
        self.last_refresh = None
        
    def refresh(self):
        """å¾è³‡æ–™åº«è¼‰å…¥æ‰€æœ‰ç”¢å“è³‡è¨Š"""
        try:
            with engine.connect() as conn:
                query = text("""
                    SELECT product_id, product_name, category, status, supplier 
                    FROM product_master_a
                """)
                df = pd.read_sql(query, conn)
                
                self.products = {
                    row['product_id']: {
                        'name': row['product_name'],
                        'category': row['category'],
                        'status': row['status'],
                        'supplier': row['supplier']
                    }
                    for _, row in df.iterrows()
                }
                self.last_refresh = datetime.now()
                logger.info(f"ğŸ“¦ è¼‰å…¥ {len(self.products)} å€‹ç”¢å“åˆ°å¿«å–")
        except Exception as e:
            logger.error(f"è¼‰å…¥ç”¢å“å¿«å–å¤±æ•—: {e}")
    
    def get(self, product_id: str) -> Optional[Dict]:
        """å–å¾—ç”¢å“è³‡è¨Š"""
        # æ¯å°æ™‚æ›´æ–°ä¸€æ¬¡
        if not self.last_refresh or (datetime.now() - self.last_refresh).seconds > 3600:
            self.refresh()
        return self.products.get(product_id)
    
    def extract_product_ids(self, text: str) -> List[str]:
        """å¾æ–‡å­—ä¸­æå–ç”¢å“ ID"""
        # æ”¯æ´ P é–‹é ­çš„ç”¢å“ç·¨è™Ÿ
        pattern = r'\b[P]\d{3}\b'
        found_ids = list(set(re.findall(pattern, str(text))))
        
        # é©—è­‰ ID æ˜¯å¦å­˜åœ¨
        valid_ids = [pid for pid in found_ids if pid in self.products]
        
        # å¦‚æœæåˆ°ç”¢å“åç¨±ï¼Œä¹Ÿæ‰¾å‡ºå°æ‡‰ ID
        for pid, info in self.products.items():
            if info['name'] and info['name'] in str(text):
                if pid not in valid_ids:
                    valid_ids.append(pid)
        
        return valid_ids

# å…¨åŸŸç”¢å“å¿«å–
product_cache = ProductCache()

# ============== ç‹€æ…‹ç®¡ç† ==============
def load_state():
    """è¼‰å…¥åŒæ­¥ç‹€æ…‹"""
    if os.path.exists(STATE_PATH):
        with open(STATE_PATH, "r") as f:
            return json.load(f)
    return {}

def save_state(state):
    """å„²å­˜åŒæ­¥ç‹€æ…‹"""
    os.makedirs(os.path.dirname(STATE_PATH), exist_ok=True)
    with open(STATE_PATH, "w") as f:
        json.dump(state, f, indent=2, default=str)

# ============== è³‡æ–™è™•ç† ==============
def process_product_master(df):
    """è™•ç†ç”¢å“ä¸»æª”è³‡æ–™"""
    for _, row in df.iterrows():
        product_id = str(row['product_id'])
        
        # çµ„åˆå„ç¨®æœå°‹æ¬„ä½
        all_fields = [
            product_id,
            row.get('product_name', ''),
            row.get('product_model', ''),
            row.get('category', ''),
            row.get('supplier', ''),
            row.get('status', '')
        ]
        
        doc = {
            "_id": f"product_{product_id}",
            "_index": "erp-products",
            "type": "product_master",
            "id": product_id,
            "doc_id": f"product_{product_id}",
            "title": f"[{product_id}] {row.get('product_name', '')} ({row.get('product_model', '')})",
            "content": f"å‹è™Ÿ: {row.get('product_model')}; åˆ†é¡: {row.get('category')}; ä¾›æ‡‰å•†: {row.get('supplier')}; ç‹€æ…‹: {row.get('status')}; åƒ¹æ ¼: {row.get('price')}; åº«å­˜: {row.get('stock_qty')}",
            "all_content": " ".join(str(f) for f in all_fields if f),
            "searchable_content": f"ç”¢å“ç·¨è™Ÿ {product_id} ç”¢å“åç¨± {row.get('product_name')} å‹è™Ÿ {row.get('product_model')} åˆ†é¡ {row.get('category')}",
            "product_ids": [product_id],
            "status": row.get('status'),
            "metadata": {
                "product_name": row.get('product_name'),
                "product_model": row.get('product_model'),
                "price": float(row['price']) if pd.notna(row.get('price')) else None,
                "stock_qty": int(row['stock_qty']) if pd.notna(row.get('stock_qty')) else None,
                "category": row.get('category'),
                "supplier": row.get('supplier'),
                "manufacture_date": str(row.get('manufacture_date')) if row.get('manufacture_date') else None
            },
            "updated_at": row.get('last_modified', datetime.now())
        }
        
        # å‹•æ…‹æ·»åŠ ç”¢å“ç‰¹å®šæ¬„ä½
        for col in df.columns:
            if col.startswith('field_') and col not in doc['metadata']:
                doc['metadata'][col] = row.get(col)
        
        yield doc

def process_warehouse(df):
    """è™•ç†å€‰åº«è³‡æ–™"""
    for _, row in df.iterrows():
        product_id = str(row.get('product_id', ''))
        location = row.get('warehouse_location', '')
        
        # å¾å¿«å–å–å¾—ç”¢å“è³‡è¨Š
        product_info = product_cache.get(product_id)
        product_name = row.get('product_name') or (product_info['name'] if product_info else '')
        
        # æå–ç›¸é—œç”¢å“ ID
        related_ids = product_cache.extract_product_ids(row.get('special_notes', ''))
        all_product_ids = [product_id] if product_id else []
        all_product_ids.extend([pid for pid in related_ids if pid not in all_product_ids])
        
        doc = {
            "_id": f"warehouse_{product_id}_{location.replace(' ', '_')}",
            "_index": "erp-warehouse",
            "type": "warehouse",
            "id": f"{product_id}:{location}",
            "doc_id": f"warehouse_{product_id}_{location.replace(' ', '_')}",
            "title": f"[{product_id}] {product_name} @ {location}",
            "content": f"åº«å­˜æ•¸é‡: {row.get('quantity')}; æœ€ä½åº«å­˜: {row.get('min_stock_level')}; ç®¡ç†äºº: {row.get('manager')}; å‚™è¨»: {row.get('special_notes')}",
            "all_content": f"{product_id} {product_name} {location} {row.get('special_notes')}",
            "searchable_content": f"ç”¢å“ {product_id} {product_name} å€‰åº« {location} æ•¸é‡ {row.get('quantity')}",
            "product_ids": all_product_ids,
            "warehouse_location": location,
            "metadata": {
                "product_id": product_id,
                "product_name": product_name,
                "warehouse_location": location,
                "quantity": int(row['quantity']) if pd.notna(row.get('quantity')) else 0,
                "min_stock_level": int(row['min_stock_level']) if pd.notna(row.get('min_stock_level')) else 0,
                "manager": row.get('manager'),
                "special_notes": row.get('special_notes'),
                "last_inventory_date": str(row.get('last_inventory_date')) if row.get('last_inventory_date') else None
            },
            "updated_at": row.get('last_modified', datetime.now())
        }
        
        yield doc

def process_complaints(df):
    """è™•ç†å®¢è¨´è³‡æ–™"""
    for _, row in df.iterrows():
        complaint_id = str(row['complaint_id'])
        description = row.get('description', '')
        
        # æå–ç”¢å“ ID
        product_ids = product_cache.extract_product_ids(description)
        
        # å–å¾—ç”¢å“åç¨±
        product_names = []
        for pid in product_ids:
            info = product_cache.get(pid)
            if info:
                product_names.append(f"{pid}({info['name']})")
        
        doc = {
            "_id": f"complaint_{complaint_id}",
            "_index": "erp-complaints",
            "type": "complaint",
            "id": complaint_id,
            "doc_id": f"complaint_{complaint_id}",
            "title": f"[{complaint_id}] {row.get('customer_company', '')} - {row.get('complaint_type')} ({row.get('status')})",
            "content": description,
            "all_content": f"{row.get('customer_name')} {row.get('customer_company')} {description} {row.get('complaint_type')}",
            "searchable_content": f"å®¢è¨´ç·¨è™Ÿ {complaint_id} å®¢æˆ¶ {row.get('customer_company')} é¡å‹ {row.get('complaint_type')} {description}",
            "product_ids": product_ids,
            "status": row.get('status'),
            "severity": row.get('severity'),
            "metadata": {
                "complaint_date": str(row.get('complaint_date')) if row.get('complaint_date') else None,
                "customer_name": row.get('customer_name'),
                "customer_company": row.get('customer_company'),
                "complaint_type": row.get('complaint_type'),
                "severity": row.get('severity'),
                "handler": row.get('handler'),
                "resolution_date": str(row.get('resolution_date')) if row.get('resolution_date') else None,
                "related_products": ", ".join(product_names) if product_names else None
            },
            "updated_at": row.get('last_modified', datetime.now())
        }
        
        yield doc

# ============== åˆ†é æŸ¥è©¢ ==============
def fetch_data_in_pages(table: str, since, page_size: int = PAGE_SIZE) -> Generator:
    """åˆ†é æŸ¥è©¢è³‡æ–™ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º"""
    offset = 0
    total_fetched = 0
    
    with engine.connect() as conn:
        while True:
            # å»ºæ§‹æŸ¥è©¢
            if since:
                query = text(f"""
                    SELECT * FROM {table}
                    WHERE last_modified > :since
                    ORDER BY last_modified ASC
                    LIMIT :limit OFFSET :offset
                """)
                params = {'since': since, 'limit': page_size, 'offset': offset}
            else:
                query = text(f"""
                    SELECT * FROM {table}
                    ORDER BY last_modified ASC
                    LIMIT :limit OFFSET :offset
                """)
                params = {'limit': page_size, 'offset': offset}
            
            # åŸ·è¡ŒæŸ¥è©¢
            df = pd.read_sql(query, conn, params=params)
            
            if df.empty:
                break
            
            total_fetched += len(df)
            logger.info(f"ğŸ“Š {table}: å–å¾—ç¬¬ {offset+1}-{offset+len(df)} ç­†è³‡æ–™")
            
            yield df
            
            offset += page_size
            
            # é˜²æ­¢ç„¡é™å¾ªç’°
            if total_fetched >= 1000000:  # æœ€å¤šè™•ç† 100 è¬ç­†
                logger.warning(f"âš ï¸ {table}: å·²è™•ç† 100 è¬ç­†ï¼Œåœæ­¢")
                break

# ============== åŒæ­¥å‡½æ•¸ ==============
def sync_table(table_name: str, processor, es_client, state: Dict) -> bool:
    """åŒæ­¥å–®ä¸€è³‡æ–™è¡¨"""
    since = state.get(table_name)
    
    logger.info(f"ğŸ”„ é–‹å§‹åŒæ­¥ {table_name}ï¼Œèµ·å§‹æ™‚é–“: {since or 'åˆå§‹åŒæ­¥'}")
    
    # ç¢ºä¿ç´¢å¼•å­˜åœ¨
    if table_name == "product_master_a":
        ensure_index(es_client, "erp-products")
    elif table_name == "product_warehouse_b":
        ensure_index(es_client, "erp-warehouse")
    elif table_name == "customer_complaint_c":
        ensure_index(es_client, "erp-complaints")
    
    total_success = 0
    total_failed = 0
    max_timestamp = since
    
    # åˆ†é è™•ç†è³‡æ–™
    for page_df in fetch_data_in_pages(table_name, since):
        # ç”¢ç”Ÿæ–‡æª”
        docs = list(processor(page_df))
        
        if not docs:
            continue
        
        # ä½¿ç”¨ parallel_bulk æå‡æ•ˆèƒ½
        try:
            for success, info in parallel_bulk(
                es_client,
                docs,
                thread_count=PARALLEL_THREADS,
                chunk_size=500,
                raise_on_error=False,
                raise_on_exception=False
            ):
                if success:
                    total_success += 1
                else:
                    total_failed += 1
                    logger.error(f"æ‰¹é‡ç´¢å¼•å¤±æ•—: {info}")
        
        except BulkIndexError as e:
            logger.error(f"æ‰¹é‡ç´¢å¼•éŒ¯èª¤: {e}")
            for error in e.errors:
                logger.error(f"  è©³ç´°éŒ¯èª¤: {error}")
        
        # æ›´æ–°æœ€å¤§æ™‚é–“æˆ³
        if 'last_modified' in page_df.columns:
            page_max = page_df['last_modified'].max()
            if pd.notna(page_max):
                if max_timestamp is None or page_max > pd.Timestamp(max_timestamp):
                    max_timestamp = page_max
    
    # æ›´æ–°ç‹€æ…‹
    if max_timestamp and max_timestamp != since:
        state[table_name] = str(max_timestamp)
        logger.info(f"âœ… {table_name}: æˆåŠŸ {total_success} ç­†, å¤±æ•— {total_failed} ç­†, æ›´æ–°åˆ° {max_timestamp}")
        return True
    elif total_success > 0:
        logger.info(f"âœ… {table_name}: æˆåŠŸ {total_success} ç­†, å¤±æ•— {total_failed} ç­†")
        return True
    else:
        logger.info(f"ğŸ’¤ {table_name}: ç„¡æ–°è³‡æ–™")
        return False

# ============== å¿«é€Ÿæª¢æŸ¥ ==============
def check_recent_changes(minutes: int = 5) -> Dict[str, int]:
    """å¿«é€Ÿæª¢æŸ¥æœ€è¿‘çš„è®Šæ›´"""
    recent_threshold = datetime.now() - timedelta(minutes=minutes)
    changes = {}
    
    with engine.connect() as conn:
        for table in ['product_master_a', 'product_warehouse_b', 'customer_complaint_c']:
            query = text(f"""
                SELECT COUNT(*) as cnt 
                FROM {table} 
                WHERE last_modified > :threshold
            """)
            result = conn.execute(query, {'threshold': recent_threshold})
            count = result.scalar()
            if count > 0:
                changes[table] = count
    
    return changes

# ============== ä¸»ç¨‹å¼ ==============
def main():
    logger.info("=" * 60)
    logger.info("ğŸš€ MySQL to Elasticsearch ç›´æ¥åŒæ­¥æœå‹™å•Ÿå‹•")
    logger.info(f"ğŸ“Š é…ç½®: BATCH={BATCH_SIZE}, PAGE={PAGE_SIZE}, THREADS={PARALLEL_THREADS}")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–
    es = get_es_client()
    state = load_state()
    product_cache.refresh()
    
    # å®šç¾©åŒæ­¥è¡¨
    tables = {
        "product_master_a": process_product_master,
        "product_warehouse_b": process_warehouse,
        "customer_complaint_c": process_complaints
    }
    
    # åˆå§‹å…¨é‡åŒæ­¥æª¢æŸ¥
    first_run = not state
    if first_run:
        logger.info("ğŸ“¥ é¦–æ¬¡åŸ·è¡Œï¼Œé–‹å§‹å…¨é‡åŒæ­¥...")
    
    # ä¸»å¾ªç’°
    consecutive_no_updates = 0
    last_quick_check = time.time()
    
    while True:
        try:
            has_updates = False
            
            # å¿«é€Ÿæª¢æŸ¥ï¼ˆæ¯åˆ†é˜ï¼‰
            if time.time() - last_quick_check > 60:
                changes = check_recent_changes(5)
                if changes:
                    logger.info(f"âš¡ å¿«é€Ÿæª¢æŸ¥ç™¼ç¾è®Šæ›´: {changes}")
                last_quick_check = time.time()
            
            # åŒæ­¥å„è¡¨
            for table_name, processor in tables.items():
                try:
                    updated = sync_table(table_name, processor, es, state)
                    if updated:
                        has_updates = True
                        save_state(state)
                        consecutive_no_updates = 0
                except Exception as e:
                    logger.error(f"âŒ åŒæ­¥ {table_name} å¤±æ•—: {e}", exc_info=True)
            
            # å‹•æ…‹èª¿æ•´ç¡çœ æ™‚é–“
            if not has_updates:
                consecutive_no_updates += 1
                # å¦‚æœé€£çºŒå¤šæ¬¡ç„¡æ›´æ–°ï¼Œå»¶é•·ç¡çœ æ™‚é–“
                sleep_time = min(SLEEP_SECONDS * (1 + consecutive_no_updates // 5), 300)
                logger.info(f"ğŸ’¤ ç„¡æ–°è³‡æ–™ï¼Œç­‰å¾… {sleep_time} ç§’...")
                time.sleep(sleep_time)
            else:
                # æœ‰æ›´æ–°æ™‚çŸ­æš«ç­‰å¾…
                time.sleep(5)
            
        except KeyboardInterrupt:
            logger.info("â¹ï¸ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰...")
            break
        except Exception as e:
            logger.error(f"âŒ ä¸»å¾ªç’°éŒ¯èª¤: {e}", exc_info=True)
            time.sleep(30)
    
    logger.info("ğŸ‘‹ åŒæ­¥æœå‹™å·²åœæ­¢")

if __name__ == "__main__":
    main()
