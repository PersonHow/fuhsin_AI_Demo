#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG (Retrieval-Augmented Generation) API æœå‹™
æä¾›æ™ºèƒ½å•ç­”ä»‹é¢ï¼Œçµåˆå‘é‡æœå°‹å’Œ OpenAI GPT
"""
import os
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import requests
from openai import OpenAI
import uvicorn

# ç’°å¢ƒè®Šæ•¸é…ç½®
ES_URL = os.environ.get("ES_URL", "http://localhost:9200")
ES_USER = os.environ.get("ES_USER", "elastic")
ES_PASS = os.environ.get("ES_PASS", "admin@12345")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
GPT_MODEL = os.environ.get("GPT_MODEL", "gpt-4o-mini")

# åˆå§‹åŒ–
app = FastAPI(title="RAG æª¢ç´¢ API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI å®¢æˆ¶ç«¯
client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

# Elasticsearch é€£ç·š
es_session = requests.Session()
es_session.auth = (ES_USER, ES_PASS)
es_session.headers.update({"Content-Type": "application/json"})

# === è³‡æ–™æ¨¡å‹ ===
class QueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"  # "keyword", "vector", "hybrid"
    top_k: int = 5
    index_pattern: str = "erp-*"
    use_gpt: bool = True
    temperature: float = 0.7

class QueryResponse(BaseModel):
    query: str
    answer: Optional[str]
    sources: List[Dict[str, Any]]
    search_mode: str
    total_hits: int
    processing_time_ms: int

class HealthResponse(BaseModel):
    status: str
    elasticsearch: bool
    openai: bool
    timestamp: str

# === æ ¸å¿ƒåŠŸèƒ½ ===
class RAGEngine:
    """RAG å¼•æ“"""
    
    @staticmethod
    def generate_embedding(text: str) -> List[float]:
        """ç”ŸæˆæŸ¥è©¢å‘é‡"""
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text[:8000]
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"ç”Ÿæˆå‘é‡å¤±æ•—: {e}")
            return None
    
    @staticmethod
    def keyword_search(query: str, index_pattern: str, size: int = 5) -> Dict:
        """é—œéµå­—æœå°‹ï¼ˆä½¿ç”¨ IK åˆ†è©ï¼‰"""
        search_body = {
            "size": size,
            "_source": {
                "excludes": ["content_vector"]
            },
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "searchable_content^3",
                        "all_content^2",
                        "field_*"
                    ],
                    "type": "best_fields",
                    "analyzer": "ik_smart"
                }
            },
            "highlight": {
                "fields": {
                    "searchable_content": {"fragment_size": 150},
                    "all_content": {"fragment_size": 150},
                    "field_*": {"fragment_size": 100}
                }
            }
        }
        
        r = es_session.post(f"{ES_URL}/{index_pattern}/_search", json=search_body)
        if r.status_code == 200:
            return r.json()
        return {"hits": {"hits": [], "total": {"value": 0}}}
    
    @staticmethod
    def vector_search(query_vector: List[float], index_pattern: str, size: int = 5) -> Dict:
        """å‘é‡æœå°‹"""
        search_body = {
            "size": size,
            "_source": {
                "excludes": ["content_vector"]
            },
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                        "params": {
                            "query_vector": query_vector
                        }
                    }
                }
            }
        }
        
        # æˆ–ä½¿ç”¨ knn æœå°‹ï¼ˆES 8.0+ï¼‰
        knn_body = {
            "size": size,
            "_source": {
                "excludes": ["content_vector"]
            },
            "knn": {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": size,
                "num_candidates": size * 10
            }
        }
        
        try:
            r = es_session.post(f"{ES_URL}/{index_pattern}/_search", json=knn_body)
            if r.status_code == 200:
                return r.json()
        except:
            # é™ç´šä½¿ç”¨ script_score
            r = es_session.post(f"{ES_URL}/{index_pattern}/_search", json=search_body)
            if r.status_code == 200:
                return r.json()
        
        return {"hits": {"hits": [], "total": {"value": 0}}}
    
    @staticmethod
    def hybrid_search(query: str, query_vector: List[float], index_pattern: str, size: int = 5) -> Dict:
        """æ··åˆæœå°‹ï¼ˆçµåˆé—œéµå­—å’Œå‘é‡ï¼‰"""
        search_body = {
            "size": size * 2,  # å–æ›´å¤šçµæœå¾Œé‡æ’åº
            "_source": {
                "excludes": ["content_vector"]
            },
            "query": {
                "bool": {
                    "should": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["searchable_content^2", "all_content", "field_*"],
                                "type": "best_fields",
                                "analyzer": "ik_smart",
                                "boost": 0.5
                            }
                        },
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                                    "params": {
                                        "query_vector": query_vector
                                    }
                                },
                                "boost": 0.5
                            }
                        }
                    ]
                }
            },
            "highlight": {
                "fields": {
                    "searchable_content": {"fragment_size": 150},
                    "all_content": {"fragment_size": 150},
                    "field_*": {"fragment_size": 100}
                }
            }
        }
        
        r = es_session.post(f"{ES_URL}/{index_pattern}/_search", json=search_body)
        if r.status_code == 200:
            result = r.json()
            # åªè¿”å›å‰ size å€‹çµæœ
            result["hits"]["hits"] = result["hits"]["hits"][:size]
            return result
        return {"hits": {"hits": [], "total": {"value": 0}}}
    
    @staticmethod
    def format_context(search_results: Dict) -> str:
        """æ ¼å¼åŒ–æœå°‹çµæœç‚ºä¸Šä¸‹æ–‡"""
        contexts = []
        for hit in search_results.get("hits", {}).get("hits", [])[:5]:
            source = hit["_source"]
            metadata = source.get("metadata", {})
            
            # æ”¶é›†é‡è¦æ¬„ä½
            content_parts = []
            
            # å„ªå…ˆé¡¯ç¤ºé«˜äº®å…§å®¹
            if "highlight" in hit:
                for field, highlights in hit["highlight"].items():
                    content_parts.extend(highlights[:2])
            
            # å¦‚æœæ²’æœ‰é«˜äº®ï¼Œä½¿ç”¨åŸå§‹å…§å®¹
            if not content_parts:
                if source.get("searchable_content"):
                    content_parts.append(source["searchable_content"][:200])
                elif source.get("all_content"):
                    content_parts.append(source["all_content"][:200])
            
            # çµ„åˆä¸Šä¸‹æ–‡
            context = f"[ä¾†æº: {metadata.get('source_file', 'unknown')}, "
            context += f"è¡¨: {metadata.get('table_name', 'unknown')}]\n"
            context += "\n".join(content_parts)
            contexts.append(context)
        
        return "\n---\n".join(contexts)
    
    @staticmethod
    def generate_answer(query: str, context: str, temperature: float = 0.7) -> str:
        """ä½¿ç”¨ GPT ç”Ÿæˆç­”æ¡ˆ"""
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è³‡æ–™åº«æŸ¥è©¢åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šï¼Œå›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
        
è¦å‰‡ï¼š
1. åªæ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦ç·¨é€ è³‡è¨Š
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜
3. å›ç­”è¦æº–ç¢ºã€ç°¡æ½”ã€æœ‰æ¢ç†
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
5. å¦‚æœæ¶‰åŠæ•¸æ“šï¼Œè«‹å¼•ç”¨å…·é«”ä¾†æº"""
        
        user_prompt = f"""å•é¡Œï¼š{query}

ç›¸é—œä¸Šä¸‹æ–‡ï¼š
{context}

è«‹æ ¹æ“šä¸Šè¿°ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚"""
        
        try:
            response = client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

# === API ç«¯é» ===
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    es_health = False
    openai_health = False
    
    # æª¢æŸ¥ Elasticsearch
    try:
        r = es_session.get(f"{ES_URL}/_cluster/health")
        es_health = r.status_code == 200
    except:
        pass
    
    # æª¢æŸ¥ OpenAI
    try:
        if OPENAI_API_KEY:
            # ç°¡å–®æ¸¬è©¦
            test_embedding = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input="test"
            )
            openai_health = True
    except:
        pass
    
    status = "healthy" if (es_health and openai_health) else "degraded"
    
    return HealthResponse(
        status=status,
        elasticsearch=es_health,
        openai=openai_health,
        timestamp=datetime.now().isoformat()
    )

@app.post("/query", response_model=QueryResponse)
async def query_data(request: QueryRequest):
    """æ™ºèƒ½æŸ¥è©¢ç«¯é»"""
    start_time = datetime.now()
    
    # ç”ŸæˆæŸ¥è©¢å‘é‡
    query_vector = None
    if request.mode in ["vector", "hybrid"]:
        query_vector = RAGEngine.generate_embedding(request.query)
        if not query_vector:
            raise HTTPException(status_code=500, detail="å‘é‡ç”Ÿæˆå¤±æ•—")
    
    # åŸ·è¡Œæœå°‹
    if request.mode == "keyword":
        search_results = RAGEngine.keyword_search(
            request.query, request.index_pattern, request.top_k
        )
    elif request.mode == "vector":
        search_results = RAGEngine.vector_search(
            query_vector, request.index_pattern, request.top_k
        )
    else:  # hybrid
        search_results = RAGEngine.hybrid_search(
            request.query, query_vector, request.index_pattern, request.top_k
        )
    
    # æ ¼å¼åŒ–çµæœ
    sources = []
    for hit in search_results.get("hits", {}).get("hits", []):
        source = hit["_source"]
        sources.append({
            "score": hit.get("_score", 0),
            "index": hit["_index"],
            "metadata": source.get("metadata", {}),
            "content": source.get("searchable_content", "")[:200],
            "highlights": hit.get("highlight", {})
        })
    
    # ç”Ÿæˆç­”æ¡ˆ
    answer = None
    if request.use_gpt and sources:
        context = RAGEngine.format_context(search_results)
        answer = RAGEngine.generate_answer(
            request.query, context, request.temperature
        )
    
    # è¨ˆç®—è™•ç†æ™‚é–“
    processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
    
    return QueryResponse(
        query=request.query,
        answer=answer,
        sources=sources,
        search_mode=request.mode,
        total_hits=search_results.get("hits", {}).get("total", {}).get("value", 0),
        processing_time_ms=processing_time
    )

@app.get("/stats")
async def get_statistics():
    """ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Š"""
    stats = {}
    
    # ç²å–ç´¢å¼•çµ±è¨ˆ
    try:
        r = es_session.get(f"{ES_URL}/erp-*/_stats")
        if r.status_code == 200:
            data = r.json()
            total_docs = sum(idx["primaries"]["docs"]["count"] 
                           for idx in data["indices"].values())
            total_size = sum(idx["primaries"]["store"]["size_in_bytes"] 
                           for idx in data["indices"].values())
            
            stats["indices"] = {
                "count": len(data["indices"]),
                "total_documents": total_docs,
                "total_size_mb": round(total_size / 1024 / 1024, 2)
            }
    except:
        pass
    
    # æª¢æŸ¥å‘é‡åŒ–é€²åº¦
    try:
        # æœ‰å‘é‡çš„æ–‡æª”æ•¸
        vector_query = {
            "size": 0,
            "query": {"exists": {"field": "content_vector"}}
        }
        r1 = es_session.post(f"{ES_URL}/erp-*/_count", json=vector_query)
        
        # ç¸½æ–‡æª”æ•¸
        r2 = es_session.post(f"{ES_URL}/erp-*/_count", json={"query": {"match_all": {}}})
        
        if r1.status_code == 200 and r2.status_code == 200:
            with_vector = r1.json()["count"]
            total = r2.json()["count"]
            stats["vectorization"] = {
                "completed": with_vector,
                "total": total,
                "progress_percent": round((with_vector / total * 100) if total > 0 else 0, 2)
            }
    except:
        pass
    
    stats["timestamp"] = datetime.now().isoformat()
    return stats

# === ä¸»ç¨‹åº ===
if __name__ == "__main__":
    if not OPENAI_API_KEY:
        print("âš ï¸ è­¦å‘Šï¼šæœªè¨­ç½® OPENAI_API_KEYï¼Œéƒ¨åˆ†åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
    
    print(f"ğŸš€ RAG API æœå‹™å•Ÿå‹•")
    print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹ï¼šEmbedding={EMBEDDING_MODEL}, GPT={GPT_MODEL}")
    print(f"ğŸŒ API æ–‡æª”ï¼šhttp://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
