#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG (Retrieval-Augmented Generation) API æœå‹™
æä¾›æ™ºèƒ½å•ç­”ä»‹é¢ï¼Œçµåˆå‘é‡æœå°‹å’Œ OpenAI GPT

ä¸»è¦åŠŸèƒ½ï¼š
1. é—œéµå­—æœå°‹ - ä½¿ç”¨ IK åˆ†è©å™¨é€²è¡Œä¸­æ–‡åˆ†è©
2. å‘é‡æœå°‹ - ä½¿ç”¨èªæ„ç›¸ä¼¼åº¦æœå°‹
3. æ··åˆæœå°‹ - çµåˆé—œéµå­—èˆ‡å‘é‡æœå°‹
4. GPT ç­”æ¡ˆç”Ÿæˆ - æ ¹æ“šæœå°‹çµæœç”Ÿæˆè‡ªç„¶èªè¨€ç­”æ¡ˆ
5. ç°¡ç¹è½‰æ› - è‡ªå‹•è™•ç†ç°¡é«”ç¹é«”æŸ¥è©¢

"""
import os, json, requests, uvicorn, logging, time, re
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from openai import OpenAI
from opencc import OpenCC

# ============================================================================
# é…ç½®ç®¡ç†æ¨¡çµ„
# ============================================================================


@dataclass
class Config:
    """ç³»çµ±é…ç½®é¡åˆ¥"""

    # Elasticsearch é…ç½®
    es_url: str = os.environ.get("ES_URL", "http://localhost:9200")
    es_user: str = os.environ.get("ES_USER", "elastic")
    es_pass: str = os.environ.get("ES_PASS", "admin@12345")

    # OpenAI é…ç½®
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    openai_base_url: str = os.environ.get(
        "OPENAI_BASE_URL", "https://api.openai.com/v1"
    )
    embedding_model: str = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
    gpt_model: str = os.environ.get("GPT_MODEL", "gpt-4o-mini")

    # API æœå‹™é…ç½®
    api_host: str = "0.0.0.0"
    api_port: int = 8010
    api_title: str = "RAG æª¢ç´¢ API"
    api_version: str = "2.0.0"

    # æœå°‹é…ç½®
    default_index_pattern: str = "erp-*"
    default_top_k: int = 5
    default_batch_size: int = 100

    # è«‹æ±‚è¶…æ™‚è¨­å®šï¼ˆç§’ï¼‰
    request_timeout: int = 30

    def validate(self) -> bool:
        """é©—è­‰å¿…è¦é…ç½®æ˜¯å¦å­˜åœ¨"""
        if not self.openai_api_key:
            logging.warning("âš ï¸ æœªè¨­ç½® OPENAI_API_KEYï¼ŒGPT åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
            return False
        return True


# ============================================================================
# æ—¥èªŒè¨­å®š
# ============================================================================


def setup_logging():
    """è¨­å®šæ—¥èªŒæ ¼å¼å’Œç­‰ç´š"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    return logging.getLogger(__name__)


# ============================================================================
# è³‡æ–™æ¨¡å‹å®šç¾©
# ============================================================================


class SearchMode(str, Enum):
    """æœå°‹æ¨¡å¼åˆ—èˆ‰"""

    KEYWORD = "keyword"  # é—œéµå­—æœå°‹
    VECTOR = "vector"  # å‘é‡æœå°‹
    HYBRID = "hybrid"  # æ··åˆæœå°‹


class QueryRequest(BaseModel):
    """
    æŸ¥è©¢è«‹æ±‚æ¨¡å‹
    å®šç¾© API æ¥æ”¶çš„æŸ¥è©¢åƒæ•¸
    """

    query: str = Field(..., description="æŸ¥è©¢å­—ä¸²", min_length=1, max_length=1000)
    mode: SearchMode = Field(default=SearchMode.HYBRID, description="æœå°‹æ¨¡å¼")
    top_k: int = Field(default=5, ge=1, le=100, description="è¿”å›çµæœæ•¸é‡")
    index_pattern: str = Field(default="erp-*", description="ç´¢å¼•æ¨¡å¼")
    use_gpt: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨ GPT ç”Ÿæˆç­”æ¡ˆ")
    temperature: float = Field(default=0.7, ge=0, le=2, description="GPT ç”Ÿæˆæº«åº¦")
    convert_to_traditional: bool = Field(
        default=True, description="æ˜¯å¦å°‡ç°¡é«”æŸ¥è©¢è½‰ç‚ºç¹é«”"
    )

    class Config:
        schema_extra = {
            "example": {
                "query": "ç”¢å“é€€è²¨æµç¨‹",
                "mode": "hybrid",
                "top_k": 5,
                "use_gpt": True,
                "temperature": 0.7,
            }
        }


class SearchResult(BaseModel):
    """å–®ç­†æœå°‹çµæœ"""

    score: float = Field(..., description="ç›¸é—œæ€§åˆ†æ•¸")
    index: str = Field(..., description="ä¾†æºç´¢å¼•")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å…ƒè³‡æ–™")
    content: str = Field(..., description="å…§å®¹æ‘˜è¦")
    highlights: Dict[str, List[str]] = Field(
        default_factory=dict, description="é«˜äº®ç‰‡æ®µ"
    )


class QueryResponse(BaseModel):
    """
    æŸ¥è©¢å›æ‡‰æ¨¡å‹
    å®šç¾© API è¿”å›çš„çµæœæ ¼å¼
    """

    query: str = Field(..., description="åŸå§‹æŸ¥è©¢")
    processed_query: str = Field(..., description="è™•ç†å¾Œçš„æŸ¥è©¢ï¼ˆç¹é«”ï¼‰")
    answer: Optional[str] = Field(None, description="GPT ç”Ÿæˆçš„ç­”æ¡ˆ")
    sources: List[SearchResult] = Field(
        default_factory=list, description="æœå°‹çµæœä¾†æº"
    )
    search_mode: str = Field(..., description="ä½¿ç”¨çš„æœå°‹æ¨¡å¼")
    total_hits: int = Field(0, description="ç¸½å‘½ä¸­æ•¸")
    processing_time_ms: int = Field(..., description="è™•ç†æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰")

    class Config:
        schema_extra = {
            "example": {
                "query": "ç”¢å“é€€è²¨",
                "processed_query": "ç”¢å“é€€è²¨",
                "answer": "æ ¹æ“šæŸ¥è©¢çµæœï¼Œç”¢å“é€€è²¨æµç¨‹å¦‚ä¸‹...",
                "sources": [],
                "search_mode": "hybrid",
                "total_hits": 10,
                "processing_time_ms": 250,
            }
        }


class HealthResponse(BaseModel):
    """å¥åº·æª¢æŸ¥å›æ‡‰"""

    status: str
    elasticsearch: bool
    openai: bool
    timestamp: str


# ============================================================================
# æ–‡å­—è™•ç†å·¥å…·
# ============================================================================


class TextProcessor:
    """
    æ–‡å­—è™•ç†å™¨
    è² è²¬ç°¡ç¹è½‰æ›å’Œæ–‡å­—æ¸…ç†
    """

    def __init__(self):
        """åˆå§‹åŒ–ç°¡ç¹è½‰æ›å™¨"""
        self.s2t = OpenCC("s2t")  # ç°¡é«”è½‰ç¹é«”
        self.t2s = OpenCC("t2s")  # ç¹é«”è½‰ç°¡é«”
        self.logger = logging.getLogger(self.__class__.__name__)

    def to_traditional(self, text: str) -> str:
        """
        å°‡æ–‡å­—è½‰æ›ç‚ºç¹é«”ä¸­æ–‡

        Args:
            text: è¼¸å…¥æ–‡å­—

        Returns:
            ç¹é«”ä¸­æ–‡æ–‡å­—
        """
        try:
            return self.s2t.convert(text)
        except Exception as e:
            self.logger.error(f"ç°¡è½‰ç¹å¤±æ•—: {e}")
            return text

    def to_simplified(self, text: str) -> str:
        """
        å°‡æ–‡å­—è½‰æ›ç‚ºç°¡é«”ä¸­æ–‡

        Args:
            text: è¼¸å…¥æ–‡å­—

        Returns:
            ç°¡é«”ä¸­æ–‡æ–‡å­—
        """
        try:
            return self.t2s.convert(text)
        except Exception as e:
            self.logger.error(f"ç¹è½‰ç°¡å¤±æ•—: {e}")
            return text

    def prepare_search_query(
        self, query: str, convert_to_traditional: bool = True
    ) -> Tuple[str, List[str]]:
        """
        æº–å‚™æœå°‹æŸ¥è©¢ï¼Œç”Ÿæˆå¤šç¨®è®Šé«”ä»¥æé«˜å¬å›ç‡

        Args:
            query: åŸå§‹æŸ¥è©¢
            convert_to_traditional: æ˜¯å¦è½‰ç‚ºç¹é«”

        Returns:
            (è™•ç†å¾Œçš„ä¸»æŸ¥è©¢, æŸ¥è©¢è®Šé«”åˆ—è¡¨)
        """
        # æ¸…ç†æŸ¥è©¢å­—ä¸²
        query = query.strip()

        # ç”ŸæˆæŸ¥è©¢è®Šé«”
        variants = [query]

        # åŠ å…¥ç¹é«”ç‰ˆæœ¬
        traditional = self.to_traditional(query)
        if traditional != query:
            variants.append(traditional)

        # åŠ å…¥ç°¡é«”ç‰ˆæœ¬
        simplified = self.to_simplified(query)
        if simplified != query:
            variants.append(simplified)

        # æ±ºå®šä¸»æŸ¥è©¢
        main_query = traditional if convert_to_traditional else query

        return main_query, list(set(variants))


# ============================================================================
# Elasticsearch å®¢æˆ¶ç«¯
# ============================================================================


class ElasticsearchClient:
    """
    Elasticsearch å®¢æˆ¶ç«¯å°è£
    æä¾›æœå°‹å’Œç´¢å¼•ç®¡ç†åŠŸèƒ½
    """

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ– ES å®¢æˆ¶ç«¯

        Args:
            config: ç³»çµ±é…ç½®
        """
        self.config = config
        self.session = self._create_session()
        self.logger = logging.getLogger(self.__class__.__name__)

    def _create_session(self) -> requests.Session:
        """å»ºç«‹ HTTP Session ä¸¦é…ç½®èªè­‰"""
        session = requests.Session()
        session.auth = (self.config.es_user, self.config.es_pass)
        session.headers.update({"Content-Type": "application/json"})
        return session

    def search(self, index_pattern: str, query_body: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸ·è¡Œæœå°‹è«‹æ±‚

        Args:
            index_pattern: ç´¢å¼•æ¨¡å¼
            query_body: ES æŸ¥è©¢ DSL

        Returns:
            æœå°‹çµæœ
        """
        try:
            response = self.session.post(
                f"{self.config.es_url}/{index_pattern}/_search",
                json=query_body,
                timeout=self.config.request_timeout,
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            self.logger.error(f"æœå°‹è«‹æ±‚å¤±æ•—: {e}")
            return {"hits": {"hits": [], "total": {"value": 0}}}

    def health_check(self) -> bool:
        """
        æª¢æŸ¥ Elasticsearch å¥åº·ç‹€æ…‹

        Returns:
            True å¦‚æœå¥åº·ï¼Œå¦å‰‡ False
        """
        try:
            response = self.session.get(
                f"{self.config.es_url}/_cluster/health", timeout=5
            )
            return response.status_code == 200
        except:
            return False

    def get_stats(self, index_pattern: str = "erp-*") -> Dict[str, Any]:
        """
        å–å¾—ç´¢å¼•çµ±è¨ˆè³‡è¨Š

        Args:
            index_pattern: ç´¢å¼•æ¨¡å¼

        Returns:
            çµ±è¨ˆè³‡è¨Š
        """
        stats = {}
        try:
            # å–å¾—ç´¢å¼•çµ±è¨ˆ
            response = self.session.get(
                f"{self.config.es_url}/{index_pattern}/_stats",
                timeout=self.config.request_timeout,
            )
            if response.status_code == 200:
                data = response.json()
                total_docs = sum(
                    idx["primaries"]["docs"]["count"]
                    for idx in data["indices"].values()
                )
                total_size = sum(
                    idx["primaries"]["store"]["size_in_bytes"]
                    for idx in data["indices"].values()
                )

                stats["indices"] = {
                    "count": len(data["indices"]),
                    "total_documents": total_docs,
                    "total_size_mb": round(total_size / 1024 / 1024, 2),
                }

            # æª¢æŸ¥å‘é‡åŒ–é€²åº¦
            vector_query = {"size": 0, "query": {"exists": {"field": "content_vector"}}}
            response = self.session.post(
                f"{self.config.es_url}/{index_pattern}/_count",
                json=vector_query,
                timeout=self.config.request_timeout,
            )
            if response.status_code == 200:
                with_vector = response.json()["count"]

                # å–å¾—ç¸½æ–‡æª”æ•¸
                total_response = self.session.post(
                    f"{self.config.es_url}/{index_pattern}/_count",
                    json={"query": {"match_all": {}}},
                    timeout=self.config.request_timeout,
                )
                if total_response.status_code == 200:
                    total = total_response.json()["count"]
                    stats["vectorization"] = {
                        "completed": with_vector,
                        "total": total,
                        "progress_percent": round(
                            (with_vector / total * 100) if total > 0 else 0, 2
                        ),
                    }
        except Exception as e:
            self.logger.error(f"å–å¾—çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}")

        stats["timestamp"] = datetime.now().isoformat()
        return stats


# ============================================================================
# å‘é‡ç”Ÿæˆå™¨
# ============================================================================


class VectorGenerator:
    """
    å‘é‡ç”Ÿæˆå™¨
    ä½¿ç”¨ OpenAI Embeddings API ç”Ÿæˆæ–‡æœ¬å‘é‡
    """

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–å‘é‡ç”Ÿæˆå™¨

        Args:
            config: ç³»çµ±é…ç½®
        """
        self.config = config
        self.client = None
        self.logger = logging.getLogger(self.__class__.__name__)

        if config.openai_api_key:
            try:
                self.client = OpenAI(
                    api_key=config.openai_api_key, base_url=config.openai_base_url
                )
                self.logger.info(
                    f"âœ… OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {config.embedding_model}"
                )
            except Exception as e:
                self.logger.error(f"âŒ OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")

    def generate(self, text: str) -> Optional[List[float]]:
        """
        ç”Ÿæˆå–®å€‹æ–‡æœ¬çš„å‘é‡

        Args:
            text: è¼¸å…¥æ–‡æœ¬

        Returns:
            å‘é‡åˆ—è¡¨ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        if not self.client:
            return None

        try:
            # é™åˆ¶æ–‡æœ¬é•·åº¦ï¼ˆOpenAI æœ‰ token é™åˆ¶ï¼‰
            text = text[:8000]

            response = self.client.embeddings.create(
                model=self.config.embedding_model, input=text
            )
            return response.data[0].embedding
        except Exception as e:
            self.logger.error(f"å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
            return None

    def health_check(self) -> bool:
        """
        æª¢æŸ¥ OpenAI API æ˜¯å¦å¯ç”¨

        Returns:
            True å¦‚æœå¯ç”¨ï¼Œå¦å‰‡ False
        """
        if not self.client:
            return False

        try:
            # å˜—è©¦ç”Ÿæˆä¸€å€‹ç°¡å–®çš„æ¸¬è©¦å‘é‡
            self.generate("test")
            return True
        except:
            return False


# ============================================================================
# æœå°‹å¼•æ“
# ============================================================================


class SearchEngine:
    """
    æœå°‹å¼•æ“
    å¯¦ç¾é—œéµå­—ã€å‘é‡å’Œæ··åˆæœå°‹
    """

    def __init__(
        self,
        es_client: ElasticsearchClient,
        vector_gen: VectorGenerator,
        text_processor: TextProcessor,
    ):
        """
        åˆå§‹åŒ–æœå°‹å¼•æ“

        Args:
            es_client: Elasticsearch å®¢æˆ¶ç«¯
            vector_gen: å‘é‡ç”Ÿæˆå™¨
            text_processor: æ–‡å­—è™•ç†å™¨
        """
        self.es_client = es_client
        self.vector_gen = vector_gen
        self.text_processor = text_processor
        self.logger = logging.getLogger(self.__class__.__name__)

    def keyword_search(
        self, query: str, index_pattern: str, size: int = 5
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œé—œéµå­—æœå°‹ - å„ªåŒ–ç‰ˆæœ¬
        ç‰¹åˆ¥åŠ å¼·å°ç‹€æ…‹æ¬„ä½çš„æœå°‹æ”¯æ´

        Args:
            query: æŸ¥è©¢å­—ä¸²
            index_pattern: ç´¢å¼•æ¨¡å¼
            size: è¿”å›çµæœæ•¸

        Returns:
            æœå°‹çµæœ
        """
        # æº–å‚™æŸ¥è©¢è®Šé«”ï¼ˆç¹ç°¡é«”ï¼‰
        _, query_variants = self.text_processor.prepare_search_query(query)

        # æª¢æ¸¬æ˜¯å¦ç‚ºç”¢å“ç·¨è™ŸæŸ¥è©¢ï¼ˆPæˆ–Wé–‹é ­åŠ æ•¸å­—ï¼‰
        product_id_pattern = re.compile(r"^[PW]\d{3}$")
        is_product_id_query = bool(product_id_pattern.match(query.strip().upper()))

        # æ§‹å»ºæŸ¥è©¢ DSL - å„ªåŒ–ç‰ˆæœ¬
        search_body = {
            "size": size * 2,  # å–æ›´å¤šçµæœä»¥æé«˜å¬å›ç‡
            "_source": {"excludes": ["content_vector"]},
            "query": {"bool": {"should": []}},
            "highlight": {
                "fields": {
                    "field_status": {"fragment_size": 50, "number_of_fragments": 1},
                    "field_complaint_status": {
                        "fragment_size": 50,
                        "number_of_fragments": 1,
                    },
                    "searchable_content": {
                        "fragment_size": 150,
                        "number_of_fragments": 3,
                    },
                    "all_content": {"fragment_size": 150, "number_of_fragments": 2},
                    "content": {"fragment_size": 150, "number_of_fragments": 2},
                    "text": {"fragment_size": 150, "number_of_fragments": 2},
                    "field_*": {"fragment_size": 100, "number_of_fragments": 2},
                },
                "pre_tags": ["<mark>"],
                "post_tags": ["</mark>"],
            },
        }

        # å¦‚æœæ˜¯ç”¢å“ç·¨è™ŸæŸ¥è©¢ï¼Œå„ªå…ˆç²¾ç¢ºåŒ¹é…
        if is_product_id_query:
            product_id = query.strip().upper()

            # 1. ç²¾ç¢ºåŒ¹é… product_ids é™£åˆ—ï¼ˆæœ€é«˜å„ªå…ˆï¼‰
            search_body["query"]["bool"]["should"].append(
                {"term": {"product_ids": {"value": product_id, "boost": 50.0}}}
            )

            # 2. ç²¾ç¢ºåŒ¹é… metadata.product_id
            search_body["query"]["bool"]["should"].append(
                {"term": {"metadata.product_id": {"value": product_id, "boost": 40.0}}}
            )

            # 3. ç²¾ç¢ºåŒ¹é… source_meta.product_id
            search_body["query"]["bool"]["should"].append(
                {
                    "term": {
                        "source_meta.product_id": {"value": product_id, "boost": 40.0}
                    }
                }
            )

            # 4. åœ¨æ¨™é¡Œä¸­æœå°‹
            search_body["query"]["bool"]["should"].append(
                {"match_phrase": {"title": {"query": product_id, "boost": 30.0}}}
            )

            # 5. åœ¨å…§å®¹ä¸­æœå°‹
            search_body["query"]["bool"]["should"].append(
                {"match_phrase": {"content": {"query": product_id, "boost": 20.0}}}
            )

            # 6. åœ¨æ–‡æœ¬ä¸­æœå°‹
            search_body["query"]["bool"]["should"].append(
                {"match_phrase": {"text": {"query": product_id, "boost": 20.0}}}
            )

            # 7. åœ¨æè¿°ä¸­æœå°‹ï¼ˆå®¢è¨´è¨˜éŒ„ï¼‰
            search_body["query"]["bool"]["should"].append(
                {
                    "match_phrase": {
                        "field_description": {"query": product_id, "boost": 15.0}
                    }
                }
            )

            # 8. å…¨æ–‡æœå°‹ä½œç‚ºå¾Œå‚™
            search_body["query"]["bool"]["should"].append(
                {
                    "multi_match": {
                        "query": product_id,
                        "fields": ["all_content", "searchable_content"],
                        "type": "phrase",
                        "boost": 10.0,
                    }
                }
            )

        else:
            # éç”¢å“ç·¨è™ŸæŸ¥è©¢ï¼Œä½¿ç”¨åŸæœ‰é‚è¼¯
            # ... [ä¿æŒåŸæœ‰çš„ç‹€æ…‹æœå°‹å’Œä¸€èˆ¬æœå°‹é‚è¼¯]

            # 1. ç²¾ç¢ºåŒ¹é…ç‹€æ…‹æ¬„ä½ï¼ˆæœ€é«˜å„ªå…ˆï¼‰
            search_body["query"]["bool"]["should"].extend(
                [
                    {"term": {"field_status.keyword": {"value": query, "boost": 20.0}}},
                    {
                        "term": {
                            "field_complaint_status.keyword": {
                                "value": query,
                                "boost": 20.0,
                            }
                        }
                    },
                    {
                        "term": {
                            "field_handling_status.keyword": {
                                "value": query,
                                "boost": 20.0,
                            }
                        }
                    },
                ]
            )

            # 2. çŸ­èªåŒ¹é…
            search_body["query"]["bool"]["should"].extend(
                [
                    {"match_phrase": {"field_status": {"query": query, "boost": 15.0}}},
                    {"match_phrase": {"all_content": {"query": query, "boost": 5.0}}},
                ]
            )

            # 3. å¤šæ¬„ä½æœå°‹ï¼ˆä½¿ç”¨è®Šé«”ï¼‰
            for variant in query_variants:
                search_body["query"]["bool"]["should"].append(
                    {
                        "multi_match": {
                            "query": variant,
                            "fields": [
                                "field_status^20",
                                "field_complaint_status^20",
                                "field_handling_status^20",
                                "field_process_status^20",
                                "field_state^20",
                                "field_complaint_id^10",
                                "field_product_id^8",
                                "field_product_name^8",
                                "field_description^5",
                                "field_complaint_description^5",
                                "field_complaint_content^5",
                                "searchable_content^3",
                                "all_content^2",
                                "content^2",
                                "text^2",
                                "field_*^1",
                            ],
                            "type": "best_fields",
                            "analyzer": "ik_smart",
                            "boost": 1.0 if variant == query else 0.8,
                        }
                    }
                )

        search_body["query"]["bool"]["minimum_should_match"] = 1

        # åŠ å…¥èšåˆä»¥äº†è§£çµæœåˆ†ä½ˆ
        search_body["aggs"] = {
            "type_distribution": {"terms": {"field": "type", "size": 10}},
            "product_distribution": {"terms": {"field": "product_ids", "size": 20}},
        }

        self.logger.info(
            f"åŸ·è¡Œé—œéµå­—æœå°‹: {query} (æ˜¯å¦ç”¢å“ç·¨è™Ÿ: {is_product_id_query})"
        )
        result = self.es_client.search(index_pattern, search_body)

        # å¾Œè™•ç†ï¼šå»é‡ä¸¦é™åˆ¶çµæœæ•¸é‡
        seen_ids = set()
        filtered_hits = []

        for hit in result.get("hits", {}).get("hits", []):
            # ä½¿ç”¨æ–‡æª”çš„å”¯ä¸€IDä¾†å»é‡
            doc_id = hit.get("_id")
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                filtered_hits.append(hit)
                if len(filtered_hits) >= size:
                    break

        result["hits"]["hits"] = filtered_hits

        # è¨˜éŒ„èšåˆçµæœ
        if "aggregations" in result:
            if "type_distribution" in result["aggregations"]:
                type_buckets = result["aggregations"]["type_distribution"]["buckets"]
                self.logger.info(f"é¡å‹åˆ†ä½ˆ: {type_buckets}")
            if "product_distribution" in result["aggregations"]:
                product_buckets = result["aggregations"]["product_distribution"][
                    "buckets"
                ]
                self.logger.info(f"ç”¢å“åˆ†ä½ˆ: {product_buckets[:5]}")  # åªé¡¯ç¤ºå‰5å€‹

        return result

    def vector_search(
        self, query: str, index_pattern: str, size: int = 5
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå‘é‡æœå°‹
        åŸºæ–¼èªæ„ç›¸ä¼¼åº¦

        Args:
            query: æŸ¥è©¢å­—ä¸²
            index_pattern: ç´¢å¼•æ¨¡å¼
            size: è¿”å›çµæœæ•¸

        Returns:
            æœå°‹çµæœ
        """
        # ç”ŸæˆæŸ¥è©¢å‘é‡
        query_vector = self.vector_gen.generate(query)
        if not query_vector:
            self.logger.warning("å‘é‡ç”Ÿæˆå¤±æ•—ï¼Œè¿”å›ç©ºçµæœ")
            return {"hits": {"hits": [], "total": {"value": 0}}}

        # æ§‹å»º KNN æŸ¥è©¢
        search_body = {
            "size": size,
            "_source": {"excludes": ["content_vector"]},
            "knn": {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": size,
                "num_candidates": size * 10,
            },
        }

        self.logger.info(f"åŸ·è¡Œå‘é‡æœå°‹: {query}")
        return self.es_client.search(index_pattern, search_body)

    def hybrid_search(
        self, query: str, index_pattern: str, size: int = 5
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ··åˆæœå°‹ - å„ªåŒ–ç‰ˆæœ¬
        çµåˆé—œéµå­—å’Œå‘é‡æœå°‹ï¼Œç‰¹åˆ¥å„ªåŒ–ç‹€æ…‹æœå°‹

        Args:
            query: æŸ¥è©¢å­—ä¸²
            index_pattern: ç´¢å¼•æ¨¡å¼
            size: è¿”å›çµæœæ•¸

        Returns:
            æœå°‹çµæœ
        """
        # æº–å‚™æŸ¥è©¢è®Šé«”
        _, query_variants = self.text_processor.prepare_search_query(query)

        # ç”ŸæˆæŸ¥è©¢å‘é‡
        query_vector = self.vector_gen.generate(query)

        # æ§‹å»ºæ··åˆæŸ¥è©¢ - å„ªåŒ–ç‰ˆæœ¬
        search_body = {
            "size": size * 3,  # å–æ›´å¤šçµæœå¾Œé‡æ’åº
            "_source": {"excludes": ["content_vector"]},
            "query": {
                "bool": {
                    "should": [
                        # é—œéµå­—æœå°‹éƒ¨åˆ†ï¼ˆæé«˜æ¬Šé‡ï¼‰
                        {
                            "bool": {
                                "should": [
                                    # ç²¾ç¢ºåŒ¹é…ç‹€æ…‹
                                    {
                                        "term": {
                                            "field_status.keyword": {
                                                "value": variant,
                                                "boost": 10.0,
                                            }
                                        }
                                    }
                                    for variant in query_variants
                                ]
                                + [
                                    # çŸ­èªåŒ¹é…
                                    {
                                        "match_phrase": {
                                            "all_content": {
                                                "query": query,
                                                "boost": 3.0,
                                            }
                                        }
                                    }
                                ]
                                + [
                                    # å¤šæ¬„ä½æœå°‹
                                    {
                                        "multi_match": {
                                            "query": variant,
                                            "fields": [
                                                "field_status^15",
                                                "field_complaint_status^15",
                                                "field_complaint_id^10",
                                                "field_product_id^8",
                                                "field_product_name^8",
                                                "field_description^5",
                                                "searchable_content^3",
                                                "all_content^2",
                                                "field_*",
                                            ],
                                            "type": "best_fields",
                                            "analyzer": "ik_smart",
                                            "boost": 0.7,  # æ··åˆæ¨¡å¼ä¸­é™ä½é—œéµå­—æ¬Šé‡
                                        }
                                    }
                                    for variant in query_variants
                                ],
                                "minimum_should_match": 1,
                                "boost": 0.6,  # æ•´é«”é—œéµå­—éƒ¨åˆ†æ¬Šé‡
                            }
                        }
                    ]
                }
            },
            "highlight": {
                "fields": {
                    "field_status": {"fragment_size": 50},
                    "searchable_content": {"fragment_size": 150},
                    "all_content": {"fragment_size": 150},
                    "field_*": {"fragment_size": 100},
                }
            },
        }

        # å¦‚æœæœ‰å‘é‡ï¼ŒåŠ å…¥å‘é‡æœå°‹
        if query_vector:
            search_body["knn"] = {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": size * 2,  # å¢åŠ å€™é¸æ•¸é‡
                "num_candidates": size * 20,  # å¢åŠ å€™é¸æ± å¤§å°
                "boost": 0.4,  # æ··åˆæ¨¡å¼ä¸­çš„å‘é‡æ¬Šé‡
            }

        self.logger.info(f"åŸ·è¡Œæ··åˆæœå°‹: {query}")
        result = self.es_client.search(index_pattern, search_body)

        # é‡æ–°æ’åºå’Œå»é‡
        hits_by_id = {}
        for hit in result.get("hits", {}).get("hits", []):
            doc_id = hit["_source"].get("field_complaint_id", hit["_id"])

            # å¦‚æœå·²å­˜åœ¨ï¼Œä¿ç•™åˆ†æ•¸è¼ƒé«˜çš„
            if doc_id not in hits_by_id or hit["_score"] > hits_by_id[doc_id]["_score"]:
                hits_by_id[doc_id] = hit

        # æŒ‰åˆ†æ•¸æ’åºä¸¦é™åˆ¶æ•¸é‡
        sorted_hits = sorted(
            hits_by_id.values(), key=lambda x: x["_score"], reverse=True
        )
        result["hits"]["hits"] = sorted_hits[:size]

        self.logger.info(
            f"æ··åˆæœå°‹çµæœ: åŸå§‹ {len(result.get('hits', {}).get('hits', []))} ç­†ï¼Œå»é‡å¾Œ {len(sorted_hits)} ç­†ï¼Œè¿”å› {len(result['hits']['hits'])} ç­†"
        )

        return result


# ============================================================================
# ç­”æ¡ˆç”Ÿæˆå™¨
# ============================================================================


class AnswerGenerator:
    """
    ç­”æ¡ˆç”Ÿæˆå™¨
    ä½¿ç”¨ GPT æ ¹æ“šæœå°‹çµæœç”Ÿæˆè‡ªç„¶èªè¨€ç­”æ¡ˆ
    """

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–ç­”æ¡ˆç”Ÿæˆå™¨

        Args:
            config: ç³»çµ±é…ç½®
        """
        self.config = config
        self.client = None
        self.logger = logging.getLogger(self.__class__.__name__)

        if config.openai_api_key:
            try:
                self.client = OpenAI(
                    api_key=config.openai_api_key, base_url=config.openai_base_url
                )
                self.logger.info(f"âœ… GPT å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {config.gpt_model}")
            except Exception as e:
                self.logger.error(f"âŒ GPT å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")

    def format_context(
        self, search_results: Dict[str, Any], max_contexts: int = 5
    ) -> str:
        """
        æ ¼å¼åŒ–æœå°‹çµæœç‚ºä¸Šä¸‹æ–‡

        Args:
            search_results: ES æœå°‹çµæœ
            max_contexts: æœ€å¤šä½¿ç”¨çš„æ–‡æª”æ•¸

        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ä¸²
        """
        contexts = []
        hits = search_results.get("hits", {}).get("hits", [])[:max_contexts]

        for hit in hits:
            source = hit["_source"]
            metadata = source.get("metadata", {})

            # æ”¶é›†é‡è¦æ¬„ä½
            content_parts = []

            # å„ªå…ˆä½¿ç”¨é«˜äº®å…§å®¹
            if "highlight" in hit:
                for field, highlights in hit["highlight"].items():
                    # åªå–å‰å…©å€‹é«˜äº®ç‰‡æ®µ
                    content_parts.extend(highlights[:2])

            # å¦‚æœæ²’æœ‰é«˜äº®ï¼Œä½¿ç”¨åŸå§‹å…§å®¹
            if not content_parts:
                if source.get("searchable_content"):
                    content_parts.append(source["searchable_content"][:300])
                elif source.get("all_content"):
                    content_parts.append(source["all_content"][:300])

            # çµ„åˆä¸Šä¸‹æ–‡
            context = f"ã€ä¾†æºæª”æ¡ˆ: {metadata.get('source_file', 'æœªçŸ¥')}, "
            context += f"è³‡æ–™è¡¨: {metadata.get('table_name', 'æœªçŸ¥')}ã€‘\n"
            context += "\n".join(content_parts)
            contexts.append(context)

        return "\n\n---\n\n".join(contexts)

    def generate(self, query: str, context: str, temperature: float = 0.7) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ

        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            context: æœå°‹çµæœä¸Šä¸‹æ–‡
            temperature: ç”Ÿæˆæº«åº¦

        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        if not self.client:
            return "æŠ±æ­‰ï¼ŒGPT æœå‹™ç›®å‰ä¸å¯ç”¨ã€‚"

        # ç³»çµ±æç¤ºè©
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è³‡æ–™åº«æŸ¥è©¢åŠ©æ‰‹ï¼Œæ“…é•·åˆ†æä¼æ¥­è³‡æ–™ä¸¦æä¾›æº–ç¢ºçš„ç­”æ¡ˆã€‚
        
è«‹éµå¾ªä»¥ä¸‹è¦å‰‡ï¼š
1. åƒ…æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šå›ç­”ï¼Œä¸è¦ç·¨é€ æˆ–æ¨æ¸¬
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€Œæ ¹æ“šç¾æœ‰è³‡æ–™ç„¡æ³•å›ç­”æ­¤å•é¡Œã€
3. å›ç­”è¦æº–ç¢ºã€å…·é«”ã€æœ‰æ¢ç†
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
5. å¦‚æœæ¶‰åŠæ•¸æ“šæˆ–å…·é«”è³‡è¨Šï¼Œè«‹å¼•ç”¨ä¾†æº
6. é©ç•¶ä½¿ç”¨é …ç›®ç¬¦è™Ÿæˆ–ç·¨è™Ÿä¾†çµ„ç¹”è³‡è¨Š
7. ä¿æŒå°ˆæ¥­ä½†å‹å–„çš„èªæ°£"""

        # ç”¨æˆ¶æç¤ºè©
        user_prompt = f"""å•é¡Œï¼š{query}

ç›¸é—œè³‡æ–™ï¼š
{context}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™å›ç­”å•é¡Œã€‚å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹èªªæ˜éœ€è¦å“ªäº›é¡å¤–è³‡è¨Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=self.config.gpt_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=temperature,
                max_tokens=1000,
                top_p=0.95,
                frequency_penalty=0.5,
                presence_penalty=0.5,
            )

            answer = response.choices[0].message.content
            self.logger.info(f"æˆåŠŸç”Ÿæˆç­”æ¡ˆï¼Œé•·åº¦: {len(answer)} å­—å…ƒ")
            return answer

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}")
            return f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"


# ============================================================================
# RAG æœå‹™ä¸»é¡åˆ¥
# ============================================================================


class RAGService:
    """
    RAG æœå‹™ä¸»é¡åˆ¥
    æ•´åˆæ‰€æœ‰å…ƒä»¶æä¾›å®Œæ•´çš„ RAG åŠŸèƒ½
    """

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ– RAG æœå‹™

        Args:
            config: ç³»çµ±é…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

        # åˆå§‹åŒ–å„å…ƒä»¶
        self.text_processor = TextProcessor()
        self.es_client = ElasticsearchClient(config)
        self.vector_gen = VectorGenerator(config)
        self.search_engine = SearchEngine(
            self.es_client, self.vector_gen, self.text_processor
        )
        self.answer_gen = AnswerGenerator(config)

        self.logger.info("âœ… RAG æœå‹™åˆå§‹åŒ–å®Œæˆ")

    def process_query(self, request: QueryRequest) -> QueryResponse:
        """
        è™•ç†æŸ¥è©¢è«‹æ±‚

        Args:
            request: æŸ¥è©¢è«‹æ±‚

        Returns:
            æŸ¥è©¢å›æ‡‰
        """
        start_time = datetime.now()

        # è™•ç†æŸ¥è©¢å­—ä¸²ï¼ˆç°¡ç¹è½‰æ›ï¼‰
        processed_query, _ = self.text_processor.prepare_search_query(
            request.query, request.convert_to_traditional
        )

        self.logger.info(f"è™•ç†æŸ¥è©¢: {request.query} -> {processed_query}")

        # åŸ·è¡Œæœå°‹
        if request.mode == SearchMode.KEYWORD:
            search_results = self.search_engine.keyword_search(
                request.query, request.index_pattern, request.top_k
            )
        elif request.mode == SearchMode.VECTOR:
            search_results = self.search_engine.vector_search(
                request.query, request.index_pattern, request.top_k
            )
        else:  # HYBRID
            search_results = self.search_engine.hybrid_search(
                request.query, request.index_pattern, request.top_k
            )

        # æ ¼å¼åŒ–æœå°‹çµæœ
        sources = []
        for hit in search_results.get("hits", {}).get("hits", []):
            source = hit["_source"]
            customdata = source.get("metadata", {})
            customdata["status"] = source.get("status")
            sources.append(
                SearchResult(
                    score=hit.get("_score", 0),
                    index=hit["_index"],
                    metadata=customdata,
                    content=source.get("searchable_content", "")[:200],
                    highlights=hit.get("highlight", {}),
                )
            )

        # ç”Ÿæˆç­”æ¡ˆï¼ˆå¦‚æœéœ€è¦ï¼‰
        answer = None
        if request.use_gpt and sources:
            # ä¿®æ­£ï¼šä½¿ç”¨å¯¦éš›æœå°‹çµæœçš„æ•¸é‡ï¼Œç¢ºä¿æ‰€æœ‰çµæœéƒ½è¢«åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­
            actual_results_count = len(search_results.get("hits", {}).get("hits", []))

            # ä½¿ç”¨ AI ç”Ÿæˆçš„å›ç­”é è¨­æœ€å¤šæ˜¯äº”ç­†ï¼Œé€™é‚Šè‡ªå·±èª¿æ•´ï¼ŒEX: æœåç­†ï¼Œè‹¥åªæœ‰ä¸ƒç­†å°±åªé¡¯ç¤ºä¸ƒç­†
            max_contexts = min(actual_results_count, request.top_k)

            self.logger.info(
                f"GPT ä¸Šä¸‹æ–‡ä½¿ç”¨ {max_contexts} ç­†çµæœï¼ˆå…± {actual_results_count} ç­†ï¼‰"
            )

            context = self.answer_gen.format_context(
                search_results, max_contexts=max_contexts
            )
            answer = self.answer_gen.generate(
                processed_query, context, request.temperature
            )

        # è¨ˆç®—è™•ç†æ™‚é–“
        processing_time = int((datetime.now() - start_time).total_seconds() * 1000)

        return QueryResponse(
            query=request.query,
            processed_query=processed_query,
            answer=answer,
            sources=sources,
            search_mode=request.mode.value,
            total_hits=search_results.get("hits", {}).get("total", {}).get("value", 0),
            processing_time_ms=processing_time,
        )

    def health_check(self) -> HealthResponse:
        """
        åŸ·è¡Œå¥åº·æª¢æŸ¥

        Returns:
            å¥åº·ç‹€æ…‹
        """
        es_health = self.es_client.health_check()
        openai_health = self.vector_gen.health_check()

        status = "healthy" if (es_health and openai_health) else "degraded"
        if not es_health and not openai_health:
            status = "unhealthy"

        return HealthResponse(
            status=status,
            elasticsearch=es_health,
            openai=openai_health,
            timestamp=datetime.now().isoformat(),
        )

    def get_stats(self) -> Dict[str, Any]:
        """
        å–å¾—ç³»çµ±çµ±è¨ˆè³‡è¨Š

        Returns:
            çµ±è¨ˆè³‡è¨Š
        """
        return self.es_client.get_stats(self.config.default_index_pattern)


# ============================================================================
# FastAPI æ‡‰ç”¨ç¨‹å¼
# ============================================================================

# åˆå§‹åŒ–é…ç½®å’Œæ—¥èªŒ
config = Config()
logger = setup_logging()

# å»ºç«‹ FastAPI æ‡‰ç”¨
app = FastAPI(
    title=config.api_title,
    version=config.api_version,
    description="æ™ºèƒ½æª¢ç´¢å’Œå•ç­”ç³»çµ± API",
    docs_url="/docs",
    redoc_url="/redoc",
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿç”¢ç’°å¢ƒæ‡‰è©²è¨­å®šå…·é«”çš„ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– RAG æœå‹™
rag_service = None


@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶"""
    global rag_service

    logger.info("ğŸš€ æ­£åœ¨å•Ÿå‹• RAG API æœå‹™...")

    # é©—è­‰é…ç½®
    if not config.validate():
        logger.warning("âš ï¸ é…ç½®é©—è­‰å¤±æ•—ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ç„¡æ³•ä½¿ç”¨")

    # åˆå§‹åŒ– RAG æœå‹™
    rag_service = RAGService(config)

    logger.info(
        f"ğŸ“Š ä½¿ç”¨æ¨¡å‹ï¼šEmbedding={config.embedding_model}, GPT={config.gpt_model}"
    )
    logger.info(f"ğŸŒ API æ–‡æª”ï¼šhttp://{config.api_host}:{config.api_port}/docs")


# ============================================================================
# API ç«¯é»
# ============================================================================


@app.get("/", tags=["æ ¹ç›®éŒ„"])
async def root():
    """æ ¹ç›®éŒ„ç«¯é»"""
    return {
        "message": "RAG API æœå‹™é‹è¡Œä¸­",
        "version": config.api_version,
        "docs": "/docs",
    }


@app.get("/health", response_model=HealthResponse, tags=["ç³»çµ±"])
async def health_check():
    """
    å¥åº·æª¢æŸ¥ç«¯é»

    æª¢æŸ¥ç³»çµ±å„å…ƒä»¶çš„å¥åº·ç‹€æ…‹
    """
    if not rag_service:
        raise HTTPException(status_code=503, detail="æœå‹™å°šæœªåˆå§‹åŒ–")

    return rag_service.health_check()


@app.post("/query", response_model=QueryResponse, tags=["æŸ¥è©¢"])
async def query_data(request: QueryRequest):
    """
    æ™ºèƒ½æŸ¥è©¢ç«¯é»

    æ”¯æ´é—œéµå­—ã€å‘é‡å’Œæ··åˆæœå°‹æ¨¡å¼ï¼Œ
    å¯é¸æ“‡ä½¿ç”¨ GPT ç”Ÿæˆè‡ªç„¶èªè¨€ç­”æ¡ˆ
    """
    if not rag_service:
        raise HTTPException(status_code=503, detail="æœå‹™å°šæœªåˆå§‹åŒ–")

    try:
        return rag_service.process_query(request)
    except Exception as e:
        logger.error(f"æŸ¥è©¢è™•ç†å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥è©¢è™•ç†å¤±æ•—: {str(e)}")


@app.get("/stats", tags=["ç³»çµ±"])
async def get_statistics():
    """
    å–å¾—ç³»çµ±çµ±è¨ˆè³‡è¨Š

    åŒ…å«ç´¢å¼•æ•¸é‡ã€æ–‡æª”æ•¸é‡ã€å‘é‡åŒ–é€²åº¦ç­‰
    """
    if not rag_service:
        raise HTTPException(status_code=503, detail="æœå‹™å°šæœªåˆå§‹åŒ–")

    try:
        return rag_service.get_stats()
    except Exception as e:
        logger.error(f"å–å¾—çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å–å¾—çµ±è¨ˆè³‡è¨Šå¤±æ•—: {str(e)}")


# ============================================================================
# éŒ¯èª¤è™•ç†
# ============================================================================


@app.exception_handler(ValueError)
async def value_error_handler(request, exc):
    """è™•ç†å€¼éŒ¯èª¤"""
    logger.error(f"å€¼éŒ¯èª¤: {exc}")
    return JSONResponse(status_code=400, content={"detail": f"ç„¡æ•ˆçš„è¼¸å…¥: {str(exc)}"})


@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨åŸŸéŒ¯èª¤è™•ç†"""
    logger.error(f"æœªé æœŸçš„éŒ¯èª¤: {exc}", exc_info=True)
    return JSONResponse(status_code=500, content={"detail": "å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤"})


# ============================================================================
# ä¸»ç¨‹å¼å…¥å£
# ============================================================================


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    if not config.openai_api_key:
        logger.warning("âš ï¸ è­¦å‘Šï¼šæœªè¨­ç½® OPENAI_API_KEYï¼Œéƒ¨åˆ†åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

    logger.info(f"ğŸš€ å•Ÿå‹• RAG API æœå‹™")
    logger.info(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹é…ç½®ï¼š")
    logger.info(f"   - Embedding: {config.embedding_model}")
    logger.info(f"   - GPT: {config.gpt_model}")
    logger.info(f"ğŸŒ API æ–‡æª”ï¼šhttp://{config.api_host}:{config.api_port}/docs")

    # å•Ÿå‹• Uvicorn ä¼ºæœå™¨
    uvicorn.run(
        app,
        host=config.api_host,
        port=config.api_port,
        log_level="info",
        reload=False,  # ç”Ÿç”¢ç’°å¢ƒè¨­ç‚º False
    )


if __name__ == "__main__":
    main()
