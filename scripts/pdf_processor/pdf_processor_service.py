#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
PDF æ–‡æª”è™•ç†æœå‹™ - ç¦èˆˆå·¥æ¥­æŠ€è¡“æ–‡ä»¶è‡ªå‹•åŒ–è™•ç†ç³»çµ±
=============================================================================

åŠŸèƒ½æ¦‚è¿°ï¼š
1. è‡ªå‹•ç›£æ§æŒ‡å®šç›®éŒ„ä¸­çš„ PDF æª”æ¡ˆ
2. è§£æç¦èˆˆå·¥æ¥­çš„æŠ€è¡“æ–‡ä»¶ï¼ˆè¨­è®Šé€šçŸ¥ã€DFMEAã€è¦æ ¼æ›¸ç­‰ï¼‰
3. æå–çµæ§‹åŒ–è³‡æ–™ä¸¦å­˜å…¥ MySQL è³‡æ–™åº«
4. æ”¯æ´ç‹€æ…‹è¿½è¹¤ï¼Œé¿å…é‡è¤‡è™•ç†
5. å¯é¸çš„ OCR åŠŸèƒ½è™•ç†æƒææª”

ç³»çµ±æ¶æ§‹ï¼š
    PDFæª”æ¡ˆ â†’ ç›£æ§ç›®éŒ„ â†’ è§£æè™•ç† â†’ MySQL â†’ åŒæ­¥åˆ° ES â†’ RAGæª¢ç´¢

ä½œè€…: [æ‚¨çš„åœ˜éšŠ]
ç‰ˆæœ¬: 1.0.0
æ›´æ–°æ—¥æœŸ: 2024
=============================================================================
"""

import os, sys, time, json, signal, hashlib, logging, pymysql, pdfplumber, re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

# ============================================================================
# ç’°å¢ƒè®Šæ•¸é…ç½®
# ============================================================================
# é€™äº›ç’°å¢ƒè®Šæ•¸åœ¨ docker-compose.yml ä¸­è¨­å®šï¼Œæä¾›éˆæ´»çš„é…ç½®é¸é …

# MySQL é€£ç·šè¨­å®š - é€£æ¥åˆ°æ‚¨çš„è³‡æ–™åº«å®¹å™¨
MYSQL_HOST = os.getenv("MYSQL_HOST", "mysql")  # Docker ç¶²è·¯ä¸­çš„ä¸»æ©Ÿå
MYSQL_PORT = int(os.getenv("MYSQL_PORT", "3306"))
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "root")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "fuhsin_erp_demo")

# PDF æª”æ¡ˆç›®éŒ„çµæ§‹
# /mnt/pdf/
#   â”œâ”€â”€ incoming/      # æ–°æª”æ¡ˆæ”¾é€™è£¡
#   â”œâ”€â”€ .done/         # è™•ç†å®Œæˆçš„æª”æ¡ˆ
#   â”œâ”€â”€ .error/        # è™•ç†å¤±æ•—çš„æª”æ¡ˆ
#   â””â”€â”€ .processing/   # æ­£åœ¨è™•ç†çš„æª”æ¡ˆï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
PDF_WATCH_DIR = Path(os.getenv("PDF_WATCH_DIR", "/mnt/pdf/incoming"))
PDF_DONE_DIR = PDF_WATCH_DIR / ".done"
PDF_ERROR_DIR = PDF_WATCH_DIR / ".error"
PDF_PROCESSING_DIR = PDF_WATCH_DIR / ".processing"

# è™•ç†åƒæ•¸è¨­å®š
SCAN_INTERVAL = int(os.getenv("SCAN_INTERVAL", "30"))  # æƒæé–“éš”ï¼ˆç§’ï¼‰
PROCESS_BATCH_SIZE = int(os.getenv("PROCESS_BATCH_SIZE", "5"))  # æ¯æ‰¹è™•ç†æª”æ¡ˆæ•¸
ENABLE_OCR = os.getenv("ENABLE_OCR", "false").lower() == "true"  # æ˜¯å¦å•Ÿç”¨ OCR
OCR_LANG = os.getenv("OCR_LANG", "chi_tra+eng")  # OCR èªè¨€ï¼šç¹é«”ä¸­æ–‡+è‹±æ–‡

# ç‹€æ…‹å’Œæ—¥èªŒæª”æ¡ˆè·¯å¾‘
STATE_FILE = Path("/state/.pdf_processor_state.json")  # è™•ç†ç‹€æ…‹è¨˜éŒ„
LOG_FILE = Path("/logs/pdf_processor.log")  # æ—¥èªŒæª”æ¡ˆ
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# ============================================================================
# æ—¥èªŒè¨­å®š
# ============================================================================
# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(LOG_FILE.parent, exist_ok=True)
os.makedirs(STATE_FILE.parent, exist_ok=True)

# é…ç½®æ—¥èªŒæ ¼å¼å’Œè¼¸å‡º
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE),  # å¯«å…¥æª”æ¡ˆ
        logging.StreamHandler(),  # è¼¸å‡ºåˆ°æ§åˆ¶å°
    ],
)
logger = logging.getLogger(__name__)

# ============================================================================
# å„ªé›…é—œé–‰æ©Ÿåˆ¶
# ============================================================================
# å…¨åŸŸè®Šæ•¸ï¼šç”¨æ–¼æ¥æ”¶ä¸­æ–·ä¿¡è™Ÿæ™‚å„ªé›…é—œé–‰
should_stop = False


def signal_handler(signum, frame):
    """
    è™•ç†ç³»çµ±ä¸­æ–·ä¿¡è™Ÿï¼ˆSIGINT, SIGTERMï¼‰
    ç•¶æ”¶åˆ° docker stop æˆ– Ctrl+C æ™‚è§¸ç™¼
    """
    global should_stop
    logger.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæº–å‚™å„ªé›…é—œé–‰...")
    should_stop = True


# è¨»å†Šä¿¡è™Ÿè™•ç†å™¨
signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
signal.signal(signal.SIGTERM, signal_handler)  # docker stop


# ============================================================================
# è³‡æ–™æ¨¡å‹å®šç¾©
# ============================================================================
@dataclass
class TechnicalDocument:
    """
    æŠ€è¡“æ–‡æª”è³‡æ–™æ¨¡å‹

    ä½¿ç”¨ @dataclass è£é£¾å™¨è‡ªå‹•ç”Ÿæˆ __init__ã€__repr__ ç­‰æ–¹æ³•
    é€™å€‹æ¨¡å‹å°æ‡‰åˆ° MySQL çš„ technical_documents è¡¨
    """

    doc_id: str  # æ–‡æª”å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆMD5 hashï¼‰
    doc_type: str  # æ–‡æª”é¡å‹ï¼ˆECN/DFMEA/SPEC/DRAWING/COMPLAINTï¼‰
    doc_number: str  # æ–‡æª”ç·¨è™Ÿï¼ˆå¦‚ EC-K-28-C-083ï¼‰
    title: str  # æ–‡æª”æ¨™é¡Œ
    product_ids: List[str]  # ç›¸é—œç”¢å“ç·¨è™Ÿåˆ—è¡¨ï¼ˆå¦‚ ['OB1-G04313AU', 'OB1-G04313A']ï¼‰
    revision: Optional[str]  # ç‰ˆæœ¬è™Ÿ
    issue_date: Optional[str]  # ç™¼è¡Œæ—¥æœŸ
    department: Optional[str]  # éƒ¨é–€
    author: Optional[str]  # ä½œè€…/è² è²¬äºº
    content: str  # å®Œæ•´æ–‡æœ¬å…§å®¹ï¼ˆç”¨æ–¼å…¨æ–‡æœå°‹ï¼‰
    summary: Optional[str]  # è‡ªå‹•ç”Ÿæˆçš„æ‘˜è¦
    keywords: List[str]  # é—œéµå­—åˆ—è¡¨ï¼ˆç”¨æ–¼æ¨™ç±¤å’Œå¿«é€Ÿæª¢ç´¢ï¼‰
    metadata: Dict  # å…¶ä»–å…ƒè³‡æ–™ï¼ˆJSON æ ¼å¼å­˜å„²ï¼‰
    file_path: str  # åŸå§‹æª”æ¡ˆè·¯å¾‘
    file_hash: str  # æª”æ¡ˆ SHA256 é›œæ¹Šå€¼ï¼ˆç”¨æ–¼å»é‡ï¼‰
    created_at: str  # å»ºç«‹æ™‚é–“
    updated_at: str  # æ›´æ–°æ™‚é–“
    page_count: int  # PDF é æ•¸
    file_size: int  # æª”æ¡ˆå¤§å°ï¼ˆbytesï¼‰


# ============================================================================
# è³‡æ–™åº«ç®¡ç†å™¨
# ============================================================================
class DatabaseManager:
    """
    è³‡æ–™åº«é€£ç·šå’Œæ“ä½œç®¡ç†

    è² è²¬ï¼š
    1. ç®¡ç† MySQL é€£ç·š
    2. å»ºç«‹è³‡æ–™è¡¨
    3. æ’å…¥/æ›´æ–°æ–‡æª”è³‡æ–™
    4. è¨˜éŒ„è™•ç†æ—¥èªŒ
    """

    def __init__(self):
        """åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨"""
        self.connection = None
        self.create_tables()  # ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨

    def get_connection(self):
        """
        å–å¾—è³‡æ–™åº«é€£ç·šï¼ˆé€£ç·šæ± æ¦‚å¿µçš„ç°¡å–®å¯¦ç¾ï¼‰
        å¦‚æœé€£ç·šä¸å­˜åœ¨æˆ–å·²æ–·é–‹ï¼Œå»ºç«‹æ–°é€£ç·š
        """
        if not self.connection or not self.connection.open:
            logger.info("å»ºç«‹æ–°çš„è³‡æ–™åº«é€£ç·š...")
            self.connection = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE,
                charset="utf8mb4",  # æ”¯æ´å®Œæ•´çš„ Unicodeï¼ˆåŒ…æ‹¬ emojiï¼‰
                autocommit=True,  # è‡ªå‹•æäº¤äº‹å‹™
            )
        return self.connection

    def create_tables(self):
        """
        å»ºç«‹å¿…è¦çš„è³‡æ–™è¡¨
        å¦‚æœè¡¨å·²å­˜åœ¨å‰‡è·³éï¼ˆCREATE TABLE IF NOT EXISTSï¼‰
        """
        create_sql = """
        -- ä¸»è¦æ–‡æª”è¡¨ï¼šå­˜å„²æ‰€æœ‰è§£æå‡ºçš„æŠ€è¡“æ–‡ä»¶
        CREATE TABLE IF NOT EXISTS technical_documents (
            id INT AUTO_INCREMENT PRIMARY KEY,
            doc_id VARCHAR(64) UNIQUE NOT NULL COMMENT 'æ–‡æª”å”¯ä¸€ID',
            doc_type VARCHAR(20) NOT NULL COMMENT 'æ–‡æª”é¡å‹',
            doc_number VARCHAR(50) COMMENT 'æ–‡æª”ç·¨è™Ÿ',
            title VARCHAR(255) NOT NULL COMMENT 'æ¨™é¡Œ',
            product_ids JSON COMMENT 'ç›¸é—œç”¢å“ç·¨è™Ÿï¼ˆJSONé™£åˆ—ï¼‰',
            revision VARCHAR(20) COMMENT 'ç‰ˆæœ¬è™Ÿ',
            issue_date DATE COMMENT 'ç™¼è¡Œæ—¥æœŸ',
            department VARCHAR(50) COMMENT 'éƒ¨é–€',
            author VARCHAR(50) COMMENT 'ä½œè€…',
            content LONGTEXT NOT NULL COMMENT 'å®Œæ•´å…§å®¹ï¼ˆç”¨æ–¼å…¨æ–‡æœå°‹ï¼‰',
            summary TEXT COMMENT 'æ‘˜è¦',
            keywords JSON COMMENT 'é—œéµå­—ï¼ˆJSONé™£åˆ—ï¼‰',
            metadata JSON COMMENT 'å…ƒè³‡æ–™ï¼ˆJSONç‰©ä»¶ï¼‰',
            file_path VARCHAR(500) COMMENT 'æª”æ¡ˆè·¯å¾‘',
            file_hash VARCHAR(64) COMMENT 'æª”æ¡ˆé›œæ¹Š',
            page_count INT DEFAULT 0 COMMENT 'é æ•¸',
            file_size INT DEFAULT 0 COMMENT 'æª”æ¡ˆå¤§å°(bytes)',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            
            -- ç´¢å¼•å„ªåŒ–æŸ¥è©¢æ•ˆèƒ½
            INDEX idx_doc_type (doc_type),        -- æŒ‰é¡å‹æŸ¥è©¢
            INDEX idx_doc_number (doc_number),    -- æŒ‰ç·¨è™ŸæŸ¥è©¢
            INDEX idx_issue_date (issue_date),    -- æŒ‰æ—¥æœŸæŸ¥è©¢
            INDEX idx_created_at (created_at),    -- æŒ‰å»ºç«‹æ™‚é–“æ’åº
            FULLTEXT idx_content (content),       -- å…¨æ–‡æœå°‹å…§å®¹
            FULLTEXT idx_title_summary (title, summary)  -- å…¨æ–‡æœå°‹æ¨™é¡Œå’Œæ‘˜è¦
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        
        -- è™•ç†è¨˜éŒ„è¡¨ï¼šè¿½è¹¤æ¯å€‹æª”æ¡ˆçš„è™•ç†ç‹€æ…‹
        CREATE TABLE IF NOT EXISTS pdf_processing_log (
            id INT AUTO_INCREMENT PRIMARY KEY,
            file_name VARCHAR(255) NOT NULL COMMENT 'æª”æ¡ˆåç¨±',
            file_hash VARCHAR(64) COMMENT 'æª”æ¡ˆé›œæ¹Š',
            status ENUM('processing', 'success', 'error') NOT NULL COMMENT 'è™•ç†ç‹€æ…‹',
            error_message TEXT COMMENT 'éŒ¯èª¤è¨Šæ¯',
            process_time_ms INT COMMENT 'è™•ç†æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            INDEX idx_status (status),
            INDEX idx_created_at (created_at)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """

        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                # åˆ†å‰² SQL èªå¥ä¸¦é€ä¸€åŸ·è¡Œ
                for statement in create_sql.split(";"):
                    if statement.strip():
                        cursor.execute(statement)
                conn.commit()
                logger.info("âœ… è³‡æ–™è¡¨å»ºç«‹/ç¢ºèªæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ å»ºç«‹è³‡æ–™è¡¨å¤±æ•—: {e}")
            raise

    def upsert_document(self, doc: TechnicalDocument) -> bool:
        """
        æ’å…¥æˆ–æ›´æ–°æ–‡æª”ï¼ˆUPSERT æ“ä½œï¼‰

        ä½¿ç”¨ INSERT ... ON DUPLICATE KEY UPDATE èªæ³•
        å¦‚æœ doc_id å·²å­˜åœ¨å‰‡æ›´æ–°ï¼Œå¦å‰‡æ’å…¥æ–°è¨˜éŒ„

        Args:
            doc: TechnicalDocument ç‰©ä»¶

        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        sql = """
        INSERT INTO technical_documents (
            doc_id, doc_type, doc_number, title, product_ids, revision,
            issue_date, department, author, content, summary, keywords,
            metadata, file_path, file_hash, page_count, file_size
        ) VALUES (
            %(doc_id)s, %(doc_type)s, %(doc_number)s, %(title)s, 
            %(product_ids)s, %(revision)s, %(issue_date)s, %(department)s,
            %(author)s, %(content)s, %(summary)s, %(keywords)s,
            %(metadata)s, %(file_path)s, %(file_hash)s, %(page_count)s, %(file_size)s
        )
        ON DUPLICATE KEY UPDATE
            doc_type = VALUES(doc_type),
            title = VALUES(title),
            product_ids = VALUES(product_ids),
            content = VALUES(content),
            summary = VALUES(summary),
            keywords = VALUES(keywords),
            metadata = VALUES(metadata),
            page_count = VALUES(page_count),
            file_size = VALUES(file_size),
            updated_at = CURRENT_TIMESTAMP
        """

        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                # å°‡ dataclass è½‰æ›ç‚ºå­—å…¸
                doc_dict = asdict(doc)

                # å°‡ Python åˆ—è¡¨/å­—å…¸è½‰æ›ç‚º JSON å­—ä¸²
                doc_dict["product_ids"] = json.dumps(
                    doc.product_ids, ensure_ascii=False
                )
                doc_dict["keywords"] = json.dumps(doc.keywords, ensure_ascii=False)
                doc_dict["metadata"] = json.dumps(doc.metadata, ensure_ascii=False)

                cursor.execute(sql, doc_dict)
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"è³‡æ–™åº«å¯«å…¥éŒ¯èª¤: {e}")
            return False

    def log_processing(
        self,
        file_name: str,
        file_hash: str,
        status: str,
        error_message: str = None,
        process_time_ms: int = 0,
    ):
        """
        è¨˜éŒ„è™•ç†ç‹€æ…‹åˆ°æ—¥èªŒè¡¨
        ç”¨æ–¼è¿½è¹¤å’Œè¨ºæ–·å•é¡Œ
        """
        sql = """
        INSERT INTO pdf_processing_log 
        (file_name, file_hash, status, error_message, process_time_ms)
        VALUES (%s, %s, %s, %s, %s)
        """
        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                cursor.execute(
                    sql, (file_name, file_hash, status, error_message, process_time_ms)
                )
                conn.commit()
        except Exception as e:
            logger.error(f"è¨˜éŒ„è™•ç†æ—¥èªŒå¤±æ•—: {e}")

    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£ç·š"""
        if self.connection:
            self.connection.close()
            logger.info("è³‡æ–™åº«é€£ç·šå·²é—œé–‰")


# ============================================================================
# PDF è§£æå™¨
# ============================================================================
class PDFParser:
    """
    PDF æ–‡ä»¶è§£æå™¨ - å°ˆé–€é‡å°ç¦èˆˆå·¥æ¥­æ–‡ä»¶å„ªåŒ–

    ä¸»è¦åŠŸèƒ½ï¼š
    1. æå– PDF æ–‡å­—å…§å®¹ï¼ˆåŸç”Ÿæ–‡å­—å±¤ï¼‰
    2. è­˜åˆ¥æ–‡æª”é¡å‹ï¼ˆè¨­è®Šé€šçŸ¥ã€DFMEA ç­‰ï¼‰
    3. æå–ç”¢å“ç·¨è™Ÿï¼ˆä½¿ç”¨æ­£å‰‡è¡¨é”å¼ï¼‰
    4. ç”Ÿæˆæ‘˜è¦å’Œé—œéµå­—
    5. å¯é¸çš„ OCR è™•ç†ï¼ˆæƒææª”ï¼‰
    """

    # ç¦èˆˆç”¢å“ç·¨è™Ÿçš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼
    # é€™äº›æ¨¡å¼åŸºæ–¼æ‚¨æä¾›çš„ PDF æ–‡ä»¶ä¸­çš„å¯¦éš›ç”¢å“ç·¨è™Ÿæ ¼å¼
    PRODUCT_PATTERNS = [
        r"OB\d-[A-Z0-9]+",  # ä¾‹ï¼šOB1-G04313AU, OB1-G04313A
        r"[FG]\d{2}-[A-Z0-9]+",  # ä¾‹ï¼šF05-L0Y513, G05-L05513
        r"[PW]\d{3}",  # ä¾‹ï¼šP001, W002ï¼ˆç”¢å“/å€‰åº«ç·¨è™Ÿï¼‰
        r"EC-K-\d{2}-[A-Z]-\d{3}",  # ä¾‹ï¼šEC-K-28-C-083ï¼ˆæ–‡ä»¶ç·¨è™Ÿï¼‰
        r"L\d{6}[A-Z]?\d?",  # ä¾‹ï¼šL113055R2, L112078ï¼ˆè®Šæ›´å–®è™Ÿï¼‰
    ]

    # æ–‡æª”é¡å‹è­˜åˆ¥é—œéµå­—
    # ç”¨æ–¼è‡ªå‹•åˆ†é¡æ–‡æª”
    DOC_TYPE_KEYWORDS = {
        "ECN": ["è¨­è®Šé€šçŸ¥", "è¨­è¨ˆè®Šæ›´", "ECN", "Engineering Change"],
        "DFMEA": ["DFMEA", "å¤±æ•ˆæ¨¡å¼", "é¢¨éšªåˆ†æ"],
        "SPEC": ["è¦æ ¼", "è¦ç¯„", "Specification", "æŠ€è¡“è¦æ±‚"],
        "DRAWING": ["åœ–é¢", "åœ–ç´™", "Drawing", "å·¥ç¨‹åœ–"],
        "COMPLAINT": ["å®¢è¨´", "å®¢æˆ¶æŠ±æ€¨", "é¡§å®¢æŠ±æ€¨", "Complaint"],
        "REPORT": ["å ±å‘Š", "æ¸¬è©¦", "Report", "Test"],
    }

    @classmethod
    def extract_text(cls, pdf_path: Path) -> tuple[str, int]:
        """
        æå– PDF å…¨æ–‡å’Œé æ•¸ - æ™ºèƒ½ç‰ˆæœ¬
        
        ç­–ç•¥ï¼š
        1. å…ˆå˜—è©¦ç›´æ¥æå–æ–‡å­—
        2. å¦‚æœæ²’æœ‰æ–‡å­—ï¼Œè‡ªå‹•ä½¿ç”¨ OCR
        3. æ··åˆå‹ PDFï¼šçµåˆæ–‡å­—å±¤å’Œ OCR
        
        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘
            
        Returns:
            tuple: (æå–çš„æ–‡å­—, é æ•¸)
        """
        text = ""
        page_count = 0
        pages_with_text = 0
        pages_without_text = 0
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                page_count = len(pdf.pages)
                logger.info(f"é–‹å§‹è™•ç† PDFï¼Œå…± {page_count} é ")
                
                # ç¬¬ä¸€éï¼šå˜—è©¦ç›´æ¥æå–æ–‡å­—
                page_texts = []
                for i, page in enumerate(pdf.pages, 1):
                    # æå–é é¢æ–‡å­—
                    page_text = page.extract_text() or ""
                    
                    # æå–è¡¨æ ¼
                    tables = page.extract_tables()
                    table_text = ""
                    for table in tables:
                        for row in table:
                            if row:
                                table_text += " | ".join(str(cell) if cell else "" for cell in row) + "\n"
                    
                    combined_text = page_text + "\n" + table_text
                    page_texts.append(combined_text.strip())
                    
                    if combined_text.strip():
                        pages_with_text += 1
                    else:
                        pages_without_text += 1
                
                # åˆ¤æ–· PDF é¡å‹
                text_percentage = pages_with_text / page_count if page_count > 0 else 0
                
                if text_percentage >= 0.8:
                    # 80% ä»¥ä¸Šé é¢æœ‰æ–‡å­— â†’ æ–‡å­—å‹ PDF
                    logger.info(f"âœ… æ–‡å­—å‹ PDFï¼ˆ{pages_with_text}/{page_count} é æœ‰æ–‡å­—ï¼‰")
                    text = "\n".join(page_texts)
                    
                elif text_percentage <= 0.2:
                    # 20% ä»¥ä¸‹é é¢æœ‰æ–‡å­— â†’ æƒæå‹ PDF
                    logger.info(f"âš ï¸ æƒæå‹ PDFï¼ˆåƒ… {pages_with_text}/{page_count} é æœ‰æ–‡å­—ï¼‰")
                    
                    # è‡ªå‹•å˜—è©¦ OCRï¼ˆä¸ç®¡ ENABLE_OCR è¨­å®šï¼‰
                    logger.info("è‡ªå‹•å•Ÿç”¨ OCR è™•ç†...")
                    ocr_text = cls.extract_text_with_ocr_smart(pdf_path, skip_pages_with_text=False)
                    if ocr_text:
                        text = ocr_text
                        logger.info(f"âœ… OCR æˆåŠŸæå– {len(ocr_text)} å­—å…ƒ")
                    else:
                        # OCR ä¹Ÿå¤±æ•—ï¼Œä½¿ç”¨åŸæœ¬æå–çš„å°‘é‡æ–‡å­—
                        text = "\n".join(page_texts)
                        logger.warning("OCR è™•ç†å¤±æ•—ï¼Œä½¿ç”¨éƒ¨åˆ†æå–çš„æ–‡å­—")
                        
                else:
                    # æ··åˆå‹ PDF
                    logger.info(f"ğŸ”€ æ··åˆå‹ PDFï¼ˆ{pages_with_text}/{page_count} é æœ‰æ–‡å­—ï¼‰")
                    
                    # å°æ²’æœ‰æ–‡å­—çš„é é¢ä½¿ç”¨ OCR
                    mixed_texts = []
                    for i, page_text in enumerate(page_texts):
                        if page_text:
                            mixed_texts.append(page_text)
                        else:
                            logger.info(f"å°ç¬¬ {i+1} é ä½¿ç”¨ OCR...")
                            ocr_text = cls.extract_single_page_ocr(pdf_path, i+1)
                            mixed_texts.append(ocr_text)
                    
                    text = "\n".join(mixed_texts)
                    
        except Exception as e:
            logger.error(f"PDFæ–‡å­—æå–éŒ¯èª¤ {pdf_path}: {e}")
        
        # å¦‚æœå®Œå…¨æ²’æœ‰æå–åˆ°æ–‡å­—ï¼Œæœ€å¾Œå˜—è©¦ OCR
        if not text and page_count > 0:
            logger.warning("å®Œå…¨ç„¡æ³•æå–æ–‡å­—ï¼Œé€²è¡Œæœ€å¾Œ OCR å˜—è©¦...")
            try:
                text = cls.extract_text_with_ocr_smart(pdf_path)
                if text:
                    logger.info(f"âœ… æœ€å¾Œ OCR å˜—è©¦æˆåŠŸï¼š{len(text)} å­—å…ƒ")
            except Exception as e:
                logger.error(f"æœ€å¾Œ OCR å˜—è©¦å¤±æ•—: {e}")
        
        # å›å‚³çµæœ
        if text:
            logger.info(f"ğŸ“„ æˆåŠŸæå–æ–‡å­—ï¼š{len(text)} å­—å…ƒï¼Œ{page_count} é ")
        else:
            logger.error(f"âŒ ç„¡æ³•æå–ä»»ä½•æ–‡å­—")
        
        return text, page_count


    @classmethod
    def extract_text_with_ocr_smart(cls, pdf_path: Path, skip_pages_with_text: bool = True) -> str:
        """
        æ™ºèƒ½ OCR è™•ç†
        
        Args:
            pdf_path: PDF è·¯å¾‘
            skip_pages_with_text: æ˜¯å¦è·³éå·²æœ‰æ–‡å­—çš„é é¢
            
        Returns:
            str: OCR è­˜åˆ¥çš„æ–‡å­—
        """
        try:
            import pytesseract
            from pdf2image import convert_from_path
            
            logger.info(f"é–‹å§‹æ™ºèƒ½ OCR è™•ç†...")
            
            # å…ˆæª¢æŸ¥å“ªäº›é é¢éœ€è¦ OCR
            pages_need_ocr = []
            if skip_pages_with_text:
                with pdfplumber.open(pdf_path) as pdf:
                    for i, page in enumerate(pdf.pages):
                        if not page.extract_text():
                            pages_need_ocr.append(i + 1)
            else:
                # è™•ç†æ‰€æœ‰é é¢
                with pdfplumber.open(pdf_path) as pdf:
                    pages_need_ocr = list(range(1, len(pdf.pages) + 1))
            
            if not pages_need_ocr:
                logger.info("æ²’æœ‰é é¢éœ€è¦ OCR")
                return ""
            
            logger.info(f"éœ€è¦ OCR çš„é é¢: {pages_need_ocr}")
            
            text = ""
            for page_num in pages_need_ocr:
                # è½‰æ›å–®é ç‚ºåœ–ç‰‡
                images = convert_from_path(
                    str(pdf_path), 
                    dpi=200,  # 200 DPI æ˜¯å¹³è¡¡è³ªé‡å’Œé€Ÿåº¦çš„å¥½é¸æ“‡
                    first_page=page_num,
                    last_page=page_num
                )
                
                if images:
                    logger.info(f"  OCR è™•ç†ç¬¬ {page_num} é ...")
                    page_text = pytesseract.image_to_string(
                        images[0], 
                        lang='chi_tra+eng',  # ç¹é«”ä¸­æ–‡ + è‹±æ–‡
                        config='--psm 3'  # è‡ªå‹•é é¢åˆ†å‰²
                    )
                    text += f"\n--- ç¬¬ {page_num} é  (OCR) ---\n"
                    text += page_text + "\n"
            
            return text
            
        except ImportError:
            logger.error("OCR ç›¸é—œå¥—ä»¶æœªå®‰è£")
            return ""
        except Exception as e:
            logger.error(f"OCR è™•ç†éŒ¯èª¤: {e}")
            return ""


    @classmethod
    def extract_single_page_ocr(cls, pdf_path: Path, page_num: int) -> str:
        """
        å°å–®ä¸€é é¢é€²è¡Œ OCR
        
        Args:
            pdf_path: PDF è·¯å¾‘
            page_num: é ç¢¼ï¼ˆ1 é–‹å§‹ï¼‰
            
        Returns:
            str: OCR æ–‡å­—
        """
        try:
            import pytesseract
            from pdf2image import convert_from_path
            
            images = convert_from_path(
                str(pdf_path),
                dpi=200,
                first_page=page_num,
                last_page=page_num
            )
            
            if images:
                return pytesseract.image_to_string(images[0], lang='chi_tra+eng')
            return ""
            
        except Exception as e:
            logger.error(f"å–®é  OCR å¤±æ•— (é  {page_num}): {e}")
            return ""
    @classmethod
    def detect_doc_type(cls, text: str, filename: str) -> str:
        """
        è­˜åˆ¥æ–‡æª”é¡å‹

        ç­–ç•¥ï¼š
        1. å…ˆæª¢æŸ¥æª”å
        2. å†æª¢æŸ¥å…§å®¹é—œéµå­—
        3. ç„¡æ³•è­˜åˆ¥å‰‡è¿”å› 'GENERAL'

        Args:
            text: æ–‡æª”å…§å®¹
            filename: æª”æ¡ˆåç¨±

        Returns:
            str: æ–‡æª”é¡å‹
        """
        text_lower = text.lower()
        filename_lower = filename.lower()

        # æª¢æŸ¥æ¯ç¨®æ–‡æª”é¡å‹çš„é—œéµå­—
        for doc_type, keywords in cls.DOC_TYPE_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in text_lower or keyword.lower() in filename_lower:
                    logger.debug(f"è­˜åˆ¥ç‚º {doc_type} é¡å‹ï¼ˆé—œéµå­—ï¼š{keyword}ï¼‰")
                    return doc_type

        # ç„¡æ³•è­˜åˆ¥ï¼Œè¿”å›é€šç”¨é¡å‹
        return "GENERAL"

    @classmethod
    def extract_product_ids(cls, text: str) -> List[str]:
        """
        æå–ç”¢å“ç·¨è™Ÿ

        ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…ç¦èˆˆçš„ç”¢å“ç·¨è™Ÿæ ¼å¼
        å»é‡ä¸¦æ’åºå¾Œè¿”å›

        Args:
            text: æ–‡æª”å…§å®¹

        Returns:
            list: ç”¢å“ç·¨è™Ÿåˆ—è¡¨
        """
        product_ids = set()

        for pattern in cls.PRODUCT_PATTERNS:
            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
            matches = re.findall(pattern, text, re.IGNORECASE)
            # è½‰æ›ç‚ºå¤§å¯«ä¸¦åŠ å…¥é›†åˆï¼ˆè‡ªå‹•å»é‡ï¼‰
            product_ids.update(m.upper() for m in matches)

        result = sorted(list(product_ids))[:20]  # æœ€å¤šä¿ç•™20å€‹
        logger.debug(f"æ‰¾åˆ° {len(result)} å€‹ç”¢å“ç·¨è™Ÿ: {result[:5]}...")

        return result

    @classmethod
    def extract_metadata(cls, text: str, doc_type: str) -> Dict:
        """
        æå–å…ƒè³‡æ–™ï¼ˆæ—¥æœŸã€ç‰ˆæœ¬è™Ÿç­‰ï¼‰

        Args:
            text: æ–‡æª”å…§å®¹
            doc_type: æ–‡æª”é¡å‹

        Returns:
            dict: å…ƒè³‡æ–™å­—å…¸
        """
        metadata = {}

        # æå–æ—¥æœŸï¼ˆæ”¯æ´å¤šç¨®æ ¼å¼ï¼‰
        date_patterns = [
            r"\d{4}/\d{1,2}/\d{1,2}",  # 2024/4/25
            r"\d{4}-\d{1,2}-\d{1,2}",  # 2024-04-25
        ]
        for pattern in date_patterns:
            dates = re.findall(pattern, text)
            if dates:
                metadata["dates"] = dates[:5]  # æœ€å¤šä¿ç•™5å€‹æ—¥æœŸ
                logger.debug(f"æ‰¾åˆ°æ—¥æœŸ: {dates[:3]}")
                break

        # æå–ç‰ˆæœ¬è™Ÿï¼ˆRev A, R2 ç­‰æ ¼å¼ï¼‰
        revision_match = re.search(r"[Rr]ev(?:ision)?[:\s]*([A-Z0-9]+)", text)
        if revision_match:
            metadata["revision"] = revision_match.group(1)
            logger.debug(f"æ‰¾åˆ°ç‰ˆæœ¬è™Ÿ: {metadata['revision']}")

        # æ ¹æ“šæ–‡æª”é¡å‹æå–ç‰¹å®šè³‡è¨Š
        if doc_type == "ECN":
            # è¨­è®Šé€šçŸ¥ç‰¹æœ‰ï¼šè®Šæ›´åŸå› 
            reason_match = re.search(r"åŸå› [ï¼š:]\s*([^\n]+)", text)
            if reason_match:
                metadata["change_reason"] = reason_match.group(1)

        elif doc_type == "DFMEA":
            # DFMEA ç‰¹æœ‰ï¼šåš´é‡åº¦è©•åˆ†
            severity_match = re.search(r"åš´é‡åº¦[ï¼š:]\s*(\d+)", text)
            if severity_match:
                metadata["severity"] = int(severity_match.group(1))

        return metadata

    @classmethod
    def extract_keywords(cls, text: str, product_ids: List[str]) -> List[str]:
        """
        æå–é—œéµå­—

        ç­–ç•¥ï¼š
        1. æª¢æŸ¥é å®šç¾©çš„æŠ€è¡“é—œéµå­—
        2. åŠ å…¥ç”¢å“ç·¨è™Ÿä½œç‚ºé—œéµå­—

        Args:
            text: æ–‡æª”å…§å®¹
            product_ids: ç”¢å“ç·¨è™Ÿåˆ—è¡¨

        Returns:
            list: é—œéµå­—åˆ—è¡¨
        """
        keywords = []

        # ç¦èˆˆç‰¹å®šæŠ€è¡“é—œéµå­—ï¼ˆåŸºæ–¼æ‚¨æä¾›çš„ PDF å…§å®¹ï¼‰
        tech_keywords = [
            "æ’ç·š",
            "å…§å´çµ„åˆ",
            "å¤–å´çµ„åˆ",
            "å½ˆç°§",
            "å¥—ç›¤",
            "æŠŠæ‰‹",
            "WiFi",
            "deadbolt",
            "è½‰è»¸",
            "å…§è»¸ç­’",
            "åº•æ¿",
            "è£é£¾",
            "æ¸¬è©¦",
            "å“è³ª",
            "è¦æ ¼",
            "å…¬å·®",
            "å°ºå¯¸",
            "æè³ª",
            "Hubspace",
            "è¨­è®Š",
            "æ”¹å–„",
            "å„ªåŒ–",
            "ä¸è‰¯",
            "çŸ¯æ­£",
        ]

        # æª¢æŸ¥æ¯å€‹é—œéµå­—æ˜¯å¦å‡ºç¾åœ¨æ–‡æª”ä¸­
        for keyword in tech_keywords:
            if keyword in text:
                keywords.append(keyword)

        # åŠ å…¥å‰5å€‹ç”¢å“ç·¨è™Ÿä½œç‚ºé—œéµå­—
        keywords.extend(product_ids[:5])

        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        result = list(set(keywords))[:15]
        logger.debug(f"æå– {len(result)} å€‹é—œéµå­—")

        return result

    @classmethod
    def generate_summary(cls, text: str, max_length: int = 300) -> str:
        """
        ç”Ÿæˆæ–‡æª”æ‘˜è¦

        ç­–ç•¥ï¼š
        1. å„ªå…ˆæå–åŒ…å«é‡è¦é—œéµå­—çš„å¥å­
        2. å¦‚æœæ²’æœ‰ï¼Œå‰‡å–æ–‡æª”é–‹é ­éƒ¨åˆ†

        Args:
            text: æ–‡æª”å…§å®¹
            max_length: æ‘˜è¦æœ€å¤§é•·åº¦

        Returns:
            str: æ‘˜è¦æ–‡å­—
        """
        # æ¸…ç†æ–‡æœ¬ï¼ˆåˆä½µç©ºç™½å­—å…ƒï¼‰
        text = re.sub(r"\s+", " ", text)
        text = text.strip()

        # å„ªå…ˆæå–åŒ…å«é‡è¦é—œéµå­—çš„å¥å­
        important_patterns = [
            r"[^ã€‚ï¼ï¼Ÿ\n]*(?:è®Šæ›´|ä¿®æ”¹|èª¿æ•´|æ”¹å–„|å„ªåŒ–)[^ã€‚ï¼ï¼Ÿ\n]*",
            r"[^ã€‚ï¼ï¼Ÿ\n]*(?:å•é¡Œ|ç¼ºé™·|ä¸è‰¯|ç•°å¸¸)[^ã€‚ï¼ï¼Ÿ\n]*",
        ]

        summary_sentences = []
        for pattern in important_patterns:
            matches = re.findall(pattern, text)
            summary_sentences.extend(matches[:2])  # æ¯ç¨®é¡å‹æœ€å¤šå–2å¥

        if summary_sentences:
            summary = "ã€‚".join(summary_sentences)[:max_length]
        else:
            # æ²’æœ‰æ‰¾åˆ°é‡è¦å¥å­ï¼Œå–å‰ max_length å­—å…ƒ
            summary = text[:max_length]

        if len(summary) == max_length:
            summary += "..."

        return summary


# ============================================================================
# ä¸»è™•ç†æœå‹™
# ============================================================================
class PDFProcessorService:
    """
    PDF è™•ç†æœå‹™ä¸»é¡

    è·è²¬ï¼š
    1. ç›£æ§ PDF æª”æ¡ˆç›®éŒ„
    2. å”èª¿è§£æå’Œå­˜å„²æµç¨‹
    3. ç®¡ç†è™•ç†ç‹€æ…‹
    4. è™•ç†éŒ¯èª¤å’Œé‡è©¦
    """

    def __init__(self):
        """åˆå§‹åŒ–æœå‹™"""
        self.db = DatabaseManager()
        self.state = self.load_state()
        self.setup_directories()
        logger.info("PDF è™•ç†æœå‹™åˆå§‹åŒ–å®Œæˆ")

    def setup_directories(self):
        """
        å»ºç«‹å¿…è¦çš„ç›®éŒ„çµæ§‹
        ç¢ºä¿æ‰€æœ‰å·¥ä½œç›®éŒ„å­˜åœ¨
        """
        for dir_path in [
            PDF_WATCH_DIR,
            PDF_DONE_DIR,
            PDF_ERROR_DIR,
            PDF_PROCESSING_DIR,
        ]:
            dir_path.mkdir(parents=True, exist_ok=True)
            logger.debug(f"ç¢ºèªç›®éŒ„å­˜åœ¨: {dir_path}")

    def load_state(self) -> Dict:
        """
        è¼‰å…¥è™•ç†ç‹€æ…‹

        ç‹€æ…‹æª”æ¡ˆè¨˜éŒ„æ¯å€‹è™•ç†éçš„æª”æ¡ˆçš„è³‡è¨Š
        é¿å…é‡è¤‡è™•ç†ç›¸åŒæª”æ¡ˆ

        Returns:
            dict: ç‹€æ…‹å­—å…¸
        """
        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f:
                    state = json.load(f)
                    logger.info(f"è¼‰å…¥ç‹€æ…‹æª”æ¡ˆï¼Œå·²è™•ç† {len(state)} å€‹æª”æ¡ˆ")
                    return state
            except Exception as e:
                logger.warning(f"è¼‰å…¥ç‹€æ…‹æª”æ¡ˆå¤±æ•—: {e}ï¼Œä½¿ç”¨ç©ºç‹€æ…‹")
                return {}
        return {}

    def save_state(self):
        """
        å„²å­˜è™•ç†ç‹€æ…‹
        æ¯è™•ç†å®Œä¸€å€‹æª”æ¡ˆå°±å„²å­˜ï¼Œç¢ºä¿æ–·é›»ä¹Ÿä¸æœƒéºå¤±é€²åº¦
        """
        try:
            with open(STATE_FILE, "w", encoding="utf-8") as f:
                json.dump(self.state, f, ensure_ascii=False, indent=2)
            logger.debug("ç‹€æ…‹å·²å„²å­˜")
        except Exception as e:
            logger.error(f"å„²å­˜ç‹€æ…‹å¤±æ•—: {e}")

    def get_file_hash(self, file_path: Path) -> str:
        """
        è¨ˆç®—æª”æ¡ˆçš„ SHA256 é›œæ¹Šå€¼
        ç”¨æ–¼åˆ¤æ–·æª”æ¡ˆæ˜¯å¦å·²è®Šæ›´

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘

        Returns:
            str: SHA256 é›œæ¹Šå€¼ï¼ˆ16é€²åˆ¶å­—ä¸²ï¼‰
        """
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            # åˆ†å¡Šè®€å–ï¼Œé¿å…å¤§æª”æ¡ˆå ç”¨éå¤šè¨˜æ†¶é«”
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    def process_pdf(self, pdf_path: Path) -> bool:
        """
        è™•ç†å–®ä¸€ PDF æª”æ¡ˆçš„å®Œæ•´æµç¨‹

        æ­¥é©Ÿï¼š
        1. è¨ˆç®—æª”æ¡ˆé›œæ¹Š
        2. ç§»åˆ°è™•ç†ä¸­ç›®éŒ„ï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
        3. è§£æ PDF å…§å®¹
        4. å»ºç«‹æ–‡æª”ç‰©ä»¶
        5. å­˜å…¥è³‡æ–™åº«
        6. ç§»åˆ°å®Œæˆæˆ–éŒ¯èª¤ç›®éŒ„

        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘

        Returns:
            bool: è™•ç†æ˜¯å¦æˆåŠŸ
        """
        start_time = time.time()
        file_hash = self.get_file_hash(pdf_path)
        file_size = pdf_path.stat().st_size

        try:
            logger.info(f"ğŸ“„ é–‹å§‹è™•ç†: {pdf_path.name} ({file_size/1024:.1f} KB)")

            # è¨˜éŒ„é–‹å§‹è™•ç†
            self.db.log_processing(pdf_path.name, file_hash, "processing")

            # æ­¥é©Ÿ1ï¼šç§»åˆ°è™•ç†ä¸­ç›®éŒ„ï¼ˆé˜²æ­¢å…¶ä»–é€²ç¨‹é‡è¤‡è™•ç†ï¼‰
            processing_path = PDF_PROCESSING_DIR / pdf_path.name
            pdf_path.rename(processing_path)
            logger.debug(f"æª”æ¡ˆç§»è‡³è™•ç†ä¸­: {processing_path}")

            # æ­¥é©Ÿ2ï¼šæå–æ–‡æœ¬
            text, page_count = PDFParser.extract_text(processing_path)

            if not text:
                raise ValueError("ç„¡æ³•æå–æ–‡æœ¬å…§å®¹ï¼ˆæª”æ¡ˆå¯èƒ½æå£æˆ–ç‚ºåœ–ç‰‡ï¼‰")

            logger.info(f"  æå–æ–‡æœ¬: {len(text)} å­—å…ƒ, {page_count} é ")

            # æ­¥é©Ÿ3ï¼šè§£ææ–‡æª”è³‡è¨Š
            doc_type = PDFParser.detect_doc_type(text, processing_path.stem)
            product_ids = PDFParser.extract_product_ids(text)
            metadata = PDFParser.extract_metadata(text, doc_type)
            keywords = PDFParser.extract_keywords(text, product_ids)
            summary = PDFParser.generate_summary(text)

            # å¾æª”åæå–æ–‡æª”ç·¨è™Ÿï¼ˆç¦èˆˆçš„å‘½åè¦å‰‡ï¼‰
            doc_number_match = re.search(
                r"[A-Z]{2,}-[A-Z]-\d{2}-[A-Z]-\d{3}|L\d{6}[A-Z]?\d?",
                processing_path.stem,
            )
            doc_number = (
                doc_number_match.group(0) if doc_number_match else processing_path.stem
            )

            # æ­¥é©Ÿ4ï¼šå»ºç«‹æ–‡æª”ç‰©ä»¶
            doc = TechnicalDocument(
                doc_id=hashlib.md5(processing_path.name.encode()).hexdigest(),
                doc_type=doc_type,
                doc_number=doc_number,
                title=processing_path.stem.replace("_", " "),  # å°‡åº•ç·šè½‰ç‚ºç©ºæ ¼
                product_ids=product_ids,
                revision=metadata.get("revision"),
                issue_date=(
                    metadata.get("dates", [None])[0] if metadata.get("dates") else None
                ),
                department=None,  # å¯å¾æ–‡æª”å…§å®¹é€²ä¸€æ­¥æå–
                author=None,  # å¯å¾æ–‡æª”å…§å®¹é€²ä¸€æ­¥æå–
                content=text,
                summary=summary,
                keywords=keywords,
                metadata=metadata,
                file_path=str(processing_path),
                file_hash=file_hash,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                page_count=page_count,
                file_size=file_size,
            )

            logger.info(
                f"  æ–‡æª”è³‡è¨Š: é¡å‹={doc_type}, ç”¢å“æ•¸={len(product_ids)}, é—œéµå­—æ•¸={len(keywords)}"
            )

            # æ­¥é©Ÿ5ï¼šå­˜å…¥è³‡æ–™åº«
            if self.db.upsert_document(doc):
                # æˆåŠŸï¼šç§»åˆ°å®Œæˆç›®éŒ„
                done_path = PDF_DONE_DIR / processing_path.name
                processing_path.rename(done_path)

                process_time = int((time.time() - start_time) * 1000)
                self.db.log_processing(
                    pdf_path.name, file_hash, "success", process_time_ms=process_time
                )

                # æ›´æ–°ç‹€æ…‹
                self.state[pdf_path.name] = {
                    "hash": file_hash,
                    "processed_at": datetime.now().isoformat(),
                    "doc_id": doc.doc_id,
                    "status": "success",
                    "page_count": page_count,
                    "doc_type": doc_type,
                    "product_count": len(product_ids),
                }
                self.save_state()

                logger.info(f"  âœ… æˆåŠŸè™•ç†ï¼Œè€—æ™‚: {process_time}ms")
                return True
            else:
                raise Exception("è³‡æ–™åº«å¯«å…¥å¤±æ•—")

        except Exception as e:
            # å¤±æ•—ï¼šç§»åˆ°éŒ¯èª¤ç›®éŒ„
            error_msg = str(e)
            logger.error(f"  âŒ è™•ç†å¤±æ•—: {error_msg}")

            # ç¢ºä¿æª”æ¡ˆåœ¨è™•ç†ä¸­ç›®éŒ„
            if processing_path.exists():
                error_path = PDF_ERROR_DIR / pdf_path.name
                processing_path.rename(error_path)
                logger.debug(f"æª”æ¡ˆç§»è‡³éŒ¯èª¤ç›®éŒ„: {error_path}")

            process_time = int((time.time() - start_time) * 1000)
            self.db.log_processing(
                pdf_path.name,
                file_hash,
                "error",
                error_message=error_msg,
                process_time_ms=process_time,
            )

            # è¨˜éŒ„éŒ¯èª¤ç‹€æ…‹
            self.state[pdf_path.name] = {
                "hash": file_hash,
                "processed_at": datetime.now().isoformat(),
                "status": "error",
                "error": error_msg,
            }
            self.save_state()
            return False

    def scan_and_process(self):
        """
        æƒæç›£æ§ç›®éŒ„ä¸¦è™•ç† PDF æª”æ¡ˆ

        æµç¨‹ï¼š
        1. æƒæ incoming ç›®éŒ„
        2. æª¢æŸ¥æ¯å€‹æª”æ¡ˆæ˜¯å¦å·²è™•ç†
        3. æ‰¹æ¬¡è™•ç†æ–°æª”æ¡ˆ
        4. è¨˜éŒ„è™•ç†çµæœ

        Returns:
            int: æœ¬æ¬¡è™•ç†çš„æª”æ¡ˆæ•¸
        """
        # æƒææ‰€æœ‰ PDF æª”æ¡ˆ
        pdf_files = sorted(PDF_WATCH_DIR.glob("*.pdf"))

        if not pdf_files:
            return 0

        logger.info(f"ğŸ” ç™¼ç¾ {len(pdf_files)} å€‹å¾…è™•ç†æª”æ¡ˆ")

        processed = 0
        # é™åˆ¶æ¯æ‰¹è™•ç†çš„æª”æ¡ˆæ•¸ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œ
        for pdf_path in pdf_files[:PROCESS_BATCH_SIZE]:
            # æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢
            if should_stop:
                logger.info("æ”¶åˆ°åœæ­¢ä¿¡è™Ÿï¼Œä¸­æ–·è™•ç†")
                break

            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éæ­¤æª”æ¡ˆ
            file_hash = self.get_file_hash(pdf_path)
            if pdf_path.name in self.state:
                if self.state[pdf_path.name].get("hash") == file_hash:
                    logger.info(f"â­ï¸ è·³éå·²è™•ç†: {pdf_path.name}")
                    # å·²è™•ç†éçš„æª”æ¡ˆç§»åˆ°å®Œæˆç›®éŒ„
                    done_path = PDF_DONE_DIR / pdf_path.name
                    pdf_path.rename(done_path)
                    continue

            # è™•ç†æª”æ¡ˆ
            if self.process_pdf(pdf_path):
                processed += 1

            # çŸ­æš«ä¼‘æ¯ï¼Œé¿å… CPU éè¼‰
            time.sleep(1)

        return processed

    def run(self):
        """
        ä¸»åŸ·è¡Œå¾ªç’°
        æŒçºŒç›£æ§ç›®éŒ„ä¸¦è™•ç†æ–°æª”æ¡ˆ
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ PDF è™•ç†æœå‹™å•Ÿå‹•")
        logger.info(f"ğŸ“ ç›£æ§ç›®éŒ„: {PDF_WATCH_DIR}")
        logger.info(f"â±  æƒæé–“éš”: {SCAN_INTERVAL} ç§’")
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {PROCESS_BATCH_SIZE}")
        logger.info(f"ğŸ” OCR ç‹€æ…‹: {'å•Ÿç”¨' if ENABLE_OCR else 'é—œé–‰'}")
        logger.info("=" * 60)

        no_file_count = 0  # é€£çºŒç„¡æª”æ¡ˆçš„æ¬¡æ•¸

        try:
            while not should_stop:
                try:
                    # åŸ·è¡Œä¸€æ¬¡æƒæå’Œè™•ç†
                    processed = self.scan_and_process()

                    if processed > 0:
                        no_file_count = 0
                        logger.info(f"âœ¨ æœ¬è¼ªè™•ç†å®Œæˆï¼Œå…± {processed} å€‹æª”æ¡ˆ")
                    else:
                        no_file_count += 1

                        # å‹•æ…‹èª¿æ•´æƒæé–“éš”ï¼ˆç„¡æª”æ¡ˆæ™‚é€æ¼¸å»¶é•·é–“éš”ï¼‰
                        sleep_time = min(SCAN_INTERVAL * (1 + no_file_count // 10), 300)

                        # æ¯10æ¬¡ç„¡æª”æ¡ˆæ‰è¼¸å‡ºä¸€æ¬¡æ—¥èªŒï¼Œé¿å…æ—¥èªŒéå¤š
                        if no_file_count % 10 == 0:
                            logger.info(f"ğŸ’¤ ç„¡æ–°æª”æ¡ˆï¼Œç­‰å¾… {sleep_time} ç§’...")

                    # ä¼‘çœ ç­‰å¾…ä¸‹æ¬¡æƒæ
                    sleep_time = 5 if processed > 0 else SCAN_INTERVAL
                    for _ in range(sleep_time):
                        if should_stop:
                            break
                        time.sleep(1)  # æ¯ç§’æª¢æŸ¥ä¸€æ¬¡åœæ­¢ä¿¡è™Ÿ

                except Exception as e:
                    logger.error(f"è™•ç†é€±æœŸéŒ¯èª¤: {e}", exc_info=True)
                    time.sleep(30)  # éŒ¯èª¤å¾Œç­‰å¾…è¼ƒé•·æ™‚é–“

        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°éµç›¤ä¸­æ–·")
        finally:
            logger.info("ğŸ›‘ æœå‹™æ­£åœ¨é—œé–‰...")
            self.db.close()
            logger.info("ğŸ‘‹ æœå‹™å·²åœæ­¢")


# ============================================================================
# ä¸»ç¨‹å¼å…¥å£
# ============================================================================
def main():
    """
    ä¸»ç¨‹å¼å…¥å£

    æ­¥é©Ÿï¼š
    1. ç­‰å¾… MySQL å°±ç·’
    2. åˆå§‹åŒ–æœå‹™
    3. å•Ÿå‹•ä¸»å¾ªç’°
    """
    # ç­‰å¾… MySQL å°±ç·’ï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
    logger.info("â³ ç­‰å¾… MySQL å°±ç·’...")
    for i in range(30):
        try:
            db = DatabaseManager()
            db.close()
            logger.info("âœ… MySQL é€£ç·šæˆåŠŸ")
            break
        except Exception as e:
            if i < 29:
                logger.info(f"ç­‰å¾… MySQL... ({i+1}/30)")
                time.sleep(2)
            else:
                logger.error(f"âŒ ç„¡æ³•é€£ç·šåˆ° MySQL: {e}")
                sys.exit(1)

    # å•Ÿå‹•æœå‹™
    service = PDFProcessorService()
    service.run()


if __name__ == "__main__":
    main()
