#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
PDF æ–‡æª”è™•ç†æœå‹™ - ç¦èˆˆå·¥æ¥­æŠ€è¡“æ–‡ä»¶è‡ªå‹•åŒ–è™•ç†ç³»çµ±
=============================================================================

åŠŸèƒ½æ¦‚è¿°ï¼š
1. è‡ªå‹•ç›£æ§æŒ‡å®šç›®éŒ„ä¸­çš„ PDF æª”æ¡ˆ
2. è§£æç¦èˆˆå·¥æ¥­çš„æŠ€è¡“æ–‡ä»¶ï¼ˆè¨­è®Šé€šçŸ¥ã€DFMEAã€è¦æ ¼æ›¸ç­‰ï¼‰
3. æå–çµæ§‹åŒ–è³‡æ–™ä¸¦å­˜å…¥ MySQL è³‡æ–™åº«
4. æ”¯æ´ç‹€æ…‹è¿½è¹¤ï¼Œé¿å…é‡è¤‡è™•ç†
5. å¯é¸çš„ OCR åŠŸèƒ½è™•ç†æƒææª”

ç³»çµ±æ¶æ§‹ï¼š
    PDFæª”æ¡ˆ â†’ ç›£æ§ç›®éŒ„ â†’ è§£æè™•ç† â†’ MySQL â†’ åŒæ­¥åˆ° ES â†’ RAGæª¢ç´¢

ä½œè€…: [æ‚¨çš„åœ˜éšŠ]
ç‰ˆæœ¬: 1.0.0
æ›´æ–°æ—¥æœŸ: 2024
=============================================================================
"""

import os, sys, time, json, signal, hashlib, logging, pymysql, pdfplumber, re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

# ============================================================================
# ç’°å¢ƒè®Šæ•¸é…ç½®
# ============================================================================
# é€™äº›ç’°å¢ƒè®Šæ•¸åœ¨ docker-compose.yml ä¸­è¨­å®šï¼Œæä¾›éˆæ´»çš„é…ç½®é¸é …

# MySQL é€£ç·šè¨­å®š - é€£æ¥åˆ°æ‚¨çš„è³‡æ–™åº«å®¹å™¨
MYSQL_HOST = os.getenv("MYSQL_HOST", "mysql")  # Docker ç¶²è·¯ä¸­çš„ä¸»æ©Ÿå
MYSQL_PORT = int(os.getenv("MYSQL_PORT", "3306"))
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "root")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "fuhsin_erp_demo")

# PDF æª”æ¡ˆç›®éŒ„çµæ§‹
# /mnt/pdf/
#   â”œâ”€â”€ incoming/      # æ–°æª”æ¡ˆæ”¾é€™è£¡
#   â”œâ”€â”€ .done/         # è™•ç†å®Œæˆçš„æª”æ¡ˆ
#   â”œâ”€â”€ .error/        # è™•ç†å¤±æ•—çš„æª”æ¡ˆ
#   â””â”€â”€ .processing/   # æ­£åœ¨è™•ç†çš„æª”æ¡ˆï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
PDF_WATCH_DIR = Path(os.getenv("PDF_WATCH_DIR", "/mnt/pdf/incoming"))
PDF_DONE_DIR = PDF_WATCH_DIR / ".done"
PDF_ERROR_DIR = PDF_WATCH_DIR / ".error"
PDF_PROCESSING_DIR = PDF_WATCH_DIR / ".processing"

# è™•ç†åƒæ•¸è¨­å®š
SCAN_INTERVAL = int(os.getenv("SCAN_INTERVAL", "30"))  # æƒæé–“éš”ï¼ˆç§’ï¼‰
PROCESS_BATCH_SIZE = int(os.getenv("PROCESS_BATCH_SIZE", "5"))  # æ¯æ‰¹è™•ç†æª”æ¡ˆæ•¸
ENABLE_OCR = os.getenv("ENABLE_OCR", "false").lower() == "true"  # æ˜¯å¦å•Ÿç”¨ OCR
OCR_LANG = os.getenv("OCR_LANG", "chi_tra+eng")  # OCR èªè¨€ï¼šç¹é«”ä¸­æ–‡+è‹±æ–‡

# ç‹€æ…‹å’Œæ—¥èªŒæª”æ¡ˆè·¯å¾‘
STATE_FILE = Path("/state/.pdf_processor_state.json")  # è™•ç†ç‹€æ…‹è¨˜éŒ„
LOG_FILE = Path("/logs/pdf_processor.log")  # æ—¥èªŒæª”æ¡ˆ
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# ============================================================================
# æ—¥èªŒè¨­å®š
# ============================================================================
# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(LOG_FILE.parent, exist_ok=True)
os.makedirs(STATE_FILE.parent, exist_ok=True)

# é…ç½®æ—¥èªŒæ ¼å¼å’Œè¼¸å‡º
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE),  # å¯«å…¥æª”æ¡ˆ
        logging.StreamHandler(),  # è¼¸å‡ºåˆ°æ§åˆ¶å°
    ],
)
logger = logging.getLogger(__name__)

# ============================================================================
# å„ªé›…é—œé–‰æ©Ÿåˆ¶
# ============================================================================
# å…¨åŸŸè®Šæ•¸ï¼šç”¨æ–¼æ¥æ”¶ä¸­æ–·ä¿¡è™Ÿæ™‚å„ªé›…é—œé–‰
should_stop = False


def signal_handler(signum, frame):
    """
    è™•ç†ç³»çµ±ä¸­æ–·ä¿¡è™Ÿï¼ˆSIGINT, SIGTERMï¼‰
    ç•¶æ”¶åˆ° docker stop æˆ– Ctrl+C æ™‚è§¸ç™¼
    """
    global should_stop
    logger.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæº–å‚™å„ªé›…é—œé–‰...")
    should_stop = True


# è¨»å†Šä¿¡è™Ÿè™•ç†å™¨
signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
signal.signal(signal.SIGTERM, signal_handler)  # docker stop


# ============================================================================
# è³‡æ–™æ¨¡å‹å®šç¾©
# ============================================================================
@dataclass
class TechnicalDocument:
    """
    æŠ€è¡“æ–‡æª”è³‡æ–™æ¨¡å‹

    ä½¿ç”¨ @dataclass è£é£¾å™¨è‡ªå‹•ç”Ÿæˆ __init__ã€__repr__ ç­‰æ–¹æ³•
    é€™å€‹æ¨¡å‹å°æ‡‰åˆ° MySQL çš„ technical_documents è¡¨
    """

    doc_id: str  # æ–‡æª”å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆMD5 hashï¼‰
    doc_type: str  # æ–‡æª”é¡å‹ï¼ˆECN/DFMEA/SPEC/DRAWING/COMPLAINTï¼‰
    doc_number: str  # æ–‡æª”ç·¨è™Ÿï¼ˆå¦‚ EC-K-28-C-083ï¼‰
    title: str  # æ–‡æª”æ¨™é¡Œ
    product_ids: List[str]  # ç›¸é—œç”¢å“ç·¨è™Ÿåˆ—è¡¨ï¼ˆå¦‚ ['OB1-G04313AU', 'OB1-G04313A']ï¼‰
    revision: Optional[str]  # ç‰ˆæœ¬è™Ÿ
    issue_date: Optional[str]  # ç™¼è¡Œæ—¥æœŸ
    department: Optional[str]  # éƒ¨é–€
    author: Optional[str]  # ä½œè€…/è² è²¬äºº
    content: str  # å®Œæ•´æ–‡æœ¬å…§å®¹ï¼ˆç”¨æ–¼å…¨æ–‡æœå°‹ï¼‰
    summary: Optional[str]  # è‡ªå‹•ç”Ÿæˆçš„æ‘˜è¦
    keywords: List[str]  # é—œéµå­—åˆ—è¡¨ï¼ˆç”¨æ–¼æ¨™ç±¤å’Œå¿«é€Ÿæª¢ç´¢ï¼‰
    metadata: Dict  # å…¶ä»–å…ƒè³‡æ–™ï¼ˆJSON æ ¼å¼å­˜å„²ï¼‰
    file_path: str  # åŸå§‹æª”æ¡ˆè·¯å¾‘
    file_hash: str  # æª”æ¡ˆ SHA256 é›œæ¹Šå€¼ï¼ˆç”¨æ–¼å»é‡ï¼‰
    created_at: str  # å»ºç«‹æ™‚é–“
    updated_at: str  # æ›´æ–°æ™‚é–“
    page_count: int  # PDF é æ•¸
    file_size: int  # æª”æ¡ˆå¤§å°ï¼ˆbytesï¼‰


# ============================================================================
# è³‡æ–™åº«ç®¡ç†å™¨
# ============================================================================
class DatabaseManager:
    """
    è³‡æ–™åº«é€£ç·šå’Œæ“ä½œç®¡ç†

    è² è²¬ï¼š
    1. ç®¡ç† MySQL é€£ç·š
    2. å»ºç«‹è³‡æ–™è¡¨
    3. æ’å…¥/æ›´æ–°æ–‡æª”è³‡æ–™
    4. è¨˜éŒ„è™•ç†æ—¥èªŒ
    """

    def __init__(self):
        """åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨"""
        self.connection = None
        self.create_tables()  # ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨

    def get_connection(self):
        """
        å–å¾—è³‡æ–™åº«é€£ç·šï¼ˆé€£ç·šæ± æ¦‚å¿µçš„ç°¡å–®å¯¦ç¾ï¼‰
        å¦‚æœé€£ç·šä¸å­˜åœ¨æˆ–å·²æ–·é–‹ï¼Œå»ºç«‹æ–°é€£ç·š
        """
        if not self.connection or not self.connection.open:
            logger.info("å»ºç«‹æ–°çš„è³‡æ–™åº«é€£ç·š...")
            self.connection = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE,
                charset="utf8mb4",  # æ”¯æ´å®Œæ•´çš„ Unicodeï¼ˆåŒ…æ‹¬ emojiï¼‰
                autocommit=True,  # è‡ªå‹•æäº¤äº‹å‹™
            )
        return self.connection

    def create_tables(self):
        """
        å»ºç«‹å¿…è¦çš„è³‡æ–™è¡¨
        å¦‚æœè¡¨å·²å­˜åœ¨å‰‡è·³éï¼ˆCREATE TABLE IF NOT EXISTSï¼‰
        """
        create_sql = """
        -- ä¸»è¦æ–‡æª”è¡¨ï¼šå­˜å„²æ‰€æœ‰è§£æå‡ºçš„æŠ€è¡“æ–‡ä»¶
        CREATE TABLE IF NOT EXISTS technical_documents (
            id INT AUTO_INCREMENT PRIMARY KEY,
            doc_id VARCHAR(64) UNIQUE NOT NULL COMMENT 'æ–‡æª”å”¯ä¸€ID',
            doc_type VARCHAR(20) NOT NULL COMMENT 'æ–‡æª”é¡å‹',
            doc_number VARCHAR(50) COMMENT 'æ–‡æª”ç·¨è™Ÿ',
            title VARCHAR(255) NOT NULL COMMENT 'æ¨™é¡Œ',
            product_ids JSON COMMENT 'ç›¸é—œç”¢å“ç·¨è™Ÿï¼ˆJSONé™£åˆ—ï¼‰',
            revision VARCHAR(20) COMMENT 'ç‰ˆæœ¬è™Ÿ',
            issue_date DATE COMMENT 'ç™¼è¡Œæ—¥æœŸ',
            department VARCHAR(50) COMMENT 'éƒ¨é–€',
            author VARCHAR(50) COMMENT 'ä½œè€…',
            content LONGTEXT NOT NULL COMMENT 'å®Œæ•´å…§å®¹ï¼ˆç”¨æ–¼å…¨æ–‡æœå°‹ï¼‰',
            summary TEXT COMMENT 'æ‘˜è¦',
            keywords JSON COMMENT 'é—œéµå­—ï¼ˆJSONé™£åˆ—ï¼‰',
            metadata JSON COMMENT 'å…ƒè³‡æ–™ï¼ˆJSONç‰©ä»¶ï¼‰',
            file_path VARCHAR(500) COMMENT 'æª”æ¡ˆè·¯å¾‘',
            file_hash VARCHAR(64) COMMENT 'æª”æ¡ˆé›œæ¹Š',
            page_count INT DEFAULT 0 COMMENT 'é æ•¸',
            file_size INT DEFAULT 0 COMMENT 'æª”æ¡ˆå¤§å°(bytes)',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            
            -- ç´¢å¼•å„ªåŒ–æŸ¥è©¢æ•ˆèƒ½
            INDEX idx_doc_type (doc_type),        -- æŒ‰é¡å‹æŸ¥è©¢
            INDEX idx_doc_number (doc_number),    -- æŒ‰ç·¨è™ŸæŸ¥è©¢
            INDEX idx_issue_date (issue_date),    -- æŒ‰æ—¥æœŸæŸ¥è©¢
            INDEX idx_created_at (created_at),    -- æŒ‰å»ºç«‹æ™‚é–“æ’åº
            FULLTEXT idx_content (content),       -- å…¨æ–‡æœå°‹å…§å®¹
            FULLTEXT idx_title_summary (title, summary)  -- å…¨æ–‡æœå°‹æ¨™é¡Œå’Œæ‘˜è¦
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        
        -- è™•ç†è¨˜éŒ„è¡¨ï¼šè¿½è¹¤æ¯å€‹æª”æ¡ˆçš„è™•ç†ç‹€æ…‹
        CREATE TABLE IF NOT EXISTS pdf_processing_log (
            id INT AUTO_INCREMENT PRIMARY KEY,
            file_name VARCHAR(255) NOT NULL COMMENT 'æª”æ¡ˆåç¨±',
            file_hash VARCHAR(64) COMMENT 'æª”æ¡ˆé›œæ¹Š',
            status ENUM('processing', 'success', 'error') NOT NULL COMMENT 'è™•ç†ç‹€æ…‹',
            error_message TEXT COMMENT 'éŒ¯èª¤è¨Šæ¯',
            process_time_ms INT COMMENT 'è™•ç†æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            INDEX idx_status (status),
            INDEX idx_created_at (created_at)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """

        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                # åˆ†å‰² SQL èªå¥ä¸¦é€ä¸€åŸ·è¡Œ
                for statement in create_sql.split(";"):
                    if statement.strip():
                        cursor.execute(statement)
                conn.commit()
                logger.info("âœ… è³‡æ–™è¡¨å»ºç«‹/ç¢ºèªæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ å»ºç«‹è³‡æ–™è¡¨å¤±æ•—: {e}")
            raise

    def upsert_document(self, doc: TechnicalDocument) -> bool:
        """
        æ’å…¥æˆ–æ›´æ–°æ–‡æª”ï¼ˆUPSERT æ“ä½œï¼‰

        ä½¿ç”¨ INSERT ... ON DUPLICATE KEY UPDATE èªæ³•
        å¦‚æœ doc_id å·²å­˜åœ¨å‰‡æ›´æ–°ï¼Œå¦å‰‡æ’å…¥æ–°è¨˜éŒ„

        Args:
            doc: TechnicalDocument ç‰©ä»¶

        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        sql = """
        INSERT INTO technical_documents (
            doc_id, doc_type, doc_number, title, product_ids, revision,
            issue_date, department, author, content, summary, keywords,
            metadata, file_path, file_hash, page_count, file_size
        ) VALUES (
            %(doc_id)s, %(doc_type)s, %(doc_number)s, %(title)s, 
            %(product_ids)s, %(revision)s, %(issue_date)s, %(department)s,
            %(author)s, %(content)s, %(summary)s, %(keywords)s,
            %(metadata)s, %(file_path)s, %(file_hash)s, %(page_count)s, %(file_size)s
        )
        ON DUPLICATE KEY UPDATE
            doc_type = VALUES(doc_type),
            title = VALUES(title),
            product_ids = VALUES(product_ids),
            content = VALUES(content),
            summary = VALUES(summary),
            keywords = VALUES(keywords),
            metadata = VALUES(metadata),
            page_count = VALUES(page_count),
            file_size = VALUES(file_size),
            updated_at = CURRENT_TIMESTAMP
        """

        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                # å°‡ dataclass è½‰æ›ç‚ºå­—å…¸
                doc_dict = asdict(doc)

                # å°‡ Python åˆ—è¡¨/å­—å…¸è½‰æ›ç‚º JSON å­—ä¸²
                doc_dict["product_ids"] = json.dumps(
                    doc.product_ids, ensure_ascii=False
                )
                doc_dict["keywords"] = json.dumps(doc.keywords, ensure_ascii=False)
                doc_dict["metadata"] = json.dumps(doc.metadata, ensure_ascii=False)

                cursor.execute(sql, doc_dict)
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"è³‡æ–™åº«å¯«å…¥éŒ¯èª¤: {e}")
            return False

    def log_processing(
        self,
        file_name: str,
        file_hash: str,
        status: str,
        error_message: str = None,
        process_time_ms: int = 0,
    ):
        """
        è¨˜éŒ„è™•ç†ç‹€æ…‹åˆ°æ—¥èªŒè¡¨
        ç”¨æ–¼è¿½è¹¤å’Œè¨ºæ–·å•é¡Œ
        """
        sql = """
        INSERT INTO pdf_processing_log 
        (file_name, file_hash, status, error_message, process_time_ms)
        VALUES (%s, %s, %s, %s, %s)
        """
        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                cursor.execute(
                    sql, (file_name, file_hash, status, error_message, process_time_ms)
                )
                conn.commit()
        except Exception as e:
            logger.error(f"è¨˜éŒ„è™•ç†æ—¥èªŒå¤±æ•—: {e}")

    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£ç·š"""
        if self.connection:
            self.connection.close()
            logger.info("è³‡æ–™åº«é€£ç·šå·²é—œé–‰")


# ============================================================================
# PDF è§£æå™¨
# ============================================================================
class PDFParser:
    """
    PDF æ–‡ä»¶è§£æå™¨ - å°ˆé–€é‡å°ç¦èˆˆå·¥æ¥­æ–‡ä»¶å„ªåŒ–

    ä¸»è¦åŠŸèƒ½ï¼š
    1. æå– PDF æ–‡å­—å…§å®¹ï¼ˆåŸç”Ÿæ–‡å­—å±¤ï¼‰
    2. è­˜åˆ¥æ–‡æª”é¡å‹ï¼ˆè¨­è®Šé€šçŸ¥ã€DFMEA ç­‰ï¼‰
    3. æå–ç”¢å“ç·¨è™Ÿï¼ˆä½¿ç”¨æ­£å‰‡è¡¨é”å¼ï¼‰
    4. ç”Ÿæˆæ‘˜è¦å’Œé—œéµå­—
    5. å¯é¸çš„ OCR è™•ç†ï¼ˆæƒææª”ï¼‰
    """

    # ç¦èˆˆç”¢å“ç·¨è™Ÿçš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼
    # é€™äº›æ¨¡å¼åŸºæ–¼æ‚¨æä¾›çš„ PDF æ–‡ä»¶ä¸­çš„å¯¦éš›ç”¢å“ç·¨è™Ÿæ ¼å¼
    PRODUCT_PATTERNS = [
        r"OB\d-[A-Z0-9]+",  # ä¾‹ï¼šOB1-G04313AU, OB1-G04313A
        r"[FG]\d{2}-[A-Z0-9]+",  # ä¾‹ï¼šF05-L0Y513, G05-L05513
        r"[PW]\d{3}",  # ä¾‹ï¼šP001, W002ï¼ˆç”¢å“/å€‰åº«ç·¨è™Ÿï¼‰
        r"EC-K-\d{2}-[A-Z]-\d{3}",  # ä¾‹ï¼šEC-K-28-C-083ï¼ˆæ–‡ä»¶ç·¨è™Ÿï¼‰
        r"L\d{6}[A-Z]?\d?",  # ä¾‹ï¼šL113055R2, L112078ï¼ˆè®Šæ›´å–®è™Ÿï¼‰
    ]

    # æ–‡æª”é¡å‹è­˜åˆ¥é—œéµå­—
    # ç”¨æ–¼è‡ªå‹•åˆ†é¡æ–‡æª”
    DOC_TYPE_KEYWORDS = {
        "ECN": ["è¨­è®Šé€šçŸ¥", "è¨­è¨ˆè®Šæ›´", "ECN", "Engineering Change"],
        "DFMEA": ["DFMEA", "å¤±æ•ˆæ¨¡å¼", "é¢¨éšªåˆ†æ"],
        "SPEC": ["è¦æ ¼", "è¦ç¯„", "Specification", "æŠ€è¡“è¦æ±‚"],
        "DRAWING": ["åœ–é¢", "åœ–ç´™", "Drawing", "å·¥ç¨‹åœ–"],
        "COMPLAINT": ["å®¢è¨´", "å®¢æˆ¶æŠ±æ€¨", "é¡§å®¢æŠ±æ€¨", "Complaint"],
        "REPORT": ["å ±å‘Š", "æ¸¬è©¦", "Report", "Test"],
    }

    @classmethod
    def extract_text(cls, pdf_path: Path) -> Tuple[str, int]:
        """
        æå– PDF å…¨æ–‡å’Œé æ•¸

        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘

        Returns:
            tuple: (æå–çš„æ–‡å­—, é æ•¸)
        """
        text = ""
        page_count = 0

        try:
            # ä½¿ç”¨ pdfplumber é–‹å•Ÿ PDF
            with pdfplumber.open(pdf_path) as pdf:
                page_count = len(pdf.pages)
                logger.debug(f"PDF å…± {page_count} é ")

                for i, page in enumerate(pdf.pages, 1):
                    # æå–é é¢æ–‡å­—
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                        logger.debug(f"  ç¬¬ {i} é æå– {len(page_text)} å­—å…ƒ")

                    # æå–è¡¨æ ¼ï¼ˆç¦èˆˆæ–‡ä»¶å¸¸åŒ…å«è¦æ ¼è¡¨ï¼‰
                    tables = page.extract_tables()
                    for table_idx, table in enumerate(tables):
                        logger.debug(f"  ç¬¬ {i} é ç™¼ç¾è¡¨æ ¼ {table_idx + 1}")
                        for row in table:
                            if row:
                                # å°‡è¡¨æ ¼è¡Œè½‰æ›ç‚ºæ–‡å­—ï¼ˆç”¨ | åˆ†éš”ï¼‰
                                text += (
                                    " | ".join(
                                        str(cell) if cell else "" for cell in row
                                    )
                                    + "\n"
                                )

        except Exception as e:
            logger.error(f"PDFæ–‡å­—æå–éŒ¯èª¤ {pdf_path}: {e}")

            # å¦‚æœå•Ÿç”¨ OCR ä¸”ç„¡æ³•æå–æ–‡å­—ï¼Œå˜—è©¦ OCR
            if ENABLE_OCR and not text:
                logger.info(f"å˜—è©¦ä½¿ç”¨ OCR è™•ç† {pdf_path}")
                text = cls.extract_text_with_ocr(pdf_path)

        return text, page_count

    @classmethod
    def extract_text_with_ocr(cls, pdf_path: Path) -> str:
        """
        ä½¿ç”¨ OCR æå–æ–‡å­—ï¼ˆè™•ç†æƒææª”ï¼‰
        éœ€è¦å®‰è£ tesseract-ocr å’Œç›¸é—œ Python å¥—ä»¶

        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘

        Returns:
            str: OCR è­˜åˆ¥çš„æ–‡å­—
        """
        try:
            import pytesseract
            from pdf2image import convert_from_path

            logger.info(f"é–‹å§‹ OCR è™•ç†...")

            # å°‡ PDF è½‰æ›ç‚ºåœ–ç‰‡ï¼ˆDPI 200 é©åˆå¤§éƒ¨åˆ†æ–‡ä»¶ï¼‰
            images = convert_from_path(str(pdf_path), dpi=200)

            text = ""
            for i, image in enumerate(images):
                logger.info(f"  OCR è™•ç†ç¬¬ {i+1}/{len(images)} é ")
                # ä½¿ç”¨ Tesseract é€²è¡Œ OCR
                # lang åƒæ•¸ï¼šchi_tra = ç¹é«”ä¸­æ–‡, eng = è‹±æ–‡
                page_text = pytesseract.image_to_string(image, lang=OCR_LANG)
                text += page_text + "\n"

            logger.info(f"OCR å®Œæˆï¼Œå…±è­˜åˆ¥ {len(text)} å­—å…ƒ")
            return text

        except ImportError:
            logger.error("OCR ç›¸é—œå¥—ä»¶æœªå®‰è£ï¼ˆpytesseract, pdf2imageï¼‰")
            return ""
        except Exception as e:
            logger.error(f"OCR è™•ç†å¤±æ•—: {e}")
            return ""

    @classmethod
    def detect_doc_type(cls, text: str, filename: str) -> str:
        """
        è­˜åˆ¥æ–‡æª”é¡å‹

        ç­–ç•¥ï¼š
        1. å…ˆæª¢æŸ¥æª”å
        2. å†æª¢æŸ¥å…§å®¹é—œéµå­—
        3. ç„¡æ³•è­˜åˆ¥å‰‡è¿”å› 'GENERAL'

        Args:
            text: æ–‡æª”å…§å®¹
            filename: æª”æ¡ˆåç¨±

        Returns:
            str: æ–‡æª”é¡å‹
        """
        text_lower = text.lower()
        filename_lower = filename.lower()

        # æª¢æŸ¥æ¯ç¨®æ–‡æª”é¡å‹çš„é—œéµå­—
        for doc_type, keywords in cls.DOC_TYPE_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in text_lower or keyword.lower() in filename_lower:
                    logger.debug(f"è­˜åˆ¥ç‚º {doc_type} é¡å‹ï¼ˆé—œéµå­—ï¼š{keyword}ï¼‰")
                    return doc_type

        # ç„¡æ³•è­˜åˆ¥ï¼Œè¿”å›é€šç”¨é¡å‹
        return "GENERAL"

    @classmethod
    def extract_product_ids(cls, text: str) -> List[str]:
        """
        æå–ç”¢å“ç·¨è™Ÿ

        ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…ç¦èˆˆçš„ç”¢å“ç·¨è™Ÿæ ¼å¼
        å»é‡ä¸¦æ’åºå¾Œè¿”å›

        Args:
            text: æ–‡æª”å…§å®¹

        Returns:
            list: ç”¢å“ç·¨è™Ÿåˆ—è¡¨
        """
        product_ids = set()

        for pattern in cls.PRODUCT_PATTERNS:
            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
            matches = re.findall(pattern, text, re.IGNORECASE)
            # è½‰æ›ç‚ºå¤§å¯«ä¸¦åŠ å…¥é›†åˆï¼ˆè‡ªå‹•å»é‡ï¼‰
            product_ids.update(m.upper() for m in matches)

        result = sorted(list(product_ids))[:20]  # æœ€å¤šä¿ç•™20å€‹
        logger.debug(f"æ‰¾åˆ° {len(result)} å€‹ç”¢å“ç·¨è™Ÿ: {result[:5]}...")

        return result

    @classmethod
    def extract_metadata(cls, text: str, doc_type: str) -> Dict:
        """
        æå–å…ƒè³‡æ–™ï¼ˆæ—¥æœŸã€ç‰ˆæœ¬è™Ÿç­‰ï¼‰

        Args:
            text: æ–‡æª”å…§å®¹
            doc_type: æ–‡æª”é¡å‹

        Returns:
            dict: å…ƒè³‡æ–™å­—å…¸
        """
        metadata = {}

        # æå–æ—¥æœŸï¼ˆæ”¯æ´å¤šç¨®æ ¼å¼ï¼‰
        date_patterns = [
            r"\d{4}/\d{1,2}/\d{1,2}",  # 2024/4/25
            r"\d{4}-\d{1,2}-\d{1,2}",  # 2024-04-25
        ]
        for pattern in date_patterns:
            dates = re.findall(pattern, text)
            if dates:
                metadata["dates"] = dates[:5]  # æœ€å¤šä¿ç•™5å€‹æ—¥æœŸ
                logger.debug(f"æ‰¾åˆ°æ—¥æœŸ: {dates[:3]}")
                break

        # æå–ç‰ˆæœ¬è™Ÿï¼ˆRev A, R2 ç­‰æ ¼å¼ï¼‰
        revision_match = re.search(r"[Rr]ev(?:ision)?[:\s]*([A-Z0-9]+)", text)
        if revision_match:
            metadata["revision"] = revision_match.group(1)
            logger.debug(f"æ‰¾åˆ°ç‰ˆæœ¬è™Ÿ: {metadata['revision']}")

        # æ ¹æ“šæ–‡æª”é¡å‹æå–ç‰¹å®šè³‡è¨Š
        if doc_type == "ECN":
            # è¨­è®Šé€šçŸ¥ç‰¹æœ‰ï¼šè®Šæ›´åŸå› 
            reason_match = re.search(r"åŸå› [ï¼š:]\s*([^\n]+)", text)
            if reason_match:
                metadata["change_reason"] = reason_match.group(1)

        elif doc_type == "DFMEA":
            # DFMEA ç‰¹æœ‰ï¼šåš´é‡åº¦è©•åˆ†
            severity_match = re.search(r"åš´é‡åº¦[ï¼š:]\s*(\d+)", text)
            if severity_match:
                metadata["severity"] = int(severity_match.group(1))

        return metadata

    @classmethod
    def extract_keywords(cls, text: str, product_ids: List[str]) -> List[str]:
        """
        æå–é—œéµå­—

        ç­–ç•¥ï¼š
        1. æª¢æŸ¥é å®šç¾©çš„æŠ€è¡“é—œéµå­—
        2. åŠ å…¥ç”¢å“ç·¨è™Ÿä½œç‚ºé—œéµå­—

        Args:
            text: æ–‡æª”å…§å®¹
            product_ids: ç”¢å“ç·¨è™Ÿåˆ—è¡¨

        Returns:
            list: é—œéµå­—åˆ—è¡¨
        """
        keywords = []

        # ç¦èˆˆç‰¹å®šæŠ€è¡“é—œéµå­—ï¼ˆåŸºæ–¼æ‚¨æä¾›çš„ PDF å…§å®¹ï¼‰
        tech_keywords = [
            "æ’ç·š",
            "å…§å´çµ„åˆ",
            "å¤–å´çµ„åˆ",
            "å½ˆç°§",
            "å¥—ç›¤",
            "æŠŠæ‰‹",
            "WiFi",
            "deadbolt",
            "è½‰è»¸",
            "å…§è»¸ç­’",
            "åº•æ¿",
            "è£é£¾",
            "æ¸¬è©¦",
            "å“è³ª",
            "è¦æ ¼",
            "å…¬å·®",
            "å°ºå¯¸",
            "æè³ª",
            "Hubspace",
            "è¨­è®Š",
            "æ”¹å–„",
            "å„ªåŒ–",
            "ä¸è‰¯",
            "çŸ¯æ­£",
        ]

        # æª¢æŸ¥æ¯å€‹é—œéµå­—æ˜¯å¦å‡ºç¾åœ¨æ–‡æª”ä¸­
        for keyword in tech_keywords:
            if keyword in text:
                keywords.append(keyword)

        # åŠ å…¥å‰5å€‹ç”¢å“ç·¨è™Ÿä½œç‚ºé—œéµå­—
        keywords.extend(product_ids[:5])

        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        result = list(set(keywords))[:15]
        logger.debug(f"æå– {len(result)} å€‹é—œéµå­—")

        return result

    @classmethod
    def generate_summary(cls, text: str, max_length: int = 300) -> str:
        """
        ç”Ÿæˆæ–‡æª”æ‘˜è¦

        ç­–ç•¥ï¼š
        1. å„ªå…ˆæå–åŒ…å«é‡è¦é—œéµå­—çš„å¥å­
        2. å¦‚æœæ²’æœ‰ï¼Œå‰‡å–æ–‡æª”é–‹é ­éƒ¨åˆ†

        Args:
            text: æ–‡æª”å…§å®¹
            max_length: æ‘˜è¦æœ€å¤§é•·åº¦

        Returns:
            str: æ‘˜è¦æ–‡å­—
        """
        # æ¸…ç†æ–‡æœ¬ï¼ˆåˆä½µç©ºç™½å­—å…ƒï¼‰
        text = re.sub(r"\s+", " ", text)
        text = text.strip()

        # å„ªå…ˆæå–åŒ…å«é‡è¦é—œéµå­—çš„å¥å­
        important_patterns = [
            r"[^ã€‚ï¼ï¼Ÿ\n]*(?:è®Šæ›´|ä¿®æ”¹|èª¿æ•´|æ”¹å–„|å„ªåŒ–)[^ã€‚ï¼ï¼Ÿ\n]*",
            r"[^ã€‚ï¼ï¼Ÿ\n]*(?:å•é¡Œ|ç¼ºé™·|ä¸è‰¯|ç•°å¸¸)[^ã€‚ï¼ï¼Ÿ\n]*",
        ]

        summary_sentences = []
        for pattern in important_patterns:
            matches = re.findall(pattern, text)
            summary_sentences.extend(matches[:2])  # æ¯ç¨®é¡å‹æœ€å¤šå–2å¥

        if summary_sentences:
            summary = "ã€‚".join(summary_sentences)[:max_length]
        else:
            # æ²’æœ‰æ‰¾åˆ°é‡è¦å¥å­ï¼Œå–å‰ max_length å­—å…ƒ
            summary = text[:max_length]

        if len(summary) == max_length:
            summary += "..."

        return summary


# ============================================================================
# ä¸»è™•ç†æœå‹™
# ============================================================================
class PDFProcessorService:
    """
    PDF è™•ç†æœå‹™ä¸»é¡

    è·è²¬ï¼š
    1. ç›£æ§ PDF æª”æ¡ˆç›®éŒ„
    2. å”èª¿è§£æå’Œå­˜å„²æµç¨‹
    3. ç®¡ç†è™•ç†ç‹€æ…‹
    4. è™•ç†éŒ¯èª¤å’Œé‡è©¦
    """

    def __init__(self):
        """åˆå§‹åŒ–æœå‹™"""
        self.db = DatabaseManager()
        self.state = self.load_state()
        self.setup_directories()
        logger.info("PDF è™•ç†æœå‹™åˆå§‹åŒ–å®Œæˆ")

    def setup_directories(self):
        """
        å»ºç«‹å¿…è¦çš„ç›®éŒ„çµæ§‹
        ç¢ºä¿æ‰€æœ‰å·¥ä½œç›®éŒ„å­˜åœ¨
        """
        for dir_path in [
            PDF_WATCH_DIR,
            PDF_DONE_DIR,
            PDF_ERROR_DIR,
            PDF_PROCESSING_DIR,
        ]:
            dir_path.mkdir(parents=True, exist_ok=True)
            logger.debug(f"ç¢ºèªç›®éŒ„å­˜åœ¨: {dir_path}")

    def load_state(self) -> Dict:
        """
        è¼‰å…¥è™•ç†ç‹€æ…‹

        ç‹€æ…‹æª”æ¡ˆè¨˜éŒ„æ¯å€‹è™•ç†éçš„æª”æ¡ˆçš„è³‡è¨Š
        é¿å…é‡è¤‡è™•ç†ç›¸åŒæª”æ¡ˆ

        Returns:
            dict: ç‹€æ…‹å­—å…¸
        """
        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f:
                    state = json.load(f)
                    logger.info(f"è¼‰å…¥ç‹€æ…‹æª”æ¡ˆï¼Œå·²è™•ç† {len(state)} å€‹æª”æ¡ˆ")
                    return state
            except Exception as e:
                logger.warning(f"è¼‰å…¥ç‹€æ…‹æª”æ¡ˆå¤±æ•—: {e}ï¼Œä½¿ç”¨ç©ºç‹€æ…‹")
                return {}
        return {}

    def save_state(self):
        """
        å„²å­˜è™•ç†ç‹€æ…‹
        æ¯è™•ç†å®Œä¸€å€‹æª”æ¡ˆå°±å„²å­˜ï¼Œç¢ºä¿æ–·é›»ä¹Ÿä¸æœƒéºå¤±é€²åº¦
        """
        try:
            with open(STATE_FILE, "w", encoding="utf-8") as f:
                json.dump(self.state, f, ensure_ascii=False, indent=2)
            logger.debug("ç‹€æ…‹å·²å„²å­˜")
        except Exception as e:
            logger.error(f"å„²å­˜ç‹€æ…‹å¤±æ•—: {e}")

    def get_file_hash(self, file_path: Path) -> str:
        """
        è¨ˆç®—æª”æ¡ˆçš„ SHA256 é›œæ¹Šå€¼
        ç”¨æ–¼åˆ¤æ–·æª”æ¡ˆæ˜¯å¦å·²è®Šæ›´

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘

        Returns:
            str: SHA256 é›œæ¹Šå€¼ï¼ˆ16é€²åˆ¶å­—ä¸²ï¼‰
        """
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            # åˆ†å¡Šè®€å–ï¼Œé¿å…å¤§æª”æ¡ˆå ç”¨éå¤šè¨˜æ†¶é«”
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    def process_pdf(self, pdf_path: Path) -> bool:
        """
        è™•ç†å–®ä¸€ PDF æª”æ¡ˆçš„å®Œæ•´æµç¨‹

        æ­¥é©Ÿï¼š
        1. è¨ˆç®—æª”æ¡ˆé›œæ¹Š
        2. ç§»åˆ°è™•ç†ä¸­ç›®éŒ„ï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
        3. è§£æ PDF å…§å®¹
        4. å»ºç«‹æ–‡æª”ç‰©ä»¶
        5. å­˜å…¥è³‡æ–™åº«
        6. ç§»åˆ°å®Œæˆæˆ–éŒ¯èª¤ç›®éŒ„

        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘

        Returns:
            bool: è™•ç†æ˜¯å¦æˆåŠŸ
        """
        start_time = time.time()
        file_hash = self.get_file_hash(pdf_path)
        file_size = pdf_path.stat().st_size

        try:
            logger.info(f"ğŸ“„ é–‹å§‹è™•ç†: {pdf_path.name} ({file_size/1024:.1f} KB)")

            # è¨˜éŒ„é–‹å§‹è™•ç†
            self.db.log_processing(pdf_path.name, file_hash, "processing")

            # æ­¥é©Ÿ1ï¼šç§»åˆ°è™•ç†ä¸­ç›®éŒ„ï¼ˆé˜²æ­¢å…¶ä»–é€²ç¨‹é‡è¤‡è™•ç†ï¼‰
            processing_path = PDF_PROCESSING_DIR / pdf_path.name
            pdf_path.rename(processing_path)
            logger.debug(f"æª”æ¡ˆç§»è‡³è™•ç†ä¸­: {processing_path}")

            # æ­¥é©Ÿ2ï¼šæå–æ–‡æœ¬
            text, page_count = PDFParser.extract_text(processing_path)

            if not text:
                raise ValueError("ç„¡æ³•æå–æ–‡æœ¬å…§å®¹ï¼ˆæª”æ¡ˆå¯èƒ½æå£æˆ–ç‚ºåœ–ç‰‡ï¼‰")

            logger.info(f"  æå–æ–‡æœ¬: {len(text)} å­—å…ƒ, {page_count} é ")

            # æ­¥é©Ÿ3ï¼šè§£ææ–‡æª”è³‡è¨Š
            doc_type = PDFParser.detect_doc_type(text, processing_path.stem)
            product_ids = PDFParser.extract_product_ids(text)
            metadata = PDFParser.extract_metadata(text, doc_type)
            keywords = PDFParser.extract_keywords(text, product_ids)
            summary = PDFParser.generate_summary(text)

            # å¾æª”åæå–æ–‡æª”ç·¨è™Ÿï¼ˆç¦èˆˆçš„å‘½åè¦å‰‡ï¼‰
            doc_number_match = re.search(
                r"[A-Z]{2,}-[A-Z]-\d{2}-[A-Z]-\d{3}|L\d{6}[A-Z]?\d?",
                processing_path.stem,
            )
            doc_number = (
                doc_number_match.group(0) if doc_number_match else processing_path.stem
            )

            # æ­¥é©Ÿ4ï¼šå»ºç«‹æ–‡æª”ç‰©ä»¶
            doc = TechnicalDocument(
                doc_id=hashlib.md5(processing_path.name.encode()).hexdigest(),
                doc_type=doc_type,
                doc_number=doc_number,
                title=processing_path.stem.replace("_", " "),  # å°‡åº•ç·šè½‰ç‚ºç©ºæ ¼
                product_ids=product_ids,
                revision=metadata.get("revision"),
                issue_date=(
                    metadata.get("dates", [None])[0] if metadata.get("dates") else None
                ),
                department=None,  # å¯å¾æ–‡æª”å…§å®¹é€²ä¸€æ­¥æå–
                author=None,  # å¯å¾æ–‡æª”å…§å®¹é€²ä¸€æ­¥æå–
                content=text,
                summary=summary,
                keywords=keywords,
                metadata=metadata,
                file_path=str(processing_path),
                file_hash=file_hash,
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                page_count=page_count,
                file_size=file_size,
            )

            logger.info(
                f"  æ–‡æª”è³‡è¨Š: é¡å‹={doc_type}, ç”¢å“æ•¸={len(product_ids)}, é—œéµå­—æ•¸={len(keywords)}"
            )

            # æ­¥é©Ÿ5ï¼šå­˜å…¥è³‡æ–™åº«
            if self.db.upsert_document(doc):
                # æˆåŠŸï¼šç§»åˆ°å®Œæˆç›®éŒ„
                done_path = PDF_DONE_DIR / processing_path.name
                processing_path.rename(done_path)

                process_time = int((time.time() - start_time) * 1000)
                self.db.log_processing(
                    pdf_path.name, file_hash, "success", process_time_ms=process_time
                )

                # æ›´æ–°ç‹€æ…‹
                self.state[pdf_path.name] = {
                    "hash": file_hash,
                    "processed_at": datetime.now().isoformat(),
                    "doc_id": doc.doc_id,
                    "status": "success",
                    "page_count": page_count,
                    "doc_type": doc_type,
                    "product_count": len(product_ids),
                }
                self.save_state()

                logger.info(f"  âœ… æˆåŠŸè™•ç†ï¼Œè€—æ™‚: {process_time}ms")
                return True
            else:
                raise Exception("è³‡æ–™åº«å¯«å…¥å¤±æ•—")

        except Exception as e:
            # å¤±æ•—ï¼šç§»åˆ°éŒ¯èª¤ç›®éŒ„
            error_msg = str(e)
            logger.error(f"  âŒ è™•ç†å¤±æ•—: {error_msg}")

            # ç¢ºä¿æª”æ¡ˆåœ¨è™•ç†ä¸­ç›®éŒ„
            if processing_path.exists():
                error_path = PDF_ERROR_DIR / pdf_path.name
                processing_path.rename(error_path)
                logger.debug(f"æª”æ¡ˆç§»è‡³éŒ¯èª¤ç›®éŒ„: {error_path}")

            process_time = int((time.time() - start_time) * 1000)
            self.db.log_processing(
                pdf_path.name,
                file_hash,
                "error",
                error_message=error_msg,
                process_time_ms=process_time,
            )

            # è¨˜éŒ„éŒ¯èª¤ç‹€æ…‹
            self.state[pdf_path.name] = {
                "hash": file_hash,
                "processed_at": datetime.now().isoformat(),
                "status": "error",
                "error": error_msg,
            }
            self.save_state()
            return False

    def scan_and_process(self):
        """
        æƒæç›£æ§ç›®éŒ„ä¸¦è™•ç† PDF æª”æ¡ˆ

        æµç¨‹ï¼š
        1. æƒæ incoming ç›®éŒ„
        2. æª¢æŸ¥æ¯å€‹æª”æ¡ˆæ˜¯å¦å·²è™•ç†
        3. æ‰¹æ¬¡è™•ç†æ–°æª”æ¡ˆ
        4. è¨˜éŒ„è™•ç†çµæœ

        Returns:
            int: æœ¬æ¬¡è™•ç†çš„æª”æ¡ˆæ•¸
        """
        # æƒææ‰€æœ‰ PDF æª”æ¡ˆ
        pdf_files = sorted(PDF_WATCH_DIR.glob("*.pdf"))

        if not pdf_files:
            return 0

        logger.info(f"ğŸ” ç™¼ç¾ {len(pdf_files)} å€‹å¾…è™•ç†æª”æ¡ˆ")

        processed = 0
        # é™åˆ¶æ¯æ‰¹è™•ç†çš„æª”æ¡ˆæ•¸ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œ
        for pdf_path in pdf_files[:PROCESS_BATCH_SIZE]:
            # æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢
            if should_stop:
                logger.info("æ”¶åˆ°åœæ­¢ä¿¡è™Ÿï¼Œä¸­æ–·è™•ç†")
                break

            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éæ­¤æª”æ¡ˆ
            file_hash = self.get_file_hash(pdf_path)
            if pdf_path.name in self.state:
                if self.state[pdf_path.name].get("hash") == file_hash:
                    logger.info(f"â­ï¸ è·³éå·²è™•ç†: {pdf_path.name}")
                    # å·²è™•ç†éçš„æª”æ¡ˆç§»åˆ°å®Œæˆç›®éŒ„
                    done_path = PDF_DONE_DIR / pdf_path.name
                    pdf_path.rename(done_path)
                    continue

            # è™•ç†æª”æ¡ˆ
            if self.process_pdf(pdf_path):
                processed += 1

            # çŸ­æš«ä¼‘æ¯ï¼Œé¿å… CPU éè¼‰
            time.sleep(1)

        return processed

    def run(self):
        """
        ä¸»åŸ·è¡Œå¾ªç’°
        æŒçºŒç›£æ§ç›®éŒ„ä¸¦è™•ç†æ–°æª”æ¡ˆ
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ PDF è™•ç†æœå‹™å•Ÿå‹•")
        logger.info(f"ğŸ“ ç›£æ§ç›®éŒ„: {PDF_WATCH_DIR}")
        logger.info(f"â±  æƒæé–“éš”: {SCAN_INTERVAL} ç§’")
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {PROCESS_BATCH_SIZE}")
        logger.info(f"ğŸ” OCR ç‹€æ…‹: {'å•Ÿç”¨' if ENABLE_OCR else 'é—œé–‰'}")
        logger.info("=" * 60)

        no_file_count = 0  # é€£çºŒç„¡æª”æ¡ˆçš„æ¬¡æ•¸

        try:
            while not should_stop:
                try:
                    # åŸ·è¡Œä¸€æ¬¡æƒæå’Œè™•ç†
                    processed = self.scan_and_process()

                    if processed > 0:
                        no_file_count = 0
                        logger.info(f"âœ¨ æœ¬è¼ªè™•ç†å®Œæˆï¼Œå…± {processed} å€‹æª”æ¡ˆ")
                    else:
                        no_file_count += 1

                        # å‹•æ…‹èª¿æ•´æƒæé–“éš”ï¼ˆç„¡æª”æ¡ˆæ™‚é€æ¼¸å»¶é•·é–“éš”ï¼‰
                        sleep_time = min(SCAN_INTERVAL * (1 + no_file_count // 10), 300)

                        # æ¯10æ¬¡ç„¡æª”æ¡ˆæ‰è¼¸å‡ºä¸€æ¬¡æ—¥èªŒï¼Œé¿å…æ—¥èªŒéå¤š
                        if no_file_count % 10 == 0:
                            logger.info(f"ğŸ’¤ ç„¡æ–°æª”æ¡ˆï¼Œç­‰å¾… {sleep_time} ç§’...")

                    # ä¼‘çœ ç­‰å¾…ä¸‹æ¬¡æƒæ
                    sleep_time = 5 if processed > 0 else SCAN_INTERVAL
                    for _ in range(sleep_time):
                        if should_stop:
                            break
                        time.sleep(1)  # æ¯ç§’æª¢æŸ¥ä¸€æ¬¡åœæ­¢ä¿¡è™Ÿ

                except Exception as e:
                    logger.error(f"è™•ç†é€±æœŸéŒ¯èª¤: {e}", exc_info=True)
                    time.sleep(30)  # éŒ¯èª¤å¾Œç­‰å¾…è¼ƒé•·æ™‚é–“

        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°éµç›¤ä¸­æ–·")
        finally:
            logger.info("ğŸ›‘ æœå‹™æ­£åœ¨é—œé–‰...")
            self.db.close()
            logger.info("ğŸ‘‹ æœå‹™å·²åœæ­¢")


# ============================================================================
# ä¸»ç¨‹å¼å…¥å£
# ============================================================================
def main():
    """
    ä¸»ç¨‹å¼å…¥å£

    æ­¥é©Ÿï¼š
    1. ç­‰å¾… MySQL å°±ç·’
    2. åˆå§‹åŒ–æœå‹™
    3. å•Ÿå‹•ä¸»å¾ªç’°
    """
    # ç­‰å¾… MySQL å°±ç·’ï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
    logger.info("â³ ç­‰å¾… MySQL å°±ç·’...")
    for i in range(30):
        try:
            db = DatabaseManager()
            db.close()
            logger.info("âœ… MySQL é€£ç·šæˆåŠŸ")
            break
        except Exception as e:
            if i < 29:
                logger.info(f"ç­‰å¾… MySQL... ({i+1}/30)")
                time.sleep(2)
            else:
                logger.error(f"âŒ ç„¡æ³•é€£ç·šåˆ° MySQL: {e}")
                sys.exit(1)

    # å•Ÿå‹•æœå‹™
    service = PDFProcessorService()
    service.run()


if __name__ == "__main__":
    main()
