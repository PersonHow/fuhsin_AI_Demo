#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡ç”Ÿæˆæœå‹™ - ä½¿ç”¨ OpenAI API ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆä¾ç…§åŸå§‹çµæ§‹åŠ å¼·ç©©å®šæ€§ï¼‰

æœ¬ç‰ˆé‡é»ï¼š
- å•Ÿå‹•æ™‚ç­‰å¾… Elasticsearch å°±ç·’ï¼ˆyellow/greenï¼‰
- å° ES è«‹æ±‚åŠ å…¥é‡è©¦ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰ï¼Œé¿å…çŸ­æš« 50x/é€£ç·šéŒ¯èª¤é€ æˆå®¹å™¨é‡å•Ÿ
- ä¾æ¨¡å‹è‡ªå‹•è¨­å®š dense_vector ç¶­åº¦ï¼ˆtext-embedding-3-small=1536, 3-large=3072ï¼‰
- æ‰¹æ¬¡æœå°‹ç¼ºå‘é‡æ–‡ä»¶ä¸¦ä»¥ _bulk æ›´æ–°
"""
from __future__ import annotations

import os, time, json, signal, requests
from datetime import datetime
from typing import List, Dict, Any, Optional
from requests.auth import HTTPBasicAuth

try:
    from openai import OpenAI
except Exception:  # é¿å…ç’°å¢ƒæš«ç„¡ openai å¥—ä»¶
    OpenAI = None  # type: ignore

# -----------------------------
# ç’°å¢ƒè®Šæ•¸
# -----------------------------
ES_URL = os.environ.get("ES_URL", "http://elasticsearch:9200").rstrip("/")
ES_USER = os.getenv("ES_USER")
ES_PASS = os.getenv("ES_PASS")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1").rstrip(
    "/"
)
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
INDEX_PATTERN = os.environ.get("INDEX_PATTERN", "erp-*")
BATCH_SIZE = int(os.environ.get("VECTOR_BATCH_SIZE", "100"))
SLEEP_SEC = int(os.environ.get("SLEEP", "10"))
ES_WAIT_TIMEOUT = int(os.environ.get("ES_WAIT_TIMEOUT", "180"))
REQUESTS_TIMEOUT = int(os.environ.get("REQUESTS_TIMEOUT", "30"))
MAX_RETRIES = int(os.environ.get("MAX_RETRIES", "5"))

# -----------------------------
# é€£ç·šç‰©ä»¶
# -----------------------------
session = requests.Session()
if ES_USER and ES_PASS:
    session.auth = HTTPBasicAuth(ES_USER, ES_PASS)
session.headers.update({"Content-Type": "application/json"})

client: Optional[OpenAI] = None
if OPENAI_API_KEY and OpenAI is not None:
    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

_SHOULD_STOP = False

# -----------------------------
# å·¥å…·æ–¹æ³•
# -----------------------------


def log(msg: str) -> None:
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)


def wait_for_es(timeout_sec: int = ES_WAIT_TIMEOUT) -> None:
    """ç­‰å¾… Elasticsearch è‡³å°‘é”åˆ° yellow å¥åº·ç‹€æ…‹ã€‚è¶…æ™‚å‰‡ä¸Ÿä¾‹å¤–ã€‚"""
    deadline = time.time() + timeout_sec
    last_err: Optional[Exception] = None
    while time.time() < deadline:
        try:
            r = session.get(
                f"{ES_URL}/_cluster/health",
                params={"wait_for_status": "yellow", "timeout": "30s"},
                timeout=REQUESTS_TIMEOUT,
            )
            if r.ok:
                status = r.json().get("status")
                if status in ("yellow", "green"):
                    log(f"ES å°±ç·’ï¼ˆstatus={status}ï¼‰")
                    return
                log(f"ES ç‹€æ…‹ {status}ï¼Œç¹¼çºŒç­‰å¾…â€¦")
        except Exception as e:
            last_err = e
        time.sleep(3)
    raise RuntimeError(f"Elasticsearch åœ¨ {timeout_sec}s å…§æœªå°±ç·’: {last_err}")


def _sleep_backoff(i: int, base: float = 1.0) -> None:
    time.sleep(base * (2**i))  # 1,2,4,8,â€¦


def http_get(
    url: str,
    *,
    params: Optional[Dict[str, Any]] = None,
    headers: Optional[Dict[str, str]] = None,
    retries: int = MAX_RETRIES,
) -> requests.Response:
    for i in range(retries):
        try:
            r = session.get(
                url, params=params, headers=headers, timeout=REQUESTS_TIMEOUT
            )
            if r.status_code in (502, 503, 504):
                raise requests.ConnectionError(f"Transient {r.status_code}")
            return r
        except (requests.ConnectionError, requests.Timeout) as e:
            if i == retries - 1:
                raise
            log(f"GET é‡è©¦ {i+1}/{retries-1}: {e}")
            _sleep_backoff(i)
    raise RuntimeError("GET é‡è©¦å·²ç”¨ç›¡")


def http_post(
    url: str,
    *,
    json_body: Optional[Dict[str, Any]] = None,
    data: Optional[str] = None,
    headers: Optional[Dict[str, str]] = None,
    retries: int = MAX_RETRIES,
) -> requests.Response:
    for i in range(retries):
        try:
            r = session.post(
                url,
                json=json_body,
                data=data,
                headers=headers,
                timeout=REQUESTS_TIMEOUT,
            )
            if r.status_code in (502, 503, 504):
                raise requests.ConnectionError(f"Transient {r.status_code}")
            return r
        except (requests.ConnectionError, requests.Timeout) as e:
            if i == retries - 1:
                raise
            log(f"POST é‡è©¦ {i+1}/{retries-1}: {e}")
            _sleep_backoff(i)
    raise RuntimeError("POST é‡è©¦å·²ç”¨ç›¡")


# -----------------------------
# å‘é‡ç”Ÿæˆå™¨ï¼ˆä¿ç•™åŸçµæ§‹/å‘½åï¼‰
# -----------------------------
class VectorGenerator:
    """å‘é‡ç”Ÿæˆå™¨"""

    def __init__(self, model: str):
        self.model = model
        if "text-embedding-3-large" in model:
            self.dimension = 3072
        else:
            self.dimension = 1536  # text-embedding-3-small / ada-002 ç›¸å®¹

    def generate(self, text: str) -> Optional[List[float]]:
        if client is None:
            log("âŒ OpenAI client æœªåˆå§‹åŒ–ï¼Œè«‹ç¢ºèª OPENAI_API_KEY èˆ‡ openai å¥—ä»¶ã€‚")
            return None
        try:
            resp = client.embeddings.create(
                model=self.model,
                input=text[:8000],
                encoding_format="float",
            )
            return resp.data[0].embedding  # type: ignore[no-any-return]
        except Exception as e:
            log(f"âš ï¸ å‘é‡ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return None

    def batch_generate(self, texts: List[str]) -> List[Optional[List[float]]]:
        if client is None:
            return [None for _ in texts]
        try:
            resp = client.embeddings.create(
                model=self.model,
                input=[t[:8000] for t in texts],
                encoding_format="float",
            )
            return [d.embedding for d in resp.data]  # type: ignore[attr-defined]
        except Exception as e:
            log(f"âš ï¸ æ‰¹é‡ç”Ÿæˆå¤±æ•—ï¼Œæ”¹ç‚ºé€ç­†ï¼š{e}")
            out: List[Optional[List[float]]] = []
            for t in texts:
                out.append(self.generate(t))
                time.sleep(0.1)
            return out


# -----------------------------
# ES æ›´æ–°å™¨ï¼ˆä¿ç•™åŸé¡åˆ¥/æ–¹æ³•åï¼‰
# -----------------------------
class ElasticsearchVectorUpdater:
    """Elasticsearch å‘é‡æ›´æ–°å™¨"""

    def __init__(self, vector_gen: VectorGenerator):
        self.vector_gen = vector_gen

    def _list_indices(self, index_pattern: str) -> List[str]:
        # å„ªå…ˆç”¨ _cat/indicesï¼›è‹¥å¤±æ•—å†é€€å› GET /{pattern}
        try:
            r = http_get(
                f"{ES_URL}/_cat/indices/{index_pattern}", params={"format": "json"}
            )
            if r.ok:
                return [row["index"] for row in r.json()]
        except Exception:
            pass
        try:
            r = http_get(f"{ES_URL}/{index_pattern}")
            if r.ok and isinstance(r.json(), dict):
                return list(r.json().keys())
        except Exception:
            pass
        return []

    def update_index_mapping(self, index_pattern: str = INDEX_PATTERN) -> None:
        """æ›´æ–°ç´¢å¼•æ˜ å°„ï¼Œæ·»åŠ å‘é‡æ¬„ä½"""
        mapping_update = {
            "properties": {
                "content_vector": {
                    "type": "dense_vector",
                    "dims": self.vector_gen.dimension,
                    "index": True,
                    "similarity": "cosine",
                },
                "vector_generated_at": {"type": "date"},
            }
        }
        indices = self._list_indices(index_pattern)
        if not indices:
            log(f"â„¹ï¸ æœªæ‰¾åˆ°ç¬¦åˆçš„ç´¢å¼•ï¼š{index_pattern}ï¼Œç¨å¾Œè³‡æ–™å¯«å…¥å†è£œ mapping")
            return
        for index in indices:
            try:
                r = session.put(
                    f"{ES_URL}/{index}/_mapping",
                    json=mapping_update,
                    timeout=REQUESTS_TIMEOUT,
                )
                if r.ok:
                    log(f"âœ… å·²æ›´æ–°ç´¢å¼•æ˜ å°„ï¼š{index}")
                else:
                    log(f"âš ï¸ æ›´æ–°ç´¢å¼•æ˜ å°„å¤±æ•—ï¼š{index} {r.status_code} {r.text[:200]}")
            except Exception as e:
                log(f"âš ï¸ ç´¢å¼• {index} æ˜ å°„æ›´æ–°ä¾‹å¤–ï¼š{e}")

    def find_documents_without_vectors(
        self, index_pattern: str = INDEX_PATTERN, size: int = 100
    ) -> List[Dict[str, Any]]:
        """æœå°‹å°šæœªå»ºç«‹ content_vector çš„æ–‡ä»¶"""
        query = {
            "size": size,
            "_source": ["searchable_content", "all_content", "field_*"],
            "query": {"bool": {"must_not": [{"exists": {"field": "content_vector"}}]}},
            "sort": [{"_doc": "asc"}],
        }
        try:
            r = http_post(f"{ES_URL}/{index_pattern}/_search", json_body=query)
            if r.ok:
                body = r.json()
                return body.get("hits", {}).get("hits", [])  # type: ignore[no-any-return]
            log(f"âš ï¸ æœå°‹å¤±æ•— {r.status_code}: {r.text[:200]}")
        except Exception as e:
            log(f"âš ï¸ æœå°‹ä¾‹å¤–ï¼š{e}")
        return []

    def _extract_text(self, source: Dict[str, Any]) -> str:
        # å„ªå…ˆä½¿ç”¨æè¿°æ€§æ¬„ä½
        priority_fields = [
            "field_description",
            "field_product_name",
            "field_complaint_type",
        ]
        text_parts = []

        for field in priority_fields:
            if field in source and source[field]:
                text_parts.append(str(source[field]))

        return " ".join(text_parts) if text_parts else source.get("all_content", "")

    def update_document_vectors(self, documents: List[Dict[str, Any]]) -> None:
        """æ›´æ–°æ–‡æª”å‘é‡"""
        if not documents:
            return
        texts = [self._extract_text(doc.get("_source", {})) for doc in documents]
        log(f"ğŸ”„ ç”Ÿæˆ {len(texts)} å€‹å‘é‡â€¦")
        embeddings = self.vector_gen.batch_generate(texts)

        # æ§‹å»º _bulk è«‹æ±‚
        lines: List[str] = []
        for doc, emb in zip(documents, embeddings):
            if not emb:
                continue
            lines.append(
                json.dumps(
                    {"update": {"_index": doc.get("_index"), "_id": doc.get("_id")}}
                )
            )
            lines.append(
                json.dumps(
                    {
                        "doc": {
                            "content_vector": emb,
                            "vector_generated_at": datetime.utcnow().isoformat(),
                        }
                    }
                )
            )
        if not lines:
            log("â„¹ï¸ æ²’æœ‰å¯æ›´æ–°çš„å‘é‡ï¼ˆæ–‡æœ¬ç‚ºç©ºæˆ–å…¨éƒ¨å¤±æ•—ï¼‰")
            return
        payload = "\n".join(lines) + "\n"
        try:
            r = http_post(
                f"{ES_URL}/_bulk",
                data=payload,
                headers={"Content-Type": "application/x-ndjson"},
            )
            if r.ok:
                res = r.json()
                if not res.get("errors"):
                    log(f"âœ… æˆåŠŸæ›´æ–° {len(documents)} å€‹æ–‡æª”çš„å‘é‡")
                else:
                    fails = sum(
                        1
                        for it in res.get("items", [])
                        if any(v.get("error") for v in it.values())
                    )
                    log(f"âš ï¸ éƒ¨åˆ†æ›´æ–°å¤±æ•—ï¼š{fails}/{len(documents)}")
            else:
                log(f"âŒ æ‰¹é‡æ›´æ–°å¤±æ•— {r.status_code}: {r.text[:200]}")
        except Exception as e:
            log(f"âŒ æ‰¹é‡æ›´æ–°ä¾‹å¤–ï¼š{e}")


# -----------------------------
# ä¸»æµç¨‹ï¼ˆä¿ç•™ä½ çš„å‘¼å«è·¯å¾‘ï¼‰
# -----------------------------


def _handle_sigterm(signum, frame):
    global _SHOULD_STOP
    _SHOULD_STOP = True
    log("æ”¶åˆ°åœæ­¢è¨Šè™Ÿï¼Œæº–å‚™çµæŸâ€¦")


def main() -> None:
    if not OPENAI_API_KEY:
        log("âŒ æœªè¨­ç½® OPENAI_API_KEYï¼Œç„¡æ³•ç”Ÿæˆå‘é‡ã€‚")
        return
    if client is None:
        log("âŒ OpenAI å¥—ä»¶æœªæ­£ç¢ºå®‰è£æˆ–åˆå§‹åŒ–ã€‚è«‹ç¢ºèª 'pip install openai' ç‰ˆæœ¬æ”¯æ´ã€‚")
        return

    signal.signal(signal.SIGTERM, _handle_sigterm)
    signal.signal(signal.SIGINT, _handle_sigterm)

    log("ğŸš€ å‘é‡æœå‹™å•Ÿå‹•")
    log(f"ğŸ“Š æ¨¡å‹ï¼š{EMBEDDING_MODEL}ï¼Œç´¢å¼•æ¨¡å¼ï¼š{INDEX_PATTERN}")

    try:
        wait_for_es()
    except Exception as e:
        log(f"âŒ ç­‰å¾… Elasticsearch å¤±æ•—ï¼š{e}")
        return

    vg = VectorGenerator(EMBEDDING_MODEL)
    updater = ElasticsearchVectorUpdater(vg)
    updater.update_index_mapping(INDEX_PATTERN)

    while not _SHOULD_STOP:
        try:
            docs = updater.find_documents_without_vectors(
                INDEX_PATTERN, size=BATCH_SIZE
            )
            if docs:
                log(f"ğŸ“ æ‰¾åˆ° {len(docs)} å€‹éœ€è¦ç”Ÿæˆå‘é‡çš„æ–‡æª”")
                updater.update_document_vectors(docs)
            else:
                log("ğŸ˜´ æ‰€æœ‰æ–‡æª”éƒ½å·²æœ‰å‘é‡ï¼Œç­‰å¾…ä¸­â€¦")
        except Exception as e:
            log(f"âŒ ä¸»å¾ªç’°éŒ¯èª¤: {e}")
        time.sleep(SLEEP_SEC)

    log("ğŸ‘‹ å‘é‡æœå‹™çµæŸã€‚")


if __name__ == "__main__":
    main()
