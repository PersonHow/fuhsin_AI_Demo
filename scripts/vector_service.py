#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡ç”Ÿæˆæœå‹™ - ä½¿ç”¨ OpenAI API ç”Ÿæˆæ–‡æœ¬å‘é‡
"""
import os
import time
import json
import requests
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import openai
from openai import OpenAI
import numpy as np

# ç’°å¢ƒè®Šæ•¸é…ç½®
ES_URL = os.environ.get("ES_URL", "http://es01:9200")
ES_USER = os.environ.get("ES_USER", "elastic")
ES_PASS = os.environ.get("ES_PASS", "admin@12345")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
BATCH_SIZE = int(os.environ.get("VECTOR_BATCH_SIZE", "100"))
SLEEP_SEC = int(os.environ.get("SLEEP", "10"))

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL  # æ”¯æ´è‡ªè¨‚ç«¯é»ï¼ˆå¦‚ Azure OpenAIï¼‰
)

# Elasticsearch é€£ç·š
session = requests.Session()
session.auth = (ES_USER, ES_PASS)
session.headers.update({"Content-Type": "application/json"})

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)

class VectorGenerator:
    """å‘é‡ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.dimension = 1536  # text-embedding-3-small çš„ç¶­åº¦
        if "text-embedding-3-large" in EMBEDDING_MODEL:
            self.dimension = 3072
        elif "ada" in EMBEDDING_MODEL:
            self.dimension = 1536
            
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆå–®å€‹æ–‡æœ¬çš„å‘é‡"""
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text[:8000],  # OpenAI é™åˆ¶
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            log(f"âš ï¸ ç”Ÿæˆå‘é‡å¤±æ•—: {e}")
            return None
    
    def batch_generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆå‘é‡"""
        try:
            # OpenAI æ”¯æ´æ‰¹é‡è«‹æ±‚
            truncated_texts = [t[:8000] for t in texts]
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=truncated_texts,
                encoding_format="float"
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            log(f"âš ï¸ æ‰¹é‡ç”Ÿæˆå‘é‡å¤±æ•—: {e}")
            # é™ç´šç‚ºé€å€‹ç”Ÿæˆ
            results = []
            for text in texts:
                emb = self.generate_embedding(text)
                results.append(emb if emb else [0.0] * self.dimension)
                time.sleep(0.1)  # é¿å… rate limit
            return results

class ElasticsearchVectorUpdater:
    """Elasticsearch å‘é‡æ›´æ–°å™¨"""
    
    def __init__(self, vector_gen: VectorGenerator):
        self.vector_gen = vector_gen
        
    def update_index_mapping(self, index_pattern: str = "erp-*"):
        """æ›´æ–°ç´¢å¼•æ˜ å°„ï¼Œæ·»åŠ å‘é‡æ¬„ä½"""
        mapping_update = {
            "properties": {
                "content_vector": {
                    "type": "dense_vector",
                    "dims": self.vector_gen.dimension,
                    "index": True,
                    "similarity": "cosine"
                },
                "vector_generated_at": {
                    "type": "date"
                }
            }
        }
        
        # ç²å–æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„ç´¢å¼•
        r = session.get(f"{ES_URL}/{index_pattern}")
        if r.status_code == 200:
            indices = list(r.json().keys())
            for index in indices:
                try:
                    r = session.put(
                        f"{ES_URL}/{index}/_mapping",
                        json=mapping_update
                    )
                    if r.status_code == 200:
                        log(f"âœ… æ›´æ–°ç´¢å¼•æ˜ å°„: {index}")
                except Exception as e:
                    log(f"âš ï¸ æ›´æ–°ç´¢å¼• {index} å¤±æ•—: {e}")
    
    def find_documents_without_vectors(self, index_pattern: str = "erp-*", size: int = 100):
        """æŸ¥æ‰¾æ²’æœ‰å‘é‡çš„æ–‡æª”"""
        query = {
            "size": size,
            "_source": ["searchable_content", "all_content", "field_*"],
            "query": {
                "bool": {
                    "must_not": [
                        {"exists": {"field": "content_vector"}}
                    ]
                }
            }
        }
        
        try:
            r = session.post(f"{ES_URL}/{index_pattern}/_search", json=query)
            if r.status_code == 200:
                return r.json()["hits"]["hits"]
        except Exception as e:
            log(f"âš ï¸ æŸ¥è©¢å¤±æ•—: {e}")
        return []
    
    def update_document_vectors(self, documents: List[Dict]):
        """æ›´æ–°æ–‡æª”å‘é‡"""
        if not documents:
            return
        
        # æº–å‚™æ–‡æœ¬
        texts = []
        for doc in documents:
            source = doc["_source"]
            # å„ªå…ˆä½¿ç”¨ searchable_contentï¼Œå…¶æ¬¡ all_content
            text = source.get("searchable_content", "") or source.get("all_content", "")
            if not text:
                # çµ„åˆæ‰€æœ‰ field_ é–‹é ­çš„æ¬„ä½
                field_texts = []
                for key, value in source.items():
                    if key.startswith("field_") and value:
                        field_texts.append(str(value))
                text = " ".join(field_texts)
            texts.append(text)
        
        # æ‰¹é‡ç”Ÿæˆå‘é‡
        log(f"ğŸ”„ ç”Ÿæˆ {len(texts)} å€‹å‘é‡...")
        embeddings = self.vector_gen.batch_generate_embeddings(texts)
        
        # æ‰¹é‡æ›´æ–°
        bulk_body = []
        for doc, embedding in zip(documents, embeddings):
            if embedding:
                bulk_body.append(json.dumps({
                    "update": {
                        "_index": doc["_index"],
                        "_id": doc["_id"]
                    }
                }))
                bulk_body.append(json.dumps({
                    "doc": {
                        "content_vector": embedding,
                        "vector_generated_at": datetime.now().isoformat()
                    }
                }))
        
        if bulk_body:
            payload = "\n".join(bulk_body) + "\n"
            try:
                r = session.post(
                    f"{ES_URL}/_bulk",
                    data=payload,
                    headers={"Content-Type": "application/x-ndjson"}
                )
                if r.status_code == 200:
                    result = r.json()
                    if not result.get("errors"):
                        log(f"âœ… æˆåŠŸæ›´æ–° {len(documents)} å€‹æ–‡æª”çš„å‘é‡")
                    else:
                        log(f"âš ï¸ éƒ¨åˆ†æ›´æ–°å¤±æ•—")
            except Exception as e:
                log(f"âŒ æ‰¹é‡æ›´æ–°å¤±æ•—: {e}")

def main():
    """ä¸»ç¨‹åº"""
    if not OPENAI_API_KEY:
        log("âŒ æœªè¨­ç½® OPENAI_API_KEY")
        return
    
    log("ğŸš€ å‘é‡ç”Ÿæˆæœå‹™å•Ÿå‹•")
    log(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {EMBEDDING_MODEL}")
    
    vector_gen = VectorGenerator()
    updater = ElasticsearchVectorUpdater(vector_gen)
    
    # åˆæ¬¡é‹è¡Œæ™‚æ›´æ–°æ˜ å°„
    updater.update_index_mapping()
    
    while True:
        try:
            # æŸ¥æ‰¾éœ€è¦ç”Ÿæˆå‘é‡çš„æ–‡æª”
            docs = updater.find_documents_without_vectors(size=BATCH_SIZE)
            
            if docs:
                log(f"ğŸ“ æ‰¾åˆ° {len(docs)} å€‹éœ€è¦ç”Ÿæˆå‘é‡çš„æ–‡æª”")
                updater.update_document_vectors(docs)
            else:
                log("ğŸ˜´ æ‰€æœ‰æ–‡æª”éƒ½å·²æœ‰å‘é‡ï¼Œç­‰å¾…ä¸­...")
            
            time.sleep(SLEEP_SEC)
            
        except Exception as e:
            log(f"âŒ ä¸»å¾ªç’°éŒ¯èª¤: {e}")
            time.sleep(SLEEP_SEC)

if __name__ == "__main__":
    main()
